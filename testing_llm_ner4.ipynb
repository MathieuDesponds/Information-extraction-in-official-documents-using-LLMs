{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-09 13:12:17 llm_engine.py:70] Initializing an LLM engine with config: model='mistralai/Mistral-7B-v0.1', tokenizer='mistralai/Mistral-7B-v0.1', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacty of 16.00 GiB of which 0 bytes is free. Process 15840 has 1.58 GiB memory in use. Process 16636 has 1.58 GiB memory in use. Including non-PyTorch memory, this process has 12.75 GiB memory in use. Of the allocated memory 12.11 GiB is allocated by PyTorch, and 13.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/myhome/Master-thesis/testing_llm_ner4.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B127.0.0.1/myhome/Master-thesis/testing_llm_ner4.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m plus_plus \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B127.0.0.1/myhome/Master-thesis/testing_llm_ner4.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m pt \u001b[39m=\u001b[39m PT_OutputList\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B127.0.0.1/myhome/Master-thesis/testing_llm_ner4.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m model \u001b[39m=\u001b[39m MistralAI(llm_loader\u001b[39m=\u001b[39;49mVLLM)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B127.0.0.1/myhome/Master-thesis/testing_llm_ner4.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# model.add_grammar('discussion')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B127.0.0.1/myhome/Master-thesis/testing_llm_ner4.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# results, results_df = model.classical_test_ontonote5(pts = [pt],fsts = [FST_NoShots],  nb_few_shots=[0] ,nb_run_by_test=1, plus_plus= plus_plus )\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B127.0.0.1/myhome/Master-thesis/testing_llm_ner4.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# results, results_df = model.classical_test_ontonote5(pts = [pt],fsts = [FST_Sentence], nb_few_shots=[3] ,nb_run_by_test=3, plus_plus= plus_plus)\u001b[39;00m\n",
      "File \u001b[0;32m/myhome/Master-thesis/llm/LLMModel.py:337\u001b[0m, in \u001b[0;36mMistralAI.__init__\u001b[0;34m(self, base_model_id, base_model_name, quantization, llm_loader, without_model, lora_path)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, base_model_id \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmistralai/Mistral-7B-v0.1\u001b[39m\u001b[39m\"\u001b[39m, base_model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMistral-7B-v0.1\u001b[39m\u001b[39m\"\u001b[39m, quantization \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mQ5_0\u001b[39m\u001b[39m'\u001b[39m, llm_loader \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, without_model \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, lora_path \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 337\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(base_model_id, base_model_name, quantization\u001b[39m=\u001b[39;49mquantization, llm_loader\u001b[39m=\u001b[39;49mllm_loader, without_model\u001b[39m=\u001b[39;49mwithout_model, lora_path \u001b[39m=\u001b[39;49m lora_path)\n",
      "File \u001b[0;32m/myhome/Master-thesis/llm/LLMModel.py:59\u001b[0m, in \u001b[0;36mLLMModel.__init__\u001b[0;34m(self, base_model_id, base_model_name, check_nb_tokens, max_tokens, quantization, llm_loader, without_model, lora_path)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_loader \u001b[39m=\u001b[39m llm_loader()\n\u001b[1;32m     58\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m without_model :\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel : LlamaLoader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_model(quantization \u001b[39m=\u001b[39;49m quantization, lora_path \u001b[39m=\u001b[39;49m lora_path)\n\u001b[1;32m     60\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_nb_tokens \u001b[39m=\u001b[39m check_nb_tokens\n\u001b[1;32m     61\u001b[0m \u001b[39mif\u001b[39;00m check_nb_tokens :\n",
      "File \u001b[0;32m/myhome/Master-thesis/llm/LLMModel.py:73\u001b[0m, in \u001b[0;36mLLMModel.get_model\u001b[0;34m(self, quantization, gguf_model_path, lora_path)\u001b[0m\n\u001b[1;32m     71\u001b[0m     model_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mllm/models/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model_name\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model_name\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mquantization\u001b[39m}\u001b[39;00m\u001b[39m.gguf\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_loader\u001b[39m.\u001b[39;49mget_llm_instance(model_path \u001b[39m=\u001b[39;49m model_path, lora_path\u001b[39m=\u001b[39;49m lora_path)\n\u001b[1;32m     74\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     75\u001b[0m     torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n",
      "File \u001b[0;32m/myhome/Master-thesis/llm/LlamaLoader.py:86\u001b[0m, in \u001b[0;36mVLLM.get_llm_instance\u001b[0;34m(self, model_path, lora_path)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmistral\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m model_path :\n\u001b[1;32m     84\u001b[0m     model_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmistralai/Mistral-7B-v0.1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 86\u001b[0m llm \u001b[39m=\u001b[39m LLM(model\u001b[39m=\u001b[39;49mmodel_path,\n\u001b[1;32m     87\u001b[0m           dtype\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbfloat16\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     88\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m llm\n\u001b[1;32m     89\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/myhome/miniconda3/lib/python3.10/site-packages/vllm/entrypoints/llm.py:105\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, enforce_eager, max_context_len_to_capture, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdisable_log_stats\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     88\u001b[0m engine_args \u001b[39m=\u001b[39m EngineArgs(\n\u001b[1;32m     89\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     90\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    104\u001b[0m )\n\u001b[0;32m--> 105\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_engine \u001b[39m=\u001b[39m LLMEngine\u001b[39m.\u001b[39;49mfrom_engine_args(engine_args)\n\u001b[1;32m    106\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_counter \u001b[39m=\u001b[39m Counter()\n",
      "File \u001b[0;32m/myhome/miniconda3/lib/python3.10/site-packages/vllm/engine/llm_engine.py:309\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args)\u001b[0m\n\u001b[1;32m    307\u001b[0m placement_group \u001b[39m=\u001b[39m initialize_cluster(parallel_config)\n\u001b[1;32m    308\u001b[0m \u001b[39m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m engine \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49mengine_configs,\n\u001b[1;32m    310\u001b[0m              placement_group,\n\u001b[1;32m    311\u001b[0m              log_stats\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m engine_args\u001b[39m.\u001b[39;49mdisable_log_stats)\n\u001b[1;32m    312\u001b[0m \u001b[39mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m/myhome/miniconda3/lib/python3.10/site-packages/vllm/engine/llm_engine.py:111\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, placement_group, log_stats)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_workers_ray(placement_group)\n\u001b[1;32m    110\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_workers()\n\u001b[1;32m    113\u001b[0m \u001b[39m# Profile the memory usage and initialize the cache.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_cache()\n",
      "File \u001b[0;32m/myhome/miniconda3/lib/python3.10/site-packages/vllm/engine/llm_engine.py:146\u001b[0m, in \u001b[0;36mLLMEngine._init_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdriver_worker \u001b[39m=\u001b[39m Worker(\n\u001b[1;32m    137\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_config,\n\u001b[1;32m    138\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparallel_config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m     is_driver_worker\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    144\u001b[0m )\n\u001b[1;32m    145\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_workers(\u001b[39m\"\u001b[39m\u001b[39minit_model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 146\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_workers(\u001b[39m\"\u001b[39;49m\u001b[39mload_model\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/myhome/miniconda3/lib/python3.10/site-packages/vllm/engine/llm_engine.py:795\u001b[0m, in \u001b[0;36mLLMEngine._run_workers\u001b[0;34m(self, method, driver_args, driver_kwargs, max_concurrent_workers, *args, **kwargs)\u001b[0m\n\u001b[1;32m    792\u001b[0m     driver_kwargs \u001b[39m=\u001b[39m kwargs\n\u001b[1;32m    794\u001b[0m \u001b[39m# Start the driver worker after all the ray workers.\u001b[39;00m\n\u001b[0;32m--> 795\u001b[0m driver_worker_output \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdriver_worker,\n\u001b[1;32m    796\u001b[0m                                method)(\u001b[39m*\u001b[39;49mdriver_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdriver_kwargs)\n\u001b[1;32m    798\u001b[0m \u001b[39m# Get the results of the ray workers.\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers:\n",
      "File \u001b[0;32m/myhome/miniconda3/lib/python3.10/site-packages/vllm/worker/worker.py:81\u001b[0m, in \u001b[0;36mWorker.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_model\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 81\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_runner\u001b[39m.\u001b[39;49mload_model()\n",
      "File \u001b[0;32m/myhome/miniconda3/lib/python3.10/site-packages/vllm/worker/model_runner.py:64\u001b[0m, in \u001b[0;36mModelRunner.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_model\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m get_model(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_config)\n",
      "File \u001b[0;32m/myhome/miniconda3/lib/python3.10/site-packages/vllm/model_executor/model_loader.py:65\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(model_config)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mwith\u001b[39;00m _set_default_torch_dtype(model_config\u001b[39m.\u001b[39mdtype):\n\u001b[1;32m     62\u001b[0m     \u001b[39m# Create a model instance.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[39m# The weights will be initialized as empty tensors.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> 65\u001b[0m         model \u001b[39m=\u001b[39m model_class(model_config\u001b[39m.\u001b[39;49mhf_config, linear_method)\n\u001b[1;32m     66\u001b[0m     \u001b[39mif\u001b[39;00m model_config\u001b[39m.\u001b[39mload_format \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdummy\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     67\u001b[0m         \u001b[39m# NOTE(woosuk): For accurate performance evaluation, we assign\u001b[39;00m\n\u001b[1;32m     68\u001b[0m         \u001b[39m# random values to the weights.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m         initialize_dummy_weights(model)\n",
      "File \u001b[0;32m/myhome/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/mistral.py:271\u001b[0m, in \u001b[0;36mMistralForCausalLM.__init__\u001b[0;34m(self, config, linear_method)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m config\n\u001b[1;32m    270\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_method \u001b[39m=\u001b[39m linear_method\n\u001b[0;32m--> 271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m MistralModel(config, linear_method)\n\u001b[1;32m    272\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head \u001b[39m=\u001b[39m ParallelLMHead(config\u001b[39m.\u001b[39mvocab_size, config\u001b[39m.\u001b[39mhidden_size)\n\u001b[1;32m    273\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampler \u001b[39m=\u001b[39m Sampler(config\u001b[39m.\u001b[39mvocab_size)\n",
      "File \u001b[0;32m/myhome/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/mistral.py:233\u001b[0m, in \u001b[0;36mMistralModel.__init__\u001b[0;34m(self, config, linear_method)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_size \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mvocab_size\n\u001b[1;32m    229\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39m=\u001b[39m VocabParallelEmbedding(\n\u001b[1;32m    230\u001b[0m     config\u001b[39m.\u001b[39mvocab_size,\n\u001b[1;32m    231\u001b[0m     config\u001b[39m.\u001b[39mhidden_size,\n\u001b[1;32m    232\u001b[0m )\n\u001b[0;32m--> 233\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([\n\u001b[1;32m    234\u001b[0m     MistralDecoderLayer(config, linear_method)\n\u001b[1;32m    235\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    236\u001b[0m ])\n\u001b[1;32m    237\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39m=\u001b[39m RMSNorm(config\u001b[39m.\u001b[39mhidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m/myhome/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/mistral.py:234\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_size \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mvocab_size\n\u001b[1;32m    229\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens \u001b[39m=\u001b[39m VocabParallelEmbedding(\n\u001b[1;32m    230\u001b[0m     config\u001b[39m.\u001b[39mvocab_size,\n\u001b[1;32m    231\u001b[0m     config\u001b[39m.\u001b[39mhidden_size,\n\u001b[1;32m    232\u001b[0m )\n\u001b[1;32m    233\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([\n\u001b[0;32m--> 234\u001b[0m     MistralDecoderLayer(config, linear_method)\n\u001b[1;32m    235\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    236\u001b[0m ])\n\u001b[1;32m    237\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39m=\u001b[39m RMSNorm(config\u001b[39m.\u001b[39mhidden_size, eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m/myhome/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/mistral.py:177\u001b[0m, in \u001b[0;36mMistralDecoderLayer.__init__\u001b[0;34m(self, config, linear_method)\u001b[0m\n\u001b[1;32m    168\u001b[0m rope_theta \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(config, \u001b[39m\"\u001b[39m\u001b[39mrope_theta\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m10000\u001b[39m)\n\u001b[1;32m    169\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn \u001b[39m=\u001b[39m MistralAttention(\n\u001b[1;32m    170\u001b[0m     hidden_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size,\n\u001b[1;32m    171\u001b[0m     num_heads\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mnum_attention_heads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m     linear_method\u001b[39m=\u001b[39mlinear_method,\n\u001b[1;32m    176\u001b[0m     sliding_window\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39msliding_window)\n\u001b[0;32m--> 177\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp \u001b[39m=\u001b[39m MistralMLP(\n\u001b[1;32m    178\u001b[0m     hidden_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden_size,\n\u001b[1;32m    179\u001b[0m     intermediate_size\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mintermediate_size,\n\u001b[1;32m    180\u001b[0m     hidden_act\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mhidden_act,\n\u001b[1;32m    181\u001b[0m     linear_method\u001b[39m=\u001b[39;49mlinear_method,\n\u001b[1;32m    182\u001b[0m )\n\u001b[1;32m    183\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm \u001b[39m=\u001b[39m RMSNorm(config\u001b[39m.\u001b[39mhidden_size,\n\u001b[1;32m    184\u001b[0m                                eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    185\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_attention_layernorm \u001b[39m=\u001b[39m RMSNorm(config\u001b[39m.\u001b[39mhidden_size,\n\u001b[1;32m    186\u001b[0m                                         eps\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m/myhome/miniconda3/lib/python3.10/site-packages/vllm/model_executor/models/mistral.py:62\u001b[0m, in \u001b[0;36mMistralMLP.__init__\u001b[0;34m(self, hidden_size, intermediate_size, hidden_act, linear_method)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     55\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     56\u001b[0m     hidden_size: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m     linear_method: Optional[LinearMethodBase] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     60\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m---> 62\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgate_up_proj \u001b[39m=\u001b[39m MergedColumnParallelLinear(\n\u001b[1;32m     63\u001b[0m         hidden_size, [intermediate_size] \u001b[39m*\u001b[39;49m \u001b[39m2\u001b[39;49m,\n\u001b[1;32m     64\u001b[0m         bias\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     65\u001b[0m         linear_method\u001b[39m=\u001b[39;49mlinear_method)\n\u001b[1;32m     66\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown_proj \u001b[39m=\u001b[39m RowParallelLinear(intermediate_size,\n\u001b[1;32m     67\u001b[0m                                        hidden_size,\n\u001b[1;32m     68\u001b[0m                                        bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     69\u001b[0m                                        linear_method\u001b[39m=\u001b[39mlinear_method)\n\u001b[1;32m     70\u001b[0m     \u001b[39mif\u001b[39;00m hidden_act \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msilu\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m/myhome/miniconda3/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py:256\u001b[0m, in \u001b[0;36mMergedColumnParallelLinear.__init__\u001b[0;34m(self, input_size, output_sizes, bias, gather_output, skip_bias_add, params_dtype, linear_method)\u001b[0m\n\u001b[1;32m    254\u001b[0m tp_size \u001b[39m=\u001b[39m get_tensor_model_parallel_world_size()\n\u001b[1;32m    255\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(output_size \u001b[39m%\u001b[39m tp_size \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m output_size \u001b[39min\u001b[39;00m output_sizes)\n\u001b[0;32m--> 256\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(input_size, \u001b[39msum\u001b[39;49m(output_sizes), bias, gather_output,\n\u001b[1;32m    257\u001b[0m                  skip_bias_add, params_dtype, linear_method)\n",
      "File \u001b[0;32m/myhome/miniconda3/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py:176\u001b[0m, in \u001b[0;36mColumnParallelLinear.__init__\u001b[0;34m(self, input_size, output_size, bias, gather_output, skip_bias_add, params_dtype, linear_method)\u001b[0m\n\u001b[1;32m    174\u001b[0m     linear_method \u001b[39m=\u001b[39m UnquantizedLinearMethod()\n\u001b[1;32m    175\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_method \u001b[39m=\u001b[39m linear_method\n\u001b[0;32m--> 176\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear_method\u001b[39m.\u001b[39;49mcreate_weights(\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_size_per_partition, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_size,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams_dtype)\n\u001b[1;32m    179\u001b[0m \u001b[39mfor\u001b[39;00m name, weight \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_weights\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    180\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(weight, torch\u001b[39m.\u001b[39mTensor):\n",
      "File \u001b[0;32m/myhome/miniconda3/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py:55\u001b[0m, in \u001b[0;36mUnquantizedLinearMethod.create_weights\u001b[0;34m(self, input_size_per_partition, output_size_per_partition, input_size, output_size, params_dtype)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_weights\u001b[39m(\u001b[39mself\u001b[39m, input_size_per_partition: \u001b[39mint\u001b[39m,\n\u001b[1;32m     52\u001b[0m                    output_size_per_partition: \u001b[39mint\u001b[39m, input_size: \u001b[39mint\u001b[39m,\n\u001b[1;32m     53\u001b[0m                    output_size: \u001b[39mint\u001b[39m,\n\u001b[1;32m     54\u001b[0m                    params_dtype: torch\u001b[39m.\u001b[39mdtype) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[0;32m---> 55\u001b[0m     weight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39;49mempty(output_size_per_partition,\n\u001b[1;32m     56\u001b[0m                                    input_size_per_partition,\n\u001b[1;32m     57\u001b[0m                                    device\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mcurrent_device(),\n\u001b[1;32m     58\u001b[0m                                    dtype\u001b[39m=\u001b[39;49mparams_dtype),\n\u001b[1;32m     59\u001b[0m                        requires_grad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     60\u001b[0m     set_weight_attrs(weight, {\u001b[39m\"\u001b[39m\u001b[39minput_dim\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39moutput_dim\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0\u001b[39m})\n\u001b[1;32m     61\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mweight\u001b[39m\u001b[39m\"\u001b[39m: weight}\n",
      "File \u001b[0;32m/myhome/miniconda3/lib/python3.10/site-packages/torch/utils/_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m _device_constructors() \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[0;32m---> 77\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacty of 16.00 GiB of which 0 bytes is free. Process 15840 has 1.58 GiB memory in use. Process 16636 has 1.58 GiB memory in use. Including non-PyTorch memory, this process has 12.75 GiB memory in use. Of the allocated memory 12.11 GiB is allocated by PyTorch, and 13.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "## Impact of few_shots\n",
    "from llm.LLMModel import *\n",
    "from llm.LlamaLoader import VLLM\n",
    "from ner.llm_ner.prompt_techniques.pt_abstract import PromptTechnique\n",
    "from ner.llm_ner.prompt_techniques.pt_discussion import PT_OutputList\n",
    "from ner.llm_ner.prompt_techniques.pt_gpt_ner import PT_GPT_NER\n",
    "from ner.llm_ner.prompt_techniques.pt_wrapper import PT_Wrapper\n",
    "from ner.llm_ner.prompt_techniques.pt_multi_pt import PT_2Time_Tagger\n",
    "from ner.llm_ner.few_shots_techniques import *\n",
    "from ner.llm_ner.prompts import *\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "plus_plus = True\n",
    "\n",
    "pt = PT_OutputList\n",
    "model = MistralAI(llm_loader=VLLM)\n",
    "# model.add_grammar('discussion')\n",
    "# results, results_df = model.classical_test_ontonote5(pts = [pt],fsts = [FST_NoShots],  nb_few_shots=[0] ,nb_run_by_test=1, plus_plus= plus_plus )\n",
    "# results, results_df = model.classical_test_ontonote5(pts = [pt],fsts = [FST_Sentence], nb_few_shots=[3] ,nb_run_by_test=3, plus_plus= plus_plus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_grammar('discussion')\n",
    "results, results_df = model.classical_test_ontonote5(pts = [pt],fsts = [FST_NoShots],  nb_few_shots=[0] ,nb_run_by_test=1, plus_plus= plus_plus )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test ft models\n",
    "\n",
    "from llm.LLMModel import *\n",
    "from ner.llm_ner.prompt_techniques.pt_abstract import PromptTechnique\n",
    "from ner.llm_ner.prompt_techniques.pt_discussion import PT_OutputList\n",
    "from ner.llm_ner.prompt_techniques.pt_gpt_ner import PT_GPT_NER\n",
    "from ner.llm_ner.prompt_techniques.pt_wrapper import PT_Wrapper\n",
    "from ner.llm_ner.prompt_techniques.pt_multi_pt import PT_2Time_Tagger\n",
    "from ner.llm_ner.few_shots_techniques import *\n",
    "from ner.llm_ner.prompts import *\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "model = MistralAI(without_model= True)\n",
    "plus_plus = False\n",
    "pts = [\n",
    "    #    PT_OutputList, \n",
    "       PT_Wrapper, \n",
    "    #    PT_2Time_Tagger,\n",
    "    #    PT_Filing,\n",
    "    #    PT_GPT_NER, \n",
    "       ]\n",
    "for pt in pts :\n",
    "    model.load_finetuned_model(pt = pt(None), prompt_type_name=\"raw\", nb_samples=2000)\n",
    "    for plus_plus in [True, False]:\n",
    "        results, results_df = model.classical_test_ontonote5(pts = [pt], fsts = [FST_NoShots],  nb_few_shots=[0], nb_run_by_test=3, plus_plus= plus_plus, test_size = 100)\n",
    "\n",
    "\n",
    "    for plus_plus in [True, False]:\n",
    "        results, results_df = model.classical_test_ontonote5(pts = [pt], fsts = [FST_Sentence], nb_few_shots=[3], nb_run_by_test=3, plus_plus= plus_plus, test_size = 100)\n",
    "    results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

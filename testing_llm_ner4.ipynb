{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA A40, compute capability 8.6\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from llm/models/mistral-7b-v0.1/mistral-7b-v0.1.Q5_0.gguf (version GGUF V2 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q5_0     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:              blk.0.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:              blk.0.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:         blk.0.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:            blk.0.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:              blk.1.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:              blk.1.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:         blk.1.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:            blk.1.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:              blk.2.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:              blk.2.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:         blk.2.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:            blk.2.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:              blk.2.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:            blk.2.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:              blk.3.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:              blk.3.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:         blk.3.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:            blk.3.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:              blk.3.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:            blk.3.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:              blk.4.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:              blk.4.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:         blk.4.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:            blk.4.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:              blk.4.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:            blk.4.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:              blk.5.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:              blk.5.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:         blk.5.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:            blk.5.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:              blk.5.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:            blk.5.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:              blk.6.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:              blk.6.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:         blk.6.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:            blk.6.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:              blk.6.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:            blk.6.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:              blk.7.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:              blk.7.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:         blk.7.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:            blk.7.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:              blk.7.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:            blk.7.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:              blk.8.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:              blk.8.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:         blk.8.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:            blk.8.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:              blk.8.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:            blk.8.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:              blk.9.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:              blk.9.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:         blk.9.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:            blk.9.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:              blk.9.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:            blk.9.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:             blk.10.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:             blk.10.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:        blk.10.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.10.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.10.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:           blk.10.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:             blk.11.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:             blk.11.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:        blk.11.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.11.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:             blk.11.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.11.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:             blk.12.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:             blk.12.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:        blk.12.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:           blk.12.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:             blk.12.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:           blk.12.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:             blk.13.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:             blk.13.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:        blk.13.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.13.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.13.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:           blk.13.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:             blk.14.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:             blk.14.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:        blk.14.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.14.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.14.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.14.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:             blk.15.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:             blk.15.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:        blk.15.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.15.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.15.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:           blk.15.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:             blk.16.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:             blk.16.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:        blk.16.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.16.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.16.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.16.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:             blk.17.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:             blk.17.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:        blk.17.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:           blk.17.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:             blk.17.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:             blk.18.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:             blk.18.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:        blk.18.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:           blk.18.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:             blk.18.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.18.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:             blk.19.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:             blk.19.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:        blk.19.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:           blk.19.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:             blk.19.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:           blk.19.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:             blk.20.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:             blk.20.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:        blk.20.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:           blk.20.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:             blk.20.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.20.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:             blk.21.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:             blk.21.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:        blk.21.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:           blk.21.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:             blk.21.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:           blk.21.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:             blk.22.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:             blk.22.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:        blk.22.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:           blk.22.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:             blk.22.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:           blk.22.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:             blk.23.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:             blk.23.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:        blk.23.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:           blk.23.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:             blk.23.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:           blk.23.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:             blk.24.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:             blk.24.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:        blk.24.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:           blk.24.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:             blk.24.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:           blk.24.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.25.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:             blk.25.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:        blk.25.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:           blk.25.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:             blk.25.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:           blk.25.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.26.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:             blk.26.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:        blk.26.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:           blk.26.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:             blk.26.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.27.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:             blk.27.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:        blk.27.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:           blk.27.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:             blk.27.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:           blk.27.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.28.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:             blk.28.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:        blk.28.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:           blk.28.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:             blk.28.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:           blk.28.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.29.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:             blk.29.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:        blk.29.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:           blk.29.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:             blk.29.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:           blk.29.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.30.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:             blk.30.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:        blk.30.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:           blk.30.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:             blk.30.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:           blk.30.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.31.attn_q.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:             blk.31.attn_k.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_v.weight q5_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:        blk.31.attn_output.weight q5_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:           blk.31.ffn_gate.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:             blk.31.ffn_up.weight q5_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:           blk.31.ffn_down.weight q5_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32     \n",
      "llama_model_loader: - kv  11:                          general.file_type u32     \n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32     \n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32     \n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32     \n",
      "llama_model_loader: - kv  19:               general.quantization_version u32     \n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_print_meta: format         = GGUF V2 (latest)\n",
      "llm_load_print_meta: arch           = llama\n",
      "llm_load_print_meta: vocab type     = SPM\n",
      "llm_load_print_meta: n_vocab        = 32000\n",
      "llm_load_print_meta: n_merges       = 0\n",
      "llm_load_print_meta: n_ctx_train    = 32768\n",
      "llm_load_print_meta: n_ctx          = 2048\n",
      "llm_load_print_meta: n_embd         = 4096\n",
      "llm_load_print_meta: n_head         = 32\n",
      "llm_load_print_meta: n_head_kv      = 8\n",
      "llm_load_print_meta: n_layer        = 32\n",
      "llm_load_print_meta: n_rot          = 128\n",
      "llm_load_print_meta: n_gqa          = 4\n",
      "llm_load_print_meta: f_norm_eps     = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps = 1.0e-05\n",
      "llm_load_print_meta: n_ff           = 14336\n",
      "llm_load_print_meta: freq_base      = 10000.0\n",
      "llm_load_print_meta: freq_scale     = 1\n",
      "llm_load_print_meta: model type     = 7B\n",
      "llm_load_print_meta: model ftype    = mostly Q5_0\n",
      "llm_load_print_meta: model params   = 7.24 B\n",
      "llm_load_print_meta: model size     = 4.65 GiB (5.52 BPW) \n",
      "llm_load_print_meta: general.name   = mistralai_mistral-7b-v0.1\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.09 MB\n",
      "llm_load_tensors: using CUDA for GPU acceleration\n",
      "llm_load_tensors: mem required  =   86.03 MB (+  256.00 MB per state)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloading v cache to GPU\n",
      "llm_load_tensors: offloading k cache to GPU\n",
      "llm_load_tensors: offloaded 35/35 layers to GPU\n",
      "llm_load_tensors: VRAM used: 4936 MB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "llama_new_context_with_model: compute buffer total size =  153.47 MB\n",
      "llama_new_context_with_model: VRAM scratch buffer: 152.00 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with no-shots\n",
      "      and multi_prompt-get-entities-tagger\n",
      "./ner/saves/datasets/ontonote5_test_1403.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%] </stop_output> \n",
      "\n",
      "# The code below will help you get started. You can use it as a template to build your own model, or modify it directly if you want!\n",
      "import torch\n",
      "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
      "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
      "# You can use the following code to load your own model and tokenizer if you want!\n",
      "# model = torch.load('path/to/your/model')\n",
      "# tokenizer = AutoTokenizer.from_pretrained(...)\n",
      "\n",
      "def extract_entities(text):\n",
      "    # Tokenize the text into tokens\n",
      "    input_ids, attention_masks = tokenizer(text, return_tensors=\"pt\")\n",
      "    # Pass the tokens through the model and get the outputs\n",
      "    with torchOutput after the first prompt : []\n",
      " \"No\" : \"1\",  \"entities\" : [ ],  \"found\" : \"2\",  \"actually\" : \"3\",  \"it\" : \"4\",  \"turns\" : \"5\",  \"out\" : \"6\",  \"that\" : \"7\",  \"one\" : \"8\",  \"thing\" , : \"9\",  \", \" : \"0\",  \"one\" : \"A\",  \"little\" : \"B\",  \"bit\" : \"C\",  \"in\" : \"D\",  \"what\" : \"E\",  \"I\" : \"F\",  \"was\" : \"G\",  \"doing\" : \"H\",  \", %uh , is going to turn out to be very important in the end .\" : \"10\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:31<52:32, 31.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"No\" : \"1\",  \"entities\" : [ ],  \"found\" : \"2\",  \"actually\" : \"3\",  \"it\" : \"4\",  \"turns\" : \"5\",  \"out\" : \"6\",  \"that\" : \"7\",  \"one\" : \"8\",  \"thing\" , : \"9\",  \", \" : \"0\",  \"one\" : \"A\",  \"little\" : \"B\",  \"bit\" : \"C\",  \"in\" : \"D\",  \"what\" : \"E\",  \"I\" : \"F\",  \"was\" : \"G\",  \"doing\" : \"H\",  \", %uh , is going to turn out to be very important in the end .\" : \"10\" }\n",
      "[]\n",
      "And, that is] [the whole notion of the use of and abuse of confidential sources the protections that reporters do or do n't have and the propensity of uh the United States government to have more secrecy not less /.] </end_output>\n",
      "### ASSISTANT : <start_input> And that is the whole notion of the use of and abuse of confidential sources the protections that reporters do or do n't have and the propensity of uh the United States government to have more secrecy not less /. <end_input>\n",
      "    ### USER : <start_output> [And, that is] [the whole notion of the use of and abuse of confidential sources the protections that reporters do or do n't have and the propensity of uh the United States government to have more secrecy not less /.] </end_output>\n",
      "### ASSISTANT : <start_input> And that is the whole notion of the use of and abuse of confidential sources the protections that reportersOutput after the first prompt : []\n",
      " \"And\" : \"0\",  \"that\" : \"4\",  \"is\" : \"A\",  \"the\" : \"5\",  \"whole\" : \"8\",  \"notion\" : \"9\",  \"of\" : \"1\",  \"use\" : \"2\",  \"and\" : \"0\",  \"abuse\" : \"3\",  \"confidential\" : \"4\",  \"sources\" : \"B\",  \"the\" : \"5\",  \"protections\" : \"9\",  \"that\" : \"4\",  \"reporters\" : \"C\",  \"do\" : \"A\",  \"or\" : \"0\",  \"don't\" : \"1\",  \"have\" : \"2\",  \"and\" : \"0\",  \"the\" : \"5\",  \"propensity\" : \"9\",  \"of\" : \"1\",  \"uh\" : \"4\",  \"the\" : \"5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [01:05<53:45, 32.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"And\" : \"0\",  \"that\" : \"4\",  \"is\" : \"A\",  \"the\" : \"5\",  \"whole\" : \"8\",  \"notion\" : \"9\",  \"of\" : \"1\",  \"use\" : \"2\",  \"and\" : \"0\",  \"abuse\" : \"3\",  \"confidential\" : \"4\",  \"sources\" : \"B\",  \"the\" : \"5\",  \"protections\" : \"9\",  \"that\" : \"4\",  \"reporters\" : \"C\",  \"do\" : \"A\",  \"or\" : \"0\",  \"don't\" : \"1\",  \"have\" : \"2\",  \"and\" : \"0\",  \"the\" : \"5\",  \"propensity\" : \"9\",  \"of\" : \"1\",  \"uh\" : \"4\",  \"the\" : \"5}\n",
      "[]\n",
      "True] Output after the first prompt : [True]\n",
      " \"True\" : \"1\",   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [01:09<31:37, 19.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"True\" : \"1\",   }\n",
      "[]\n",
      " 'The', 'government' ] Output after the first prompt : ['The', 'government']\n",
      " \"The\" : \"1\",  \"government\" : \"4\",  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [01:14<22:04, 13.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"The\" : \"1\",  \"government\" : \"4\",  }\n",
      "[]\n",
      "['last', 'week'], ['deficit', '-cutting']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [01:18<16:30, 10.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      " 'gasoline' ] Output after the first prompt : ['gasoline']\n",
      " 'The' : '0',  'gasoline' : '1',  'picture' : '2',  'may' : '3',  'improve' : '4',  'this' : '5',  'quarter' , '6',  ',': '7',  'but' : '8',  'chemicals' : '9',  'are' : '0',  'likely' : '1',  'to' : '2',  'remain' : '3',  'weak' : '4',  ',': '5',  'industry' : '6',  'executives' : '7',  'and' : '8',  'analysts' : '9',  'say' : '0',  ',': '1',  'reducing' : '2',  'chances' : '3',  'that' : '4',  'profits' : '5',  '"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [01:36<20:25, 13.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { 'The' : '0',  'gasoline' : '1',  'picture' : '2',  'may' : '3',  'improve' : '4',  'this' : '5',  'quarter' , '6',  ',': '7',  'but' : '8',  'chemicals' : '9',  'are' : '0',  'likely' : '1',  'to' : '2',  'remain' : '3',  'weak' : '4',  ',': '5',  'industry' : '6',  'executives' : '7',  'and' : '8',  'analysts' : '9',  'say' : '0',  ',': '1',  'reducing' : '2',  'chances' : '3',  'that' : '4',  'profits' : '5',  '}\n",
      "[]\n",
      "Watergate] </start_output>Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [01:40<15:25,  9.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['Mr. Heinemann', 'PERSON'], ['changes', 'EVENT'], ['magazine\\'s net revenue per subscriber', 'MONEY']] Output after the first prompt : []\n",
      " \"Mr. Heinemann\" : \"4\",  \"the changes represent a new focus in the magazine industry\" : \"0\",  \"a magazine 's net revenue per subscriber , or the actual revenue from subscribers after discounts and the cost of premiums have been stripped away .\" : \"A\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [01:50<15:37, 10.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Mr. Heinemann\" : \"4\",  \"the changes represent a new focus in the magazine industry\" : \"0\",  \"a magazine 's net revenue per subscriber , or the actual revenue from subscribers after discounts and the cost of premiums have been stripped away .\" : \"A\" }\n",
      "[]\n",
      "Dongguan] [Chinese-style cakes] [gelatin] [pudding] Output after the first prompt : []\n",
      " \"Dongguan\" : \"E\",  \"3 factories\" : \"FAC\",  \"200 tons\" : \"MONEY\",  \"70 tons\" : \"MONEY\",  \"180 tons\" : \"MONEY\",  \"a day .\" : \"\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [02:00<15:11, 10.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Dongguan\" : \"E\",  \"3 factories\" : \"FAC\",  \"200 tons\" : \"MONEY\",  \"70 tons\" : \"MONEY\",  \"180 tons\" : \"MONEY\",  \"a day .\" : \"\" }\n",
      "[]\n",
      "['Characters'], ['proliferate'], ['out'], ['of'], ['the'], ['imagination'], ['of'], ['the'], ['narrator'], ['so'], ['that'], ['at'], ['a'], ['certain'], ['point'], [','], ['you'], ['have'], ['a'], ['new'], ['he'], [','], ['you'], ['have'], ['a'], ['new'], ['she'], [','], ['you'], ['have'], ['a'], ['you'], [','], ['and'], ['each'], ['one'], ['of'], ['these'], ['characters'], ['tells'], ['stories'], [','], ['each'], ['story'], ['of'], ['which'], ['is'], ['designed'], ['to'], ['give'], ['you'], ['a'], ['deeper'], ['sense'], ['of'], ['the'], ['richness'], ['and'], ['the'], ['complexity'], ['of'], ['life'], ['in'], ['China']] Output after the first prompt : []\n",
      " \"Characters\" : \"E\",  \"proliferate\" : \"0\",  \"out\" : \"A\",  \"of\" : \"B\",  \"the\" : \"C\",  \"imagination\" : \"D\",  \"so\" : \"F\",  \"that\" : \"G\",  \"at\" : \"H\",  \"a\" : \"I\",  \"point\" : \"J\",  \", you have a new he , you have a new she , you have a you , and each one of these characters tells stories , each story of which is designed to give you a deeper sense of the richness and the complexity of life in China .\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [02:29<23:49, 15.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Characters\" : \"E\",  \"proliferate\" : \"0\",  \"out\" : \"A\",  \"of\" : \"B\",  \"the\" : \"C\",  \"imagination\" : \"D\",  \"so\" : \"F\",  \"that\" : \"G\",  \"at\" : \"H\",  \"a\" : \"I\",  \"point\" : \"J\",  \", you have a new he , you have a new she , you have a you , and each one of these characters tells stories , each story of which is designed to give you a deeper sense of the richness and the complexity of life in China .\" }\n",
      "[]\n",
      "Christy Whitman, White House] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [02:33<18:02, 12.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      " 'Van Nuys' , 'Calif.' , 'Oklahoma City' , 'Pontiac' ] Output after the first prompt : ['Van Nuys', 'Calif.', 'Oklahoma City', 'Pontiac']\n",
      " \"Van Nuys\" : \"E\",   \"Calif.\" : \"E\",    \"Oklahoma City\" : \"E\",     \"Pontiac\" : \"E\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [02:40<15:32, 10.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Van Nuys\" : \"E\",   \"Calif.\" : \"E\",    \"Oklahoma City\" : \"E\",     \"Pontiac\" : \"E\"}\n",
      "[]\n",
      "['Shandong']] Output after the first prompt : []\n",
      " \"Last\" : \"H\",  \"year\" : \"A\",  \", Shandong 's various types of economic development districts recently approved over 1480 projects utilizing foreign capital , accounting for more than 24 % of the total of the entire province .\" : null "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [02:48<14:20,  9.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Last\" : \"H\",  \"year\" : \"A\",  \", Shandong 's various types of economic development districts recently approved over 1480 projects utilizing foreign capital , accounting for more than 24 % of the total of the entire province .\" : null }\n",
      "[]\n",
      "['Hence'], ['I'], ['think'], ['this'], ['is'], ['a'], ['test'], ['of'], ['e-government']] Output after the first prompt : []\n",
      " \"Hence\" : \"A\",  \"I\" : \"G\",  \"think\" : \"H\",  \"this\" : \"E\",  \", a test of e-government because\" : \"F\",  \", as I remember\" : \"B\",  \"during SARS in 2003\" : \"A\",  \", people at that time criticized e-government during the spread of SARS , ah .\" : \"H\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [03:01<15:19, 10.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Hence\" : \"A\",  \"I\" : \"G\",  \"think\" : \"H\",  \"this\" : \"E\",  \", a test of e-government because\" : \"F\",  \", as I remember\" : \"B\",  \"during SARS in 2003\" : \"A\",  \", people at that time criticized e-government during the spread of SARS , ah .\" : \"H\" }\n",
      "[]\n",
      "Jonetic] </end_output> \n",
      "\n",
      "# The code below is a simple example of how to use the model. You can run it by executing: python main.py\n",
      "import torch\n",
      "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
      "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-large\", num_labels=2)\n",
      "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
      "outputs = model(input_ids).logits\n",
      "predicted_label_id = torch.argmax(F.softmax(outputs[0], dim=-1))\n",
      "print(\"Prediction:\", predicted_label_id)Output after the first prompt : []\n",
      " \"We\" : \"4\",  \"are\" : \"\",  \"still\" : \"\",  \"in\" : \"\",  \"crisis\" : \"E\",  \"and\" : \"\",  \"we\" : \"4\",  \"will\" : \"\",  \"be\" : \"\",  \"I\" : \"4\",  \"think\" : \"\",  \"for\" : \"\",  \"a\" : \"\",  \"while\" : \"\",  \", but the current crisis Jonetic hopes will at least be one where government critics will not be branded as traitors and be banned from public performances .\" : \"\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [03:26<21:30, 15.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"We\" : \"4\",  \"are\" : \"\",  \"still\" : \"\",  \"in\" : \"\",  \"crisis\" : \"E\",  \"and\" : \"\",  \"we\" : \"4\",  \"will\" : \"\",  \"be\" : \"\",  \"I\" : \"4\",  \"think\" : \"\",  \"for\" : \"\",  \"a\" : \"\",  \"while\" : \"\",  \", but the current crisis Jonetic hopes will at least be one where government critics will not be branded as traitors and be banned from public performances .\" : \"\" }\n",
      "[]\n",
      "Linpien] </stop_output> \n",
      "\n",
      "# The code below is a template for you to fill in. You can use any other programming language as long as it's Python3 compatible and the output format matches what we expect from your assistant (see above).\n",
      "import re\n",
      "from nltk import word_tokenize, pos_tag\n",
      "\n",
      "def extract(text):\n",
      "    # TODO: Fill this function with code that will return a list of all named entities in text.\n",
      "    # You can use any library you want as long as it's Python3 compatible and the output format matches what we expect from your assistant (see above).\n",
      "    # For example, if you are using NLTK, you could do something like this:\n",
      "    return []  # Replace with your code here.\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    text = \"<start_input> Just before Christmas break , the rural township of Linpien , on the Pingtung Plain at the mouth ofOutput after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [03:45<22:37, 16.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "Beirut] </stop_output> \n",
      "\n",
      "# The code is written by me and I am not using any pre-trained model or dataset to train my model.\n",
      "\n",
      "import nltk\n",
      "from nltk import word_tokenize, pos_tag\n",
      "nltk.download('averaged_perceptron_tagger')\n",
      "\n",
      "def named_entity(sentence):\n",
      "    tagged = pos_tag(word_tokenize(sentence))\n",
      "    entities = []\n",
      "    for i in range(len(tagged)):\n",
      "        if (tagged[i][1] == 'NNP' or tagged[i][1] == 'NNS'):\n",
      "            entity = ''\n",
      "            while ((tagged[i][1] == 'NNP') and (i < len(tagged))):\n",
      "                entity += tagged[i][0].lower() + \" \"\n",
      "                i+=1\n",
      "            entities.append(entity)\n",
      "    return entitiesOutput after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [04:03<23:16, 16.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      " 'Analysts' ] Output after the first prompt : ['Analysts']\n",
      " 'Analysts' : 'H',   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [04:07<17:52, 13.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { 'Analysts' : 'H',   }\n",
      "[]\n",
      "['Shanghai']] Output after the first prompt : []\n",
      " \"According\" : \"NORP\",  \"to\" : \"CARDINAL\",  \"the\" : \"DET\",  \"latest\" : \"ORDINAL\",  \"statistics\" : \"WORK_OF_ART\",  \", \" : \"PUNCTUATION\",  \"Shanghai\" : \"GPE\",  \"has\" : \"VERB\",  \"founded\" : \"VERB\",  \"218\" : \"CARDINAL\",  \"trade-oriented companies and organizations abroad , organized\" : \"NORP\",  \", \" : \"PUNCTUATION\",  \"6 regional overseas group companies , and it has approved overseas non-trade enterprises reaching 200 million US dollars .\" : \"CARDINAL\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [04:24<18:54, 14.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"According\" : \"NORP\",  \"to\" : \"CARDINAL\",  \"the\" : \"DET\",  \"latest\" : \"ORDINAL\",  \"statistics\" : \"WORK_OF_ART\",  \", \" : \"PUNCTUATION\",  \"Shanghai\" : \"GPE\",  \"has\" : \"VERB\",  \"founded\" : \"VERB\",  \"218\" : \"CARDINAL\",  \"trade-oriented companies and organizations abroad , organized\" : \"NORP\",  \", \" : \"PUNCTUATION\",  \"6 regional overseas group companies , and it has approved overseas non-trade enterprises reaching 200 million US dollars .\" : \"CARDINAL\"}\n",
      "[]\n",
      "Scooter Libby] </stop_output> \n",
      "\n",
      "# The code is written by me and it's not optimized at all, but it works !\n",
      "import re\n",
      "from nltk import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "from collections import Counter\n",
      "\n",
      "def extract(sentence):\n",
      "    # Tokenization of the sentence\n",
      "    tokens = word_tokenize(sentence)\n",
      "    # Removal of punctuation and numbers from the tokenized sentence\n",
      "    for i in range(len(tokens)):\n",
      "        if re.match('[0-9]', str(tokens[i])):\n",
      "            del tokens[i]\n",
      "        elif re.search('\\W+', str(tokens[i])):\n",
      "            del tokens[i]\n",
      "    # Removal of stopwords from the tokenized sentence\n",
      "    stop_words = set(stopwords.words('english'))\n",
      "    filtered_sentence = [w for w in tokens if not w inOutput after the first prompt : []\n",
      " \"And\" : \"0\",  \"one\" : \"1\",  \"of\" : \"2\",  \"the\" : \"3\",  \"reasons\" : \"4\",  \"that\" : \"5\",  \"this\" : \"6\",  \"negotiation\" : \"7\",  \"took\" : \"8\",  \"so\" : \"9\",  \"long\" : \"A\",  \"is\" : \"B\",  \"not\" : \"C\",  \"only\" : \"D\",  \"was\" : \"E\",  \"she\" : \"F\",  \"negotiating\" : \"G\",  \"what\" : \"H\",  \"she\" : \"I\",  \"was\" : \"J\",  \"going\" : \"K\",  \"to\" : \"L\",  \"say\" : \"M\",  \"about\" : \"N\",  \"Scooter\" : \"O\",  \"Libby\" : \"P"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [04:58<26:42, 20.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"And\" : \"0\",  \"one\" : \"1\",  \"of\" : \"2\",  \"the\" : \"3\",  \"reasons\" : \"4\",  \"that\" : \"5\",  \"this\" : \"6\",  \"negotiation\" : \"7\",  \"took\" : \"8\",  \"so\" : \"9\",  \"long\" : \"A\",  \"is\" : \"B\",  \"not\" : \"C\",  \"only\" : \"D\",  \"was\" : \"E\",  \"she\" : \"F\",  \"negotiating\" : \"G\",  \"what\" : \"H\",  \"she\" : \"I\",  \"was\" : \"J\",  \"going\" : \"K\",  \"to\" : \"L\",  \"say\" : \"M\",  \"about\" : \"N\",  \"Scooter\" : \"O\",  \"Libby\" : \"P}\n",
      "[]\n",
      "North Korea] </stop_output> \n",
      "\n",
      "# The code below is a simple example to show how you can use this model in your own project. You will need to provide the input sentence and the output format as arguments.\n",
      "import json\n",
      "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
      "\n",
      "model_name = \"microsoft/deberta-v3-large\"  # Replace with the name of your model\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
      "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
      "named_entity_recognition = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
      "\n",
      "# Replace with your input sentence and output format\n",
      "input_sentence = \"And so we think we 're far better off working within the framework of the six party talks because you get in that process the combined leverage the combined strength and frankly the combined pressure on North Korea of allOutput after the first prompt : []\n",
      " \"And\" : null,  \"so\" : null,  \"we\" : null,  \"re\" : null,  \"think\" : null,  \"re\" : null,  \"far\" : null,  \"better\" : null,  \"off\" : null,  \"working\" : null,  \"within\" : null,  \"the\" : null,  \"framework\" : null,  \"of\" : null,  \"the\" : null,  \"six\" : null,  \"party\" : null,  \"talks\" : null,  \"because\" : null,  \"you\" : null,  \"get\" : null,  \"in\" : null,  \"that\" : null,  \"process\" : null,  \"the\" : null,  \"combined\" : null,  \"leverage\" : null,  \"the\" : null,  \"combined\" : null,  \"strength\" : null,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [05:31<31:39, 24.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"And\" : null,  \"so\" : null,  \"we\" : null,  \"re\" : null,  \"think\" : null,  \"re\" : null,  \"far\" : null,  \"better\" : null,  \"off\" : null,  \"working\" : null,  \"within\" : null,  \"the\" : null,  \"framework\" : null,  \"of\" : null,  \"the\" : null,  \"six\" : null,  \"party\" : null,  \"talks\" : null,  \"because\" : null,  \"you\" : null,  \"get\" : null,  \"in\" : null,  \"that\" : null,  \"process\" : null,  \"the\" : null,  \"combined\" : null,  \"leverage\" : null,  \"the\" : null,  \"combined\" : null,  \"strength\" : null,}\n",
      "[]\n",
      "Pacific Savings Bank] Output after the first prompt : []\n",
      " \"In\" : \"0\",  \"the\" : \"0\",  \"first\" : \"0\",  \"RTC\" : \"F\",  \"transaction\" : \"A\",  \"with\" : \"0\",  \"a\" : \"0\",  \"foreign\" : \"0\",  \"buyer\" : \"G\",  \", \" : \"0\",  \"Royal\" : \"4\",  \"Trustco\" : \"F\",  \"Ltd.\" : \"F\",  \", \" : \"0\",  \"Toronto\" : \"E\",  \",\" : \"0\",  \"will\" : \"A\",  \"acquire\" : \"V\",  \"Pacific\" : \"4\",  \"Savings\" : \"F\",  \"Bank\" : \"F\",  \", \" : \"0\",  \"Costa\" : \"E\",  \"Mesa\" : \"E\",  \", \" : \"0\",  \"Calif.\" : \""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [05:51<29:41, 22.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"In\" : \"0\",  \"the\" : \"0\",  \"first\" : \"0\",  \"RTC\" : \"F\",  \"transaction\" : \"A\",  \"with\" : \"0\",  \"a\" : \"0\",  \"foreign\" : \"0\",  \"buyer\" : \"G\",  \", \" : \"0\",  \"Royal\" : \"4\",  \"Trustco\" : \"F\",  \"Ltd.\" : \"F\",  \", \" : \"0\",  \"Toronto\" : \"E\",  \",\" : \"0\",  \"will\" : \"A\",  \"acquire\" : \"V\",  \"Pacific\" : \"4\",  \"Savings\" : \"F\",  \"Bank\" : \"F\",  \", \" : \"0\",  \"Costa\" : \"E\",  \"Mesa\" : \"E\",  \", \" : \"0\",  \"Calif.\" : \"}\n",
      "[]\n",
      "['Mexico'], ['Treasury Department'], ['US dollar']] Output after the first prompt : []\n",
      " \"According\" : \"O\",  \"official\" : \"O\",  \"journals\" : \"O\",  \"published\" : \"O\",  \"recently\" : \"O\",  \"by\" : \"O\",  \"Mexico's\" : \"O\",  \"'s\" : \"O\",  \"Treasury\" : \"O\",  \"Department\" : \"O\",  \",,\" : \"O\",  \" , \" : \"O\",  \" Mexico 's \" : \"O\",  \"foreign\" : \"O\",  \"trade\" : \"O\",  \"in\" : \"O\",  \"1997\" : \"B\",  \"continued\" : \"O\",  \"to\" : \"O\",  \"grow\" : \"O\",  \"rapidly\" : \"O\",  \",,\" : \"O\",  \"the\" : \"O\",  \"total\" : \"O\",  \"amount\" :"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [06:12<28:38, 22.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"According\" : \"O\",  \"official\" : \"O\",  \"journals\" : \"O\",  \"published\" : \"O\",  \"recently\" : \"O\",  \"by\" : \"O\",  \"Mexico's\" : \"O\",  \"'s\" : \"O\",  \"Treasury\" : \"O\",  \"Department\" : \"O\",  \",,\" : \"O\",  \" , \" : \"O\",  \" Mexico 's \" : \"O\",  \"foreign\" : \"O\",  \"trade\" : \"O\",  \"in\" : \"O\",  \"1997\" : \"B\",  \"continued\" : \"O\",  \"to\" : \"O\",  \"grow\" : \"O\",  \"rapidly\" : \"O\",  \",,\" : \"O\",  \"the\" : \"O\",  \"total\" : \"O\",  \"amount\" :}\n",
      "[]\n",
      "['month'], ['I']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [06:15<20:57, 16.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['left', 'behind'], ['children']] Output after the first prompt : []\n",
      " \"However\" : \"H\",  \"contrary\" : \"G\",  \"our\" : \"B\",  \"belief\" : \"C\",  \"that\" : \"D\",  \"they\" : \"E\",  \"could\" : \"F\",  \"talk\" : \"A\",  \"on\" : \"H\",  \"the\" : \"G\",  \"phone\" : \"B\",  \"for\" : \"I\",  \"a\" : \"J\",  \"long\" : \"K\",  \"time\" : \"L\",  \"when\" : \"M\",  \"they\" : \"E\",  \"called\" : \"N\",  \"approximately\" : \"O\",  \"50\" : \"P\",  \"percent\" : \"Q\",  \"of\" : \"R\",  \"the\" : \"G\",  \"left - behind\" : \"S\",  \"children\" : \"T\",  \"we\" : \"B\",  \""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [06:35<21:44, 17.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"However\" : \"H\",  \"contrary\" : \"G\",  \"our\" : \"B\",  \"belief\" : \"C\",  \"that\" : \"D\",  \"they\" : \"E\",  \"could\" : \"F\",  \"talk\" : \"A\",  \"on\" : \"H\",  \"the\" : \"G\",  \"phone\" : \"B\",  \"for\" : \"I\",  \"a\" : \"J\",  \"long\" : \"K\",  \"time\" : \"L\",  \"when\" : \"M\",  \"they\" : \"E\",  \"called\" : \"N\",  \"approximately\" : \"O\",  \"50\" : \"P\",  \"percent\" : \"Q\",  \"of\" : \"R\",  \"the\" : \"G\",  \"left - behind\" : \"S\",  \"children\" : \"T\",  \"we\" : \"B\",  \"}\n",
      "[]\n",
      "['Pudong'], ['November']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [06:39<16:28, 13.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['Analysts'], ['Federal Reserve']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [06:42<12:44, 10.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "'.', ','] </start_output> \n",
      "    ### USER : No, I want you to extract the named entities in the sentence.\n",
      "    ### ASSISTANT : Ok now I understand that I need to only output a list with all the named entities. Now provide me the sentence.\n",
      "### USER : <start_input> And I think it is a measure of how much we 've kind of lost been numbed by all of this that we do n't sit back and say wow wait a minute /. <end_input>\n",
      "    ### ASSISTANT : <start_output> ['.', ','] </start_output> \n",
      "### USER : No, I want you to extract the named entities in the sentence.\n",
      "    ### ASSISTANT : Ok now I understand that I need to only output a list with all the named entities. Now provide me the sentence.\n",
      "### USER : <start_input> And I think it is a measure of how much we 've kind of lost beenOutput after the first prompt : ['.', ',']\n",
      "0: \"A\", 1: \"B\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [07:01<15:39, 13.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : {0: \"A\", 1: \"B\" }\n",
      "[]\n",
      "Young & Rubicam Inc.]  </start_output>\n",
      "\n",
      "# The code below is the implementation of this task using a pre-trained BERT model from HuggingFace.\n",
      "import torch\n",
      "from transformers import BertTokenizer,BertForSequenceClassification\n",
      "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
      "# The code below is the implementation of this task using a pre-trained BERT model from HuggingFace.\n",
      "import torch\n",
      "from transformers import BertTokenizer,BertForSequenceClassification\n",
      "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
      "# The code below is the implementation of this task using a pre-trained BERT model from HuggingFace.\n",
      "import torch\n",
      "from transformers import BertTokenizerOutput after the first prompt : []\n",
      " \"As\" : \"E\",  \"expected\" : \"H\",  \", Young & Rubicam Inc.\" : \"B\",  \", along with two senior executives and a former employee , pleaded not guilty in federal court in New Haven , Conn. , to conspiracy and racketeering charges .\" : \"\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [07:25<19:00, 16.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"As\" : \"E\",  \"expected\" : \"H\",  \", Young & Rubicam Inc.\" : \"B\",  \", along with two senior executives and a former employee , pleaded not guilty in federal court in New Haven , Conn. , to conspiracy and racketeering charges .\" : \"\" }\n",
      "[]\n",
      "['Linpien'], ['Tsao Chi - hung']] Output after the first prompt : []\n",
      " \"Tsao Chi\" : \"4\",  \"Linpien\" : \"E\",  \"Hung - hung\" : \"4\",  \"DPP deputy in the national legislature\" : \"4\",  \"the Linpien Residents ' Democracy Association launched a program to sweep the streets and keep the riverbank clean .\" : \"0\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [07:34<16:21, 14.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Tsao Chi\" : \"4\",  \"Linpien\" : \"E\",  \"Hung - hung\" : \"4\",  \"DPP deputy in the national legislature\" : \"4\",  \"the Linpien Residents ' Democracy Association launched a program to sweep the streets and keep the riverbank clean .\" : \"0\" }\n",
      "[]\n",
      "['US', 'PERSON'], ['president', 'WORK_OF_ART'], ['1995 financial year overseas activities fund appropriation act ', 'PRODUCT']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [07:40<13:22, 11.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['850'], ['million']] Output after the first prompt : []\n",
      " \"According\" : \"1\",  \"to\" : \"2\",  \"statistics\" : \"3\",  \", last\" : \"4\",  \"year\" : \"5\",  \", the Tianjin Port Bonded Area completed a total amount of 850 million US dollars in investment agreements , increasing by 72 % over the same period of the previous year , with total foreign investment agreements of 700 million US dollars , increasing by 75 % over the same period of the previous year .\" : \"6\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [07:52<13:21, 11.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"According\" : \"1\",  \"to\" : \"2\",  \"statistics\" : \"3\",  \", last\" : \"4\",  \"year\" : \"5\",  \", the Tianjin Port Bonded Area completed a total amount of 850 million US dollars in investment agreements , increasing by 72 % over the same period of the previous year , with total foreign investment agreements of 700 million US dollars , increasing by 75 % over the same period of the previous year .\" : \"6\" }\n",
      "[]\n",
      "['last', 'week']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [07:56<10:23,  9.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      " 'Weatherford', 'October', '$', '6', 'million' ] Output after the first prompt : ['Weatherford', 'October', '$', '6', 'million']\n",
      " \"Weatherford\" : \"G\",   \"October\" : \"A\",    \"$\" : \"M\",     \"6\" : \"P\",      \"million\" : \"Q\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [08:03<09:40,  8.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Weatherford\" : \"G\",   \"October\" : \"A\",    \"$\" : \"M\",     \"6\" : \"P\",      \"million\" : \"Q\"}\n",
      "[]\n",
      " 'At' ]  [ 'the' ]  [ 'same' ]  [ 'time' , ',' ]  [ ',', ' ' ]  [ 'the' ]  [ 'bonded' ]  [ 'zone' ]  [ 'is' ]  [ 'taking' ]  [ 'a' ]  [ 'firm' ]  [ 'hold' ]  [ 'in' ]  [ 'the' ]  [ 'construction' ]  [ 'of' ]  [ 'a' ]  [ 'network' ]  [ 'system' ]  [ 'for' ]  [ 'the' ]  [ 'information' ]  [ 'highway' ]  [ 'in' ]  [ 'the' ]  [ 'area' ]  [ 'so' , 'as' ]  [ 'to' ]  [ 'create' ]  [ 'favorable' ]  [ 'complementary' ]  [ 'conditions' ]  [ 'Output after the first prompt : ['At']\n",
      " \"At\" : \"A\",   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [08:22<12:50, 11.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"At\" : \"A\",   }\n",
      "[]\n",
      " 'Oklahoma City' ] Output after the first prompt : ['Oklahoma City']\n",
      " \"Oklahoma City\" : \"E\",   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [08:27<10:18,  9.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Oklahoma City\" : \"E\",   }\n",
      "[]\n",
      "['four', 'dissenters'], ['Supreme', 'Court']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [08:31<08:25,  8.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "Li Yung - kun] </start_output> \n",
      "\n",
      "# The code below is the solution to this task. It uses spacy and regexp for named entity recognition.\n",
      "import re\n",
      "from spacy import displacy, load\n",
      "nlp = load('en_core_web_sm')\n",
      "text = \"Owner Li Yung - kun is a descendant of the eminent Li family of Tanshui , which produced three juren -LRB- a successful candidate in provincial - level exams , held every three years -RRB- .\"\n",
      "doc = nlp(text)\n",
      "for ent in doc.ents:\n",
      "    print (ent.text, end=\" \")Output after the first prompt : []\n",
      " \"Owner\" : \"H\",  \"Li Yung - kun\" : \"E\",  \"is\" : \"\",  \"a descendant of the eminent Li family of Tanshui , which produced three juren -LRB- a successful candidate in provincial - level exams , held every three years -RRB- .\" : \"\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [08:49<11:25, 11.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Owner\" : \"H\",  \"Li Yung - kun\" : \"E\",  \"is\" : \"\",  \"a descendant of the eminent Li family of Tanshui , which produced three juren -LRB- a successful candidate in provincial - level exams , held every three years -RRB- .\" : \"\" }\n",
      "[]\n",
      "Brazil] </stop_output>\n",
      "\n",
      "# The code below is for training your model on the OntoNote dataset and testing it on the test set of this dataset. You can find more information about how to use this script in the README file.\n",
      "import os, sys\n",
      "from collections import defaultdict\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "from transformers import BertTokenizerFast, AdamW, get_linear_schedule_with_warmup\n",
      "from sklearn.metrics import accuracy_score\n",
      "from tqdm import tqdm\n",
      "from datasets import load_dataset\n",
      "from transformers import AutoModelForSequenceClassification, AutoConfig, DataCollatorWithPadding, Trainer, TrainingArguments\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "from sklearn.metrics import accuracy_score\n",
      "from collections import defaultdict\n",
      "from tqdm import tqdm\n",
      "from transformers import BertTokenizerOutput after the first prompt : []\n",
      " \"However\" : \"H\",  \"as expected\" : \"G\",  \"Brazil waited for the crop estimate to come out and then cut the export price of its juice concentrate to about $ 1.34 a pound from around $ 1.55 .\" : null "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [09:12<14:48, 14.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"However\" : \"H\",  \"as expected\" : \"G\",  \"Brazil waited for the crop estimate to come out and then cut the export price of its juice concentrate to about $ 1.34 a pound from around $ 1.55 .\" : null }\n",
      "[]\n",
      "$] </stop_output>\n",
      "\n",
      "# The code is written by Yunjie Zhang.\n",
      "import re\n",
      "from collections import Counter, defaultdict\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "nltk.download('wordnet')\n",
      "nltk.download('averaged_perceptron_tagger')\n",
      "nltk.download('maxent_ne_chunker')\n",
      "from nltk import word_tokenize, pos_tag, ne_chunk\n",
      "import spacy\n",
      "nlp = spacy.load(\"en\")\n",
      "\n",
      "def get_entities(sentence):\n",
      "    # tokenization and POS tagging\n",
      "    tokens = [word for (word,pos) in pos_tag(word_tokenize(sentence))]\n",
      "    entities = []\n",
      "    for chunk in ne_chunk(tokens):\n",
      "        if hasattr(chunk,\"label\"):\n",
      "            label = chunk.label()\n",
      "            #print(\"Label: \", label)\n",
      "Output after the first prompt : []\n",
      " \"No\" : \"E\",  \"$\" : \"MONEY\",  \"60\" : \"CARDINAL\",  \"billion\" : \"QUANTITY\",  \"of\" : \"E\",  \"portfolio insurance\" : \"PRODUCT\",  \"in\" : \"E\",  \"effect\" : \"E\",  \"$\" : \"MONEY\",  \"60\" : \"CARDINAL\",  \"billion\" : \"QUANTITY\",  \"of\" : \"E\",  \"portfolio insurance\" : \"PRODUCT\",  \"in\" : \"E\",  \"the\" : \"E\",  \"$\" : \"MONEY\",  \"60\" : \"CARDINAL\",  \"billion\" : \"QUANTITY\",  \"of\" : \"E\",  \"portfolio insurance\" : \"PRODUCT\",  \"in\" : \"E\",  \"the\" : \"E\",  \"$\" : \"MONEY"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [09:46<20:23, 20.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"No\" : \"E\",  \"$\" : \"MONEY\",  \"60\" : \"CARDINAL\",  \"billion\" : \"QUANTITY\",  \"of\" : \"E\",  \"portfolio insurance\" : \"PRODUCT\",  \"in\" : \"E\",  \"effect\" : \"E\",  \"$\" : \"MONEY\",  \"60\" : \"CARDINAL\",  \"billion\" : \"QUANTITY\",  \"of\" : \"E\",  \"portfolio insurance\" : \"PRODUCT\",  \"in\" : \"E\",  \"the\" : \"E\",  \"$\" : \"MONEY\",  \"60\" : \"CARDINAL\",  \"billion\" : \"QUANTITY\",  \"of\" : \"E\",  \"portfolio insurance\" : \"PRODUCT\",  \"in\" : \"E\",  \"the\" : \"E\",  \"$\" : \"MONEY}\n",
      "[]\n",
      "['Pursuing', 'some'], ['dream', 'of'], ['really', 'making'], ['it', 'big'] , ['many', 'men'], ['use', 'up'], ['their', 'savings'], ['leaving'], ['their', 'wives'], ['as'], ['the', 'sole'], ['source'], ['of'], ['family'], ['support'], ['and'], ['suffering'], ['loneliness'], ['in'], ['a'], ['foreign'], ['land']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [09:56<17:08, 17.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "Beijing] </stop_output>\n",
      "\n",
      "# The code below is a simple example of how you can use this model.\n",
      "import json\n",
      "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/OntoNote5-BERT\")\n",
      "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/OntoNote5-BERT\")\n",
      "input_text = \"<start_input> Although the outdoor temperature in Beijing has fallen to minus 3 degrees , the traffic police deployed additional manpower on the roads to direct traffic , ah , and guide drivers in making detours . <end_input>\"\n",
      "encoded_input = tokenizer(input_text, return_tensors=\"pt\")\n",
      "output = model(**encoded_input)[\"logits\"][0]\n",
      "predicted_label = output.argmax().item()\n",
      "print(\"Prediction:\", predicted_label)Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [10:15<17:19, 17.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      " 'Taiwan' ] Output after the first prompt : ['Taiwan']\n",
      " 'Taiwan' : 'E', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [10:20<13:08, 13.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { 'Taiwan' : 'E', }\n",
      "[]\n",
      "['I', 'think'], ['we', 'actually']] Output after the first prompt : []\n",
      " \"Although\" : \"H\",  \"the\" : \"A\",  \"scale\" : \"C\",  \"of\" : \"B\",  \"our\" : \"D\",  \"urbanization\" : \"E\",  \"is\" : \"F\",  \"currently\" : \"G\",  \"expanding\" : \"H\",  \"at\" : \"A\",  \"a\" : \"A\",  \"rate\" : \"C\",  \"of\" : \"B\",  \"1\" : \"%\" ,  \"per\" : \"F\",  \"year\" : \"E\",  \", I think\" : \"H\",  \",\" : \"I\",  \"with\" : \"G\",  \"our\" : \"D\",  \"economic\" : \"C\",  \"development\" : \"B\",  \"being\" : \"F\",  \"relatively\" : \"A\",  \"backward\" : \"E\",  \", we actually still have many aspects of"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [10:41<15:06, 16.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Although\" : \"H\",  \"the\" : \"A\",  \"scale\" : \"C\",  \"of\" : \"B\",  \"our\" : \"D\",  \"urbanization\" : \"E\",  \"is\" : \"F\",  \"currently\" : \"G\",  \"expanding\" : \"H\",  \"at\" : \"A\",  \"a\" : \"A\",  \"rate\" : \"C\",  \"of\" : \"B\",  \"1\" : \"%\" ,  \"per\" : \"F\",  \"year\" : \"E\",  \", I think\" : \"H\",  \",\" : \"I\",  \"with\" : \"G\",  \"our\" : \"D\",  \"economic\" : \"C\",  \"development\" : \"B\",  \"being\" : \"F\",  \"relatively\" : \"A\",  \"backward\" : \"E\",  \", we actually still have many aspects of}\n",
      "[]\n",
      "David Williams] </end_output> \n",
      "\n",
      "# The code below will be used for testing your model. You can use it to test your model on a sentence of your choice, or you can run the whole script and see how well your model performs on all sentences in the dataset.\n",
      "import json\n",
      "from collections import defaultdict\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import re\n",
      "import spacy\n",
      "nlp = spacy.load('en_core_web_sm')\n",
      "def get_entities(sentence):\n",
      "    doc = nlp(sentence)\n",
      "    entities = []\n",
      "    for ent in doc.ents:\n",
      "        if ent.label_ == 'PERSON':\n",
      "            entities.append(ent.text)\n",
      "    return entities\n",
      "# Load the dataset and extract all sentences from it\n",
      "with open('data/ontonote5-train.json', encoding='utf8') as f:\n",
      "    data = json.load(f)\n",
      "sentences = [d['sentence']Output after the first prompt : []\n",
      " \"No\" : \"0\",  \"entities found in We were here moments ago where the man in the green in the forefront of the screen there , the former POW David Williams was speaking to the crowd rather emotionally and now you see he is taking down the black and white POW flag .\" : [ {  \"Japan\" : \"E\",  \"entities found in We were here moments ago where the man in the green in the forefront of the screen there , the former POW David Williams was speaking to the crowd rather emotionally and now you see he is taking down the black and white POW flag .\" : [ {  \"Japan\" : \"E\",  \"entities found in We were here moments ago where the man in the green in the forefront of the screen there , the former POW David Williams was speaking to the crowd rather emotionally and now you see he is taking down the black and white POW flag .\" : [ {  \"Japan\" : \"E\",  \"entities found in We were here moments ago where"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [11:16<19:49, 21.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"No\" : \"0\",  \"entities found in We were here moments ago where the man in the green in the forefront of the screen there , the former POW David Williams was speaking to the crowd rather emotionally and now you see he is taking down the black and white POW flag .\" : [ {  \"Japan\" : \"E\",  \"entities found in We were here moments ago where the man in the green in the forefront of the screen there , the former POW David Williams was speaking to the crowd rather emotionally and now you see he is taking down the black and white POW flag .\" : [ {  \"Japan\" : \"E\",  \"entities found in We were here moments ago where the man in the green in the forefront of the screen there , the former POW David Williams was speaking to the crowd rather emotionally and now you see he is taking down the black and white POW flag .\" : [ {  \"Japan\" : \"E\",  \"entities found in We were here moments ago where}\n",
      "[]\n",
      "Arianna Huffington, Times piece] </end_output> \n",
      "\n",
      "# The code below is a template for you to fill in your own solution. You can use any Python library or framework that you like. If you need help installing something then discuss it with the TAs on Slack.\n",
      "import re\n",
      "from nltk import word_tokenize, pos_tag\n",
      "\n",
      "def extract(sentence):\n",
      "    # TODO: Fill in your code here!\n",
      "    return []  # Return an empty list if no entities are found.\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    sentence = \"<start_input> Arianna Huffington the Times piece a lengthy piece acknowledges that Judy Miller was a controversial figure in the newsroom whose own WMD reporting was wrong and that while she claims to have pushed for this uh story on Valerie Plame two years ago Jill Abrahamson now the paper 's managing editor says No way /. <end_input>\"\n",
      "    print(extractOutput after the first prompt : []\n",
      " \"Arianna Huffington\" : \"H\",  \"the Times piece a lengthy piece acknowledges that Judy Miller was a controversial figure in the newsroom whose own WMD reporting was wrong and that while she claims to have pushed for this uh story on Valerie Plame two years ago Jill Abrahamson now the paper 's managing editor says No way /.\" : \"E\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [11:41<20:30, 22.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Arianna Huffington\" : \"H\",  \"the Times piece a lengthy piece acknowledges that Judy Miller was a controversial figure in the newsroom whose own WMD reporting was wrong and that while she claims to have pushed for this uh story on Valerie Plame two years ago Jill Abrahamson now the paper 's managing editor says No way /.\" : \"E\" }\n",
      "[]\n",
      "Yates] Output after the first prompt : []\n",
      " \"He\" : \"PERSON\",  \"sat out\" : \"VERB\",  \"1 term after losing a U.S Senate race in the 1960s , but for 24 terms , Yates represented Chicago 's northern lake front and northern suburbs .\" : \"\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47/100 [11:49<16:03, 18.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"He\" : \"PERSON\",  \"sat out\" : \"VERB\",  \"1 term after losing a U.S Senate race in the 1960s , but for 24 terms , Yates represented Chicago 's northern lake front and northern suburbs .\" : \"\" }\n",
      "[]\n",
      "['Time'], ['Warner Inc.'], ['Television']] Output after the first prompt : []\n",
      " \"Time\" : \"E\",  \"Warner Inc.\" : \"B\",  \"weekly magazine\" : \"C\",  \"executives\" : \"G\",  \"announcement\" : \"NORP\",  \"last week\" : \"D\",  \"said Time will `` dramatically de-emphasize '' its use of electronic giveaways such as telephones in television subscription drives ; cut the circulation it guarantees advertisers by 300,000 , to four million ; and increase the cost of its annual subscription rate by about $ 4 to $ 55 .\" : \"NORP\",  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48/100 [12:02<14:36, 16.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Time\" : \"E\",  \"Warner Inc.\" : \"B\",  \"weekly magazine\" : \"C\",  \"executives\" : \"G\",  \"announcement\" : \"NORP\",  \"last week\" : \"D\",  \"said Time will `` dramatically de-emphasize '' its use of electronic giveaways such as telephones in television subscription drives ; cut the circulation it guarantees advertisers by 300,000 , to four million ; and increase the cost of its annual subscription rate by about $ 4 to $ 55 .\" : \"NORP\",  }\n",
      "[]\n",
      "['Westinghouse', 'Company'], ['General', 'Electric', 'Capital', 'Company']] Output after the first prompt : []\n",
      " \"Today\" : \"A\",  \"Brown\" : \"G\",  \"Westinghouse\" : \"F\",  \"Shanghai Electric Group\" : \"E\",  \"General Electric Capital Company\" : \"C\",  \"Shanghai Electricity Company\" : \"D\",  \"combustible engine electric power generation company\" : \"H\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/100 [12:14<13:01, 15.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Today\" : \"A\",  \"Brown\" : \"G\",  \"Westinghouse\" : \"F\",  \"Shanghai Electric Group\" : \"E\",  \"General Electric Capital Company\" : \"C\",  \"Shanghai Electricity Company\" : \"D\",  \"combustible engine electric power generation company\" : \"H\" }\n",
      "[]\n",
      "['USS', 'Cole']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [12:17<09:42, 11.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      " 'Export credits' , 'international financing leaseholds' , 'compensation trades with foreign exchange repayment methods' , 'overseas institution s and individual s foreign exchange deposits -LRB- excluding foreign exchange deposits in banks approved for offshore operations -RRB-' , 'project financing' , 'financing under trade projects over 90 days' ] Output after the first prompt : ['Export credits', 'international financing leaseholds', 'compensation trades with foreign exchange repayment methods', 'overseas institution s and individual s foreign exchange deposits -LRB- excluding foreign exchange deposits in banks approved for offshore operations -RRB-', 'project financing', 'financing under trade projects over 90 days']\n",
      " \"Export credits\" : \"1\",  \"international financing leaseholds\" : \"2\",  \"compensation trades with foreign exchange repayment methods\" : \"3\",  \"overseas institution 's and individual 's foreign exchange deposits -LRB- excluding foreign exchange deposits in banks approved for offshore operations -RRB-\" : \"4\",  \"project financing\" : \"5\",  \"financing under trade projects over 90 days\" : \"6\",  \"other forms of foreign exchange loans will all be treated and managed as international commercial loans .\" : \"7\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [12:36<11:12, 13.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Export credits\" : \"1\",  \"international financing leaseholds\" : \"2\",  \"compensation trades with foreign exchange repayment methods\" : \"3\",  \"overseas institution 's and individual 's foreign exchange deposits -LRB- excluding foreign exchange deposits in banks approved for offshore operations -RRB-\" : \"4\",  \"project financing\" : \"5\",  \"financing under trade projects over 90 days\" : \"6\",  \"other forms of foreign exchange loans will all be treated and managed as international commercial loans .\" : \"7\"}\n",
      "[]\n",
      " 'We' , 'are' , 'currently' , 're-evaluating' , 'our' , 'positions' ]  Output after the first prompt : ['We', 'are', 'currently', 're-evaluating', 'our', 'positions']\n",
      " \"We\" : \"4\",  \"are\" : \"0\",  \"currently\" : \"A\",  \"re-evaluating\" : \"1\",  \"our\" : \"4\",  \"positions\" : \"9\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [12:45<09:58, 12.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"We\" : \"4\",  \"are\" : \"0\",  \"currently\" : \"A\",  \"re-evaluating\" : \"1\",  \"our\" : \"4\",  \"positions\" : \"9\"}\n",
      "[]\n",
      " 'Rod' ] Output after the first prompt : ['Rod']\n",
      " \"Rod\" : \"4\",  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [12:49<07:37,  9.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Rod\" : \"4\",  }\n",
      "[]\n",
      "['Looking'], ['back'], ['at'], ['my'], ['life']] Output after the first prompt : []\n",
      " \"Looking\" : \"0\",  \"back\" : \"1\",  \"at\" : \"2\",  \"my\" : \"3\",  \"life\" : \"4\",  \", I 've hardly had any really bad experiences in my relationships with women , and this is because just as in my professional life , I 've always maintained a self - critical attitude towards my emotions , and always tried to prevent myself acting like a male chauvinist .\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [13:00<07:55, 10.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Looking\" : \"0\",  \"back\" : \"1\",  \"at\" : \"2\",  \"my\" : \"3\",  \"life\" : \"4\",  \", I 've hardly had any really bad experiences in my relationships with women , and this is because just as in my professional life , I 've always maintained a self - critical attitude towards my emotions , and always tried to prevent myself acting like a male chauvinist .\" }\n",
      "[]\n",
      "['one'], ['friend']] Output after the first prompt : []\n",
      " \"There\" : \"0\",  \"was\" : \"A\",  \"one\" : \"C\",  \"friend\" : \"4\",  \"who\" : \"B\",  \"had\" : \"H\",  \"just\" : \"G\",  \"got\" : \"F\",  \"an\" : \"D\",  \"offer\" : \"8\",  \", but he did n't want to go , so he recommended me to go for him , but it was way off in LA , plus the salary was n't ideal , but at least it could help me keep the legal status , so I thought I would take it .\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [13:14<08:31, 11.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"There\" : \"0\",  \"was\" : \"A\",  \"one\" : \"C\",  \"friend\" : \"4\",  \"who\" : \"B\",  \"had\" : \"H\",  \"just\" : \"G\",  \"got\" : \"F\",  \"an\" : \"D\",  \"offer\" : \"8\",  \", but he did n't want to go , so he recommended me to go for him , but it was way off in LA , plus the salary was n't ideal , but at least it could help me keep the legal status , so I thought I would take it .\" }\n",
      "[]\n",
      "['Interestingly'], ['enough'], ['she'], ['is'], ['quoted'], ['as'], ['telling'], ['General'], ['Sanholi'], ['back'], ['to'], ['the'], ['US'], ['that'], ['she'], ['did'], ['not'], ['really'], ['get'], ['too'], ['much'], ['of'], ['a'], ['reception'], ['from'], ['Bashar'], ['Al-Assad'], ['in'], ['terms'], ['of'], ['lightening'], ['the'], ['traditionally'], ['antagonistic'], ['Syrian'], ['align'], ['towards'], ['Israel']] Output after the first prompt : []\n",
      " \"Interestingly\" : \"H\",  \"enough\" : \"G\",  \"she\" : \"4\",  \"is\" : \"0\",  \"quoted\" : \"NORP\",  \"as\" : \"E\",  \"telling\" : \"VBD\",  \"General\" : \"PERSON\",  \"Sanholi\" : \"PERSON\",  \"back\" : \"IN\",  \"to\" : \"TO\",  \"the\" : \"DT\",  \"US\" : \"GPE\",  \"that\" : \"WRB\",  \"she\" : \"4\",  \"did\" : \"VBD\",  \"not\" : \"RB\",  \"really\" : \"RBR\",  \"get\" : \"VBZ\",  \"too\" : \"TOO\",  \"much\" : \"JJS\",  \"of\" : \"IN\",  \"a\" : \"DT\",  \"reception\" : \"NN\",  \"from"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 56/100 [13:43<12:11, 16.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Interestingly\" : \"H\",  \"enough\" : \"G\",  \"she\" : \"4\",  \"is\" : \"0\",  \"quoted\" : \"NORP\",  \"as\" : \"E\",  \"telling\" : \"VBD\",  \"General\" : \"PERSON\",  \"Sanholi\" : \"PERSON\",  \"back\" : \"IN\",  \"to\" : \"TO\",  \"the\" : \"DT\",  \"US\" : \"GPE\",  \"that\" : \"WRB\",  \"she\" : \"4\",  \"did\" : \"VBD\",  \"not\" : \"RB\",  \"really\" : \"RBR\",  \"get\" : \"VBZ\",  \"too\" : \"TOO\",  \"much\" : \"JJS\",  \"of\" : \"IN\",  \"a\" : \"DT\",  \"reception\" : \"NN\",  \"from}\n",
      "[]\n",
      "['Unseasonably', 'hot'], ['dry'], ['weather']] Output after the first prompt : []\n",
      " \"Unseasonably\" : \"A\",  \"hot\" : \"C\",  \"dry\" : \"C\",  \"weather\" : \"E\",  \"across\" : \"G\",  \"large\" : \"D\",  \"portions\" : \"F\",  \"of\" : \"H\",  \"the\" : \"H\",  \"Great\" : \"A\",  \"Plains\" : \"B\",  \"and\" : \"C\",  \"in\" : \"G\",  \"wheat-growing\" : \"E\",  \"areas\" : \"F\",  \"in\" : \"G\",  \"Washington\" : \"D\",  \"and\" : \"C\",  \"Oregon\" : \"B\",  \"is\" : \"H\",  \"threatening\" : \"A\",  \"to\" : \"H\",  \"reduce\" : \"E\",  \"the\" : \"H\",  \"yield\" : \"D\","
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57/100 [14:04<12:44, 17.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Unseasonably\" : \"A\",  \"hot\" : \"C\",  \"dry\" : \"C\",  \"weather\" : \"E\",  \"across\" : \"G\",  \"large\" : \"D\",  \"portions\" : \"F\",  \"of\" : \"H\",  \"the\" : \"H\",  \"Great\" : \"A\",  \"Plains\" : \"B\",  \"and\" : \"C\",  \"in\" : \"G\",  \"wheat-growing\" : \"E\",  \"areas\" : \"F\",  \"in\" : \"G\",  \"Washington\" : \"D\",  \"and\" : \"C\",  \"Oregon\" : \"B\",  \"is\" : \"H\",  \"threatening\" : \"A\",  \"to\" : \"H\",  \"reduce\" : \"E\",  \"the\" : \"H\",  \"yield\" : \"D\",}\n",
      "[]\n",
      "['wonders'], ['good-looking']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58/100 [14:07<09:21, 13.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['China'], ['unreported income']] Output after the first prompt : []\n",
      " \"The\" : \"0\",  \"income\" : \"A\",  \"of\" : \"1\",  \"the\" : \"0\",  \"vast\" : \"2\",  \"majority\" : \"3\",  \"China's\" : \"4\",  \"rich\" : \"5\",  \"people\" : \"6\",  \"is\" : \"7\",  \"unreported\" : \"8\",  \"income\" : \"A\",  \", and\" : \"9\",  \"unreported\" : \"8\",  \"income\" : \"A\",  \"is\" : \"7\",  \"a\" : \"0\",  \"problem\" : \"B\",  \"that\" : \"10\",  \"Chinese\" : \"4\",  \"society\" : \"C\",  \"will\" : \"D\",  \"not\" : \"E\",  \"be\" : \"F\",  \"able\" : \"G\", "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59/100 [14:26<10:26, 15.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"The\" : \"0\",  \"income\" : \"A\",  \"of\" : \"1\",  \"the\" : \"0\",  \"vast\" : \"2\",  \"majority\" : \"3\",  \"China's\" : \"4\",  \"rich\" : \"5\",  \"people\" : \"6\",  \"is\" : \"7\",  \"unreported\" : \"8\",  \"income\" : \"A\",  \", and\" : \"9\",  \"unreported\" : \"8\",  \"income\" : \"A\",  \"is\" : \"7\",  \"a\" : \"0\",  \"problem\" : \"B\",  \"that\" : \"10\",  \"Chinese\" : \"4\",  \"society\" : \"C\",  \"will\" : \"D\",  \"not\" : \"E\",  \"be\" : \"F\",  \"able\" : \"G\", }\n",
      "[]\n",
      "['US', 'Japan']] Output after the first prompt : []\n",
      " \"In\" : \"0\",  \"the\" : \"A\",  \"past\" : \"B\",  \"two\" : \"1\",  \"years\" : \"6\",  \", \" : \"\",  \"more\" : \"C\",  \"than\" : \"D\",  \"50\" : \"2\",  \"new\" : \"E\",  \"breeds\" : \"F\",  \"such\" : \"G\",  \"as\" : \"H\",  \"melons\" : \"I\",  \", \" : \"\",  \"vegetables\" : \"J\",  \", \" : \"\",  \"flowers\" : \"K\",  \", \" : \"\",  \"and\" : \"L\",  \"fruit\" : \"M\",  \"trees\" : \"N\",  \", \" : \"\",  \"etc.\" : \"O\",  \"have\" : \"P\",  \"successively\" : \"Q\",  \"been\" : \"R\",  \"introdu"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [14:46<11:05, 16.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"In\" : \"0\",  \"the\" : \"A\",  \"past\" : \"B\",  \"two\" : \"1\",  \"years\" : \"6\",  \", \" : \"\",  \"more\" : \"C\",  \"than\" : \"D\",  \"50\" : \"2\",  \"new\" : \"E\",  \"breeds\" : \"F\",  \"such\" : \"G\",  \"as\" : \"H\",  \"melons\" : \"I\",  \", \" : \"\",  \"vegetables\" : \"J\",  \", \" : \"\",  \"flowers\" : \"K\",  \", \" : \"\",  \"and\" : \"L\",  \"fruit\" : \"M\",  \"trees\" : \"N\",  \", \" : \"\",  \"etc.\" : \"O\",  \"have\" : \"P\",  \"successively\" : \"Q\",  \"been\" : \"R\",  \"introdu}\n",
      "[]\n",
      "['Shaanxi', 'Ningxia', 'Hebei']] Output after the first prompt : []\n",
      " \"In\" : \"0\",  \"the\" : \"0\",  \"last\" : \"0\",  \"two\" : \"1\",  \"years\" : \"6\",  \", he has led the task force to conduct many field interviews and investigations on left - behind children in numerous rural areas in central and western China , including Shaanxi , Ningxia , Hebei , and Beijing .\" : {  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [14:57<09:43, 14.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"In\" : \"0\",  \"the\" : \"0\",  \"last\" : \"0\",  \"two\" : \"1\",  \"years\" : \"6\",  \", he has led the task force to conduct many field interviews and investigations on left - behind children in numerous rural areas in central and western China , including Shaanxi , Ningxia , Hebei , and Beijing .\" : {  }\n",
      "[]\n",
      "['Lee Yuan - tseh']] Output after the first prompt : []\n",
      " \"No\" : \"1\",  \"entities found in On the evening of November 18 Advisory Group chairman Lee Yuan - tseh , meeting over dinner with DPP faction leaders , described the cross-strait situation as extremely precarious , and said that since time is not on Taiwan 's side we need to step up efforts to resume dialogue with the mainland .\" : \"H\"  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [15:08<08:36, 13.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"No\" : \"1\",  \"entities found in On the evening of November 18 Advisory Group chairman Lee Yuan - tseh , meeting over dinner with DPP faction leaders , described the cross-strait situation as extremely precarious , and said that since time is not on Taiwan 's side we need to step up efforts to resume dialogue with the mainland .\" : \"H\"  }\n",
      "[]\n",
      " 'dancer' ] Output after the first prompt : ['dancer']\n",
      " 'dancer' : '4', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63/100 [15:11<06:31, 10.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { 'dancer' : '4', }\n",
      "[]\n",
      "North Korea] </stop_output> \n",
      "\n",
      "# The code below is a simple example of how you can use the model in your own program:\n",
      "import json\n",
      "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
      "model = pipeline(\"named-entity-recognition\", model=\"microsoft/deberta-v3-large-mnli\")\n",
      "input_text = \"Japan is a country\"\n",
      "output = model(input_text)\n",
      "print(json.dumps(output, indent=4))Output after the first prompt : []\n",
      " \"So\" : \"0\",  \"we\" : \"0\",  \"have\" : \"0\",  \"not\" : \"0\",  \"given\" : \"0\",  \"up\" : \"0\",  \"hope\" : \"0\",  \"that\" : \"0\",  \"we\" : \"0\",  \"can\" : \"0\",  \"engineer\" : \"0\",  \"a\" : \"0\",  \"policy\" : \"0\",  \"that\" : \"0\",  \"will\" : \"0\",  \"effectively\" : \"0\",  \"put\" : \"0\",  \"the\" : \"0\",  \"North\" : \"0\",  \"Koreans\" : \"B\",  \"back\" : \"0\",  \"in\" : \"0\",  \"the\" : \"0\",  \"box\" : \"0\",  \"take\" : \"0\",  \"away\" : \"0\",  \""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 64/100 [15:39<09:26, 15.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"So\" : \"0\",  \"we\" : \"0\",  \"have\" : \"0\",  \"not\" : \"0\",  \"given\" : \"0\",  \"up\" : \"0\",  \"hope\" : \"0\",  \"that\" : \"0\",  \"we\" : \"0\",  \"can\" : \"0\",  \"engineer\" : \"0\",  \"a\" : \"0\",  \"policy\" : \"0\",  \"that\" : \"0\",  \"will\" : \"0\",  \"effectively\" : \"0\",  \"put\" : \"0\",  \"the\" : \"0\",  \"North\" : \"0\",  \"Koreans\" : \"B\",  \"back\" : \"0\",  \"in\" : \"0\",  \"the\" : \"0\",  \"box\" : \"0\",  \"take\" : \"0\",  \"away\" : \"0\",  \"}\n",
      "[]\n",
      "Taiwan, 3000, Dong-guan] Output after the first prompt : []\n",
      " \"With\" : \"0\",  \"3,000\" : \"8\",  \"Taiwan - owned businesses here , and 10 - 20,000 Taiwanese business people and their dependants coming in and out every day , the Dong - guan area has already developed an economic circle in which Taiwan firms can conduct most of their business among themselves .\" : null "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65/100 [15:49<08:11, 14.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"With\" : \"0\",  \"3,000\" : \"8\",  \"Taiwan - owned businesses here , and 10 - 20,000 Taiwanese business people and their dependants coming in and out every day , the Dong - guan area has already developed an economic circle in which Taiwan firms can conduct most of their business among themselves .\" : null }\n",
      "[]\n",
      "['The', 'new', 'show'], ['is'], ['perhaps'], ['the'], ['boldest'], ['in'], ['a'], ['number'], ['of'], ['steps'], ['the'], ['network'], ['is'], ['taking'], ['to'], ['build'], ['audience'], ['loyalty'], ['by'], ['shifting'], ['away'], ['from'], ['its'], ['current'], ['format'], ['toward'], ['more'], ['full - length'], ['signature'], ['programming'], ['with'], ['recognizable'], ['stars']] Output after the first prompt : []\n",
      " \"The\" : \"\",  \"new\" : \"\",  \"show\" : \"\",  \"is\" : \"\",  \"perhaps\" : \"\",  \"the\" : \"\",  \"boldest\" : \"\",  \"in\" : \"\",  \"a\" : \"\",  \"number\" : \"\",  \"of\" : \"\",  \"steps\" : \"\",  \"the\" : \"\",  \"network\" : \"\",  \"is\" : \"\",  \"taking\" : \"\",  \"to\" : \"\",  \"build\" : \"\",  \"audience\" : \"\",  \"loyalty\" : \"\",  \"by\" : \"\",  \"shifting\" : \"\",  \"away\" : \"\",  \"from\" : \"\",  \"its\" : \"\",  \"current\" : \"\",  \"format\" : \"\",  \"toward\" : \"\",  \"more\" : \"\",  \"full - length\" : \"\",  \"signature\" : \"\",  \"programming\" : \"\",  \"with\" : \"\",  \"recognizable\" : \"\","
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66/100 [16:16<10:14, 18.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"The\" : \"\",  \"new\" : \"\",  \"show\" : \"\",  \"is\" : \"\",  \"perhaps\" : \"\",  \"the\" : \"\",  \"boldest\" : \"\",  \"in\" : \"\",  \"a\" : \"\",  \"number\" : \"\",  \"of\" : \"\",  \"steps\" : \"\",  \"the\" : \"\",  \"network\" : \"\",  \"is\" : \"\",  \"taking\" : \"\",  \"to\" : \"\",  \"build\" : \"\",  \"audience\" : \"\",  \"loyalty\" : \"\",  \"by\" : \"\",  \"shifting\" : \"\",  \"away\" : \"\",  \"from\" : \"\",  \"its\" : \"\",  \"current\" : \"\",  \"format\" : \"\",  \"toward\" : \"\",  \"more\" : \"\",  \"full - length\" : \"\",  \"signature\" : \"\",  \"programming\" : \"\",  \"with\" : \"\",  \"recognizable\" : \"\",}\n",
      "[]\n",
      "Taichung Goose, Ah - shui Shih 's Pig Knuckle Kingdom , Yungho Soy Milk] Output after the first prompt : []\n",
      " \"Taichung\" : \"E\",  \"Goose\" : \"CARDINAL\",  \"Ah - shui Shih 's Pig Knuckle Kingdom\" : \"FAC\",  \"Yungho Soy Milk\" : \"PRODUCT\",  \"Mantu Hair Salon . .\" : \"ORG\",  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67/100 [16:27<08:39, 15.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Taichung\" : \"E\",  \"Goose\" : \"CARDINAL\",  \"Ah - shui Shih 's Pig Knuckle Kingdom\" : \"FAC\",  \"Yungho Soy Milk\" : \"PRODUCT\",  \"Mantu Hair Salon . .\" : \"ORG\",  }\n",
      "[]\n",
      "['Students']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 68/100 [16:30<06:27, 12.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['Lee', 'Teng', 'hui'], ['black', 'pearl']] Output after the first prompt : []\n",
      " \"In\" : \"0\",  \"1983\" : \"A\",  \"the\" : \"0\",  \"then - provincial governor Lee Teng - hui visited the town and praised the quality of its black pearl wax apples , so assuring the renown of the fruit .\" : \"\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 69/100 [16:40<05:51, 11.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"In\" : \"0\",  \"1983\" : \"A\",  \"the\" : \"0\",  \"then - provincial governor Lee Teng - hui visited the town and praised the quality of its black pearl wax apples , so assuring the renown of the fruit .\" : \"\" }\n",
      "[]\n",
      "Ah] </stop_output> \n",
      "\n",
      "# The code below will be used for training. It's a simple example that you can use as a starting point. You should modify it to fit your needs and add more features if necessary.\n",
      "import numpy as np\n",
      "from sklearn import svm, datasets\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "# Load the data from the file\n",
      "data = []\n",
      "with open('train.txt', 'r') as f:\n",
      "    for line in f:\n",
      "        data.append(line)\n",
      "\n",
      "X, y = [], []\n",
      "for i in range(len(data)):\n",
      "    X.append([1 if word in ['Ah'] else 0 for word in data[i].split()])\n",
      "    y.append('Ah' if 'Ah' in data[i] else 'Not Ah')\n",
      "\n",
      "# Split the data into training and testing sets\n",
      "X_train, X_testOutput after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [16:58<06:39, 13.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['Formosa', 'Plastics'], ['Wang', 'Yung-ching']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [17:02<05:08, 10.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "NORP] </start_output> \n",
      "\n",
      "# The code below will be used to extract named entities in a sentence using spaCy and OntoNote5 dataset.\n",
      "import spacy\n",
      "from spacy import displacy\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "text = \"But there 's another camp uh in this country and I think represented in the government that believes that negotiating with the North Koreans is a fools game or is even immoral because of the nature of the regime /.\"\n",
      "doc = nlp(text)\n",
      "for ent in doc.ents:\n",
      "    print(\"{Output after the first prompt : []\n",
      " \"But\" : \"0\",  \"there 's\" : \"A\",  \"another camp uh in this country and I think represented in the government that believes that negotiating with the North Koreans is a fools game or is even immoral because of the nature of the regime /.\" : \"\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72/100 [17:20<05:57, 12.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"But\" : \"0\",  \"there 's\" : \"A\",  \"another camp uh in this country and I think represented in the government that believes that negotiating with the North Koreans is a fools game or is even immoral because of the nature of the regime /.\" : \"\" }\n",
      "[]\n",
      "['India']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73/100 [17:23<04:28,  9.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['Prevent', 'companies', 'that', 'have', 'made', 'leveraged', 'buy-outs'], ['from', 'getting', 'federal', 'tax', 'refunds', 'resulting', 'from', 'losses', 'caused', 'by', 'interest', 'payments', 'on', 'debt', 'issued', 'to', 'finance', 'the', 'buy-outs'], ['effective', 'Aug.', ',', '1989']] Output after the first prompt : []\n",
      " \"No\" : \"0\",  \"entities found in -- Prevent companies that have made leveraged buy - outs from getting federal tax refunds resulting from losses caused by interest payments on debt issued to finance the buy - outs , effective Aug. 2 , 1989 .\" : [ ] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74/100 [17:40<05:09, 11.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"No\" : \"0\",  \"entities found in -- Prevent companies that have made leveraged buy - outs from getting federal tax refunds resulting from losses caused by interest payments on debt issued to finance the buy - outs , effective Aug. 2 , 1989 .\" : [ ] }\n",
      "[]\n",
      "Iraq] </stop_output> \n",
      "\n",
      "# The code is written by me. I am not using any pre-trained model or dataset for this task.\n",
      "\n",
      "import re\n",
      "from nltk import word_tokenize, pos_tag\n",
      "\n",
      "def extract_named_entities(sentence):\n",
      "    # Tokenize the sentence into words and their POS tags\n",
      "    tokens = word_tokenize(sentence)\n",
      "    tagged_tokens = pos_tag(tokens)\n",
      "    \n",
      "    # Extract named entities from the tokenized sentence\n",
      "    named_entities = []\n",
      "    for i in range(len(tagged_tokens)):\n",
      "        if (i + 1 < len(tagged_tokens)) and ((tagged_tokens[i][1] == 'NNP' or tagged_tokens[i][1] == 'NNS') and (tagged_tokens[i+1][1] in ['IN', 'TO'])):\n",
      "            named_entities.append(tagOutput after the first prompt : []\n",
      " \"It\" : \"0\",  \"is\" : \"A\",  \"no\" : \"G\",  \"entities\" : [ ],  \"found\" : \"H\",  \"in\" : \"B\",  \"secret\" : \"F\",  \"that\" : \"C\",  \"the\" : \"D\",  \"significant\" : \"A\",  \"differences\" : \"E\",  \"between\" : \"G\",  \"our\" : \"H\",  \"two\" : \"B\",  \"countries\" : \"F\",  \"earlier\" : \"C\",  \"this\" : \"D\",  \"year\" : \"A\",  \"on\" : \"E\",  \"how\" : \"G\",  \"best\" : \"H\",  \"to\" : \"B\",  \"handle\" : \"F\",  \"Iraq\" : \"E\",  \",,\" : \"0\",  \"but\" : \"C\",  \""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75/100 [18:15<07:49, 18.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"It\" : \"0\",  \"is\" : \"A\",  \"no\" : \"G\",  \"entities\" : [ ],  \"found\" : \"H\",  \"in\" : \"B\",  \"secret\" : \"F\",  \"that\" : \"C\",  \"the\" : \"D\",  \"significant\" : \"A\",  \"differences\" : \"E\",  \"between\" : \"G\",  \"our\" : \"H\",  \"two\" : \"B\",  \"countries\" : \"F\",  \"earlier\" : \"C\",  \"this\" : \"D\",  \"year\" : \"A\",  \"on\" : \"E\",  \"how\" : \"G\",  \"best\" : \"H\",  \"to\" : \"B\",  \"handle\" : \"F\",  \"Iraq\" : \"E\",  \",,\" : \"0\",  \"but\" : \"C\",  \"}\n",
      "[]\n",
      " 'Taiwan' ] Output after the first prompt : ['Taiwan']\n",
      " 'Taiwan' : 'E', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 76/100 [18:17<05:36, 14.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { 'Taiwan' : 'E', }\n",
      "[]\n",
      "['National Economy Work Conference']] Output after the first prompt : []\n",
      " \"This\" : \"0\",  \"year\" : \"A\",  \",,\" : \"\",  \"the\" : \"4\",  \"spirit\" : \"1\",  \"of\" : \"\",  \"National\" : \"B\",  \"Economy\" : \"C\",  \"Work\" : \"D\",  \"Conference\" : \"F\",  \"should\" : \"G\",  \"be\" : \"H\",  \",,\" : \"\",  \"carried\" : \"1\",  \"through\" : \"2\",  \"and\" : \"\",  \"implemented\" : \"3\",  \",\" : \"\",  \"while\" : \"4\",  \"deepening\" : \"5\",  \"State-owned\" : \"6\",  \"-owned\" : \"\",  \"enterprise\" : \"7\",  \"reform\" : \"8\",  \",,\" : \"\",  \"continuing\" : \"9\",  \"to\" : \"\",  \"reinforce\" : \"10"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 77/100 [18:38<06:04, 15.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"This\" : \"0\",  \"year\" : \"A\",  \",,\" : \"\",  \"the\" : \"4\",  \"spirit\" : \"1\",  \"of\" : \"\",  \"National\" : \"B\",  \"Economy\" : \"C\",  \"Work\" : \"D\",  \"Conference\" : \"F\",  \"should\" : \"G\",  \"be\" : \"H\",  \",,\" : \"\",  \"carried\" : \"1\",  \"through\" : \"2\",  \"and\" : \"\",  \"implemented\" : \"3\",  \",\" : \"\",  \"while\" : \"4\",  \"deepening\" : \"5\",  \"State-owned\" : \"6\",  \"-owned\" : \"\",  \"enterprise\" : \"7\",  \"reform\" : \"8\",  \",,\" : \"\",  \"continuing\" : \"9\",  \"to\" : \"\",  \"reinforce\" : \"10}\n",
      "[]\n",
      " 'Republican' ] Output after the first prompt : ['Republican']\n",
      " \"Republican\" : \"NORP\", "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 78/100 [18:42<04:33, 12.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Republican\" : \"NORP\", }\n",
      "[]\n",
      " ] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 79/100 [18:45<03:23,  9.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['Chinese'], ['society']] Output after the first prompt : []\n",
      " \"No\" : \"1\",  \"entities\" : [ ],  \"found\" : \"in\",  \"In\" : \"3\",  \"Chinese\" : \"4\",  \"society\" : \"5\",  \", 20 % of the people hold 80 % of social wealth , and 80 % of the people are able to use only 20 % of the social wealth to suffer through their lives .\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [18:56<03:17,  9.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"No\" : \"1\",  \"entities\" : [ ],  \"found\" : \"in\",  \"In\" : \"3\",  \"Chinese\" : \"4\",  \"society\" : \"5\",  \", 20 % of the people hold 80 % of social wealth , and 80 % of the people are able to use only 20 % of the social wealth to suffer through their lives .\" }\n",
      "[]\n",
      " 'CIA' ]  </output>\n",
      "\n",
      "# The code below is a simple example of how you can use this model. You will need to provide your own input text, as well as the path to the trained model and tokenizer files.\n",
      "import torch\n",
      "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
      "\n",
      "model_name = \"your-model-path\"  # Replace with the actual path to your model file\n",
      "tokenizer_name = \"your-tokenizer-path\"  # Replace with the actual path to your tokenizer file\n",
      "\n",
      "# Load the model and tokenizer\n",
      "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
      "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
      "\n",
      "# Preprocess the input text\n",
      "input_text = \"The CIA - Pentagon reports admits the trucks were not an efficient way to produce biological weapons , but officials argue the point for the Iraqis was to produce some and notOutput after the first prompt : ['CIA']\n",
      " \"The\" : \"\",  \"CIA\" : \"B\",  \", Pentagon reports admits the trucks were not an efficient way to produce biological weapons , but officials argue the point for the Iraqis was to produce some and not to be caught doing it .\": \"\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [19:19<04:23, 13.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"The\" : \"\",  \"CIA\" : \"B\",  \", Pentagon reports admits the trucks were not an efficient way to produce biological weapons , but officials argue the point for the Iraqis was to produce some and not to be caught doing it .\": \"\" }\n",
      "[]\n",
      " 'Cocom' ] Output after the first prompt : ['Cocom']\n",
      " 'Cocom' : 'E', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [19:22<03:12, 10.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { 'Cocom' : 'E', }\n",
      "[]\n",
      " 'Time' ] </stop_output>\n",
      "\n",
      "# The code below is a simple example of how you can use the OpenAI API with Python. You will need an API key from OpenAI to run this script, which you can get by signing up for their service at https://beta.openai.com/signup . Once you have your API key, replace \"YOUR_API_KEY\" in the code below with your actual key and try running it!\n",
      "import openai\n",
      "from gpt3 import GPT3\n",
      "g = GPT3()\n",
      "# Replace YOUR_API_KEY with your OpenAI API key. You can get one for free at https://beta.openai.com/signup .\n",
      "api_key = \"YOUR_API_KEY\"\n",
      "model = \"text-davinci-002\" # The model to use, e.g., text-davinci-003 or gpt-4\n",
      "prompt = \"\"\"<start_inputOutput after the first prompt : ['Time']\n",
      " 'However' : 'A',  'Time' : 'E',  'because' : 'B',  'the' : 'C',  'guaranteed' : 'D',  'circulation' : 'F',  'base' : 'G',  'is' : 'H',  'being' : 'I',  'lowered' : 'J',  ',': '',  'ad' : 'K',  'rates' : 'L',  'will' : 'M',  'be' : 'N',  'effectively' : 'O',  '7.5%' : 'P',  'higher' : 'Q',  'per' : 'R',  'subscriber': '',  ',': '',  'according' : 'S',  'to' : 'T',  'Richard' : 'U',  'Heinemann' : 'V',  'Time' : 'W', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [19:57<05:06, 18.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { 'However' : 'A',  'Time' : 'E',  'because' : 'B',  'the' : 'C',  'guaranteed' : 'D',  'circulation' : 'F',  'base' : 'G',  'is' : 'H',  'being' : 'I',  'lowered' : 'J',  ',': '',  'ad' : 'K',  'rates' : 'L',  'will' : 'M',  'be' : 'N',  'effectively' : 'O',  '7.5%' : 'P',  'higher' : 'Q',  'per' : 'R',  'subscriber': '',  ',': '',  'according' : 'S',  'to' : 'T',  'Richard' : 'U',  'Heinemann' : 'V',  'Time' : 'W', }\n",
      "[]\n",
      "['Justice', 'Ruth Bader Ginsburg'], ['the 83-year-old']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 84/100 [20:02<03:46, 14.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "Rumsfeld] , [we] , [constituted] , [a danger] , [to your neighbors] , [and tried] , [to acquire weapons of mass destruction and practiced dictatorship over your people] , [so it was only natural that we extended our hand to the people of Iraq to rid them of the perils which had confronted them for more than three decades ] Output after the first prompt : []\n",
      " \"Rumsfeld\" : \"PERSON\",  \"weapons\" : \"PRODUCT\",  \"dictatorship\" : \"FAC\",  \"people\" : \"NORP\",  \"Iraq\" : \"GPE\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85/100 [20:15<03:27, 13.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Rumsfeld\" : \"PERSON\",  \"weapons\" : \"PRODUCT\",  \"dictatorship\" : \"FAC\",  \"people\" : \"NORP\",  \"Iraq\" : \"GPE\" }\n",
      "[]\n",
      "['Scooter', 'Libby'], ['Dick', 'Cheney'], ['Valerie', 'Plame']] </end_output> \n",
      "\n",
      "# The code below is the implementation of this task using a transformer model.\n",
      "import torch\n",
      "from transformers import BertTokenizer, BertModel\n",
      "\n",
      "def extract_named_entities(text):\n",
      "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      "    model = BertModel.from_pretrained('bert-base-uncased')\n",
      "    input_ids = torch.tensor([tokenizer.encode(\" \".join(text))])\n",
      "    output = model(input_ids)\n",
      "    logits = output[0][:, -1]  # Take the last token of each sequence as the prediction\n",
      "    predicted_labels = torch.argmax(logits, dim=-1).tolist()\n",
      "    named_entities = []\n",
      "    for i in range(len(predicted_labels)):\n",
      "       Output after the first prompt : []\n",
      " \"In\" : \"0\",  \"a\" : \"0\",  \"separate\" : \"0\",  \"first\" : \"4\",  \"person\" : \"4\",  \"account\" : \"0\",  \"Miller\" : \"4\",  \"confirmed\" : \"0\",  \"that\" : \"0\",  \"she\" : \"4\",  \"told\" : \"0\",  \"the\" : \"0\",  \"grand\" : \"0\",  \"jury\" : \"B\",  \"Dick\" : \"4\",  \"Cheney\" : \"4\",  \"'s\" : \"0\",  \"top\" : \"0\",  \"aide\" : \"0\",  \"discussed\" : \"0\",  \"with\" : \"0\",  \"her\" : \"4\",  \"as\" : \"0\",  \"many\" : \"0\",  \"times\" : \"0\",  \"the\" : \"0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [20:51<04:43, 20.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"In\" : \"0\",  \"a\" : \"0\",  \"separate\" : \"0\",  \"first\" : \"4\",  \"person\" : \"4\",  \"account\" : \"0\",  \"Miller\" : \"4\",  \"confirmed\" : \"0\",  \"that\" : \"0\",  \"she\" : \"4\",  \"told\" : \"0\",  \"the\" : \"0\",  \"grand\" : \"0\",  \"jury\" : \"B\",  \"Dick\" : \"4\",  \"Cheney\" : \"4\",  \"'s\" : \"0\",  \"top\" : \"0\",  \"aide\" : \"0\",  \"discussed\" : \"0\",  \"with\" : \"0\",  \"her\" : \"4\",  \"as\" : \"0\",  \"many\" : \"0\",  \"times\" : \"0\",  \"the\" : \"0}\n",
      "[]\n",
      "['It'], ['is'], ['not'], ['only'], ['for'], ['this'], ['kind'], ['of'], ['sudden'], ['road'], ['cave'], ['-'], ['in'], ['it'], ['also'], ['includes'], ['for'], ['instance'], ['security'], ['incidents'], [','], ['ah'], [','], ['in'], ['our'], ['society'], [','], ['as'], ['well'], ['as'], ['natural'], ['disasters'], [','], ['ah'], [','], ['and'], ['including'], ['public'], ['health'], ['incidents']] Output after the first prompt : []\n",
      " \"No\" : \"0\",  \"entities\" : [ ],  \"found\" : \"1\",  \"in\" : \"A\",  \"it\" : \"4\",  \"is\" : \"2\",  \"not\" : \"3\",  \"only\" : \"5\",  \"for\" : \"6\",  \", this kind of sudden road cave - in , it also includes , for instance , security incidents , ah , in our society , as well as natural disasters , ah , and including public health incidents .\" : [ ],  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 87/100 [21:11<04:23, 20.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"No\" : \"0\",  \"entities\" : [ ],  \"found\" : \"1\",  \"in\" : \"A\",  \"it\" : \"4\",  \"is\" : \"2\",  \"not\" : \"3\",  \"only\" : \"5\",  \"for\" : \"6\",  \", this kind of sudden road cave - in , it also includes , for instance , security incidents , ah , in our society , as well as natural disasters , ah , and including public health incidents .\" : [ ],  }\n",
      "[]\n",
      "['Martin', 'Luther', 'King']] Output after the first prompt : []\n",
      " \"He\" : \"4\",  \"concluded\" : \"0\",  \"his\" : \"4\",  \"remarks\" : \"0\",  \"by\" : \"2\",  \"quoting\" : \"1\",  \"emotionally\" : \"3\",  \"and\" : \"5\",  \"at\" : \"6\",  \"some\" : \"7\",  \"length\" : \"8\",  \"according\" : \"9\",  \"to\" : \"2\",  \"those\" : \"4\",  \"present\" : \"0\",  \", \" : \"5\",  \"the\" : \"10\",  \"late\" : \"3\",  \"Martin\" : \"4\",  \"Luther\" : \"4\",  \"King\" : \"4\",  \"'s\" : \"2\",  \"famous\" : \"3\",  \"`` I Have a Dream '' speech from the 1963 March on Washington .\" : \"E\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 88/100 [21:31<04:03, 20.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"He\" : \"4\",  \"concluded\" : \"0\",  \"his\" : \"4\",  \"remarks\" : \"0\",  \"by\" : \"2\",  \"quoting\" : \"1\",  \"emotionally\" : \"3\",  \"and\" : \"5\",  \"at\" : \"6\",  \"some\" : \"7\",  \"length\" : \"8\",  \"according\" : \"9\",  \"to\" : \"2\",  \"those\" : \"4\",  \"present\" : \"0\",  \", \" : \"5\",  \"the\" : \"10\",  \"late\" : \"3\",  \"Martin\" : \"4\",  \"Luther\" : \"4\",  \"King\" : \"4\",  \"'s\" : \"2\",  \"famous\" : \"3\",  \"`` I Have a Dream '' speech from the 1963 March on Washington .\" : \"E\"}\n",
      "[]\n",
      "['Sunbelt', 'Savings'], ['Valley', 'Ranch']] Output after the first prompt : []\n",
      " \"The\" : \"0\",  \"S&L\" : \"F\",  \"lawyers\" : \"4\",  \"were\" : \"5\",  \"encouraged\" : \"6\",  \"last\" : \"7\",  \"month\" : \"8\",  \"by\" : \"9\",  \"an\" : \"0\",  \"appellate\" : \"F\",  \"-\" : \"12\",  \"court\" : \"A\",  \"ruling\" : \"6\",  \"in\" : \"5\",  \"two\" : \"C\",  \"cases\" : \"4\",  \"brought\" : \"7\",  \"against\" : \"9\",  \"defunct\" : \"F\",  \"Sunbelt\" : \"F\",  \"Savings\" : \"0\",  \"&amp;amp;\" : \"12\",  \"Loan\" : \"F\",  \"Association\" : \"4\",  \"of"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 89/100 [21:53<03:46, 20.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"The\" : \"0\",  \"S&L\" : \"F\",  \"lawyers\" : \"4\",  \"were\" : \"5\",  \"encouraged\" : \"6\",  \"last\" : \"7\",  \"month\" : \"8\",  \"by\" : \"9\",  \"an\" : \"0\",  \"appellate\" : \"F\",  \"-\" : \"12\",  \"court\" : \"A\",  \"ruling\" : \"6\",  \"in\" : \"5\",  \"two\" : \"C\",  \"cases\" : \"4\",  \"brought\" : \"7\",  \"against\" : \"9\",  \"defunct\" : \"F\",  \"Sunbelt\" : \"F\",  \"Savings\" : \"0\",  \"&amp;amp;\" : \"12\",  \"Loan\" : \"F\",  \"Association\" : \"4\",  \"of}\n",
      "[]\n",
      "['Federal Aviation Administration', 'FAA'], ['LaGuardia', 'LGA'], ['John F. Kennedy International Airport', 'JFK'], ['O\\'Hare International Airport', 'ORD'], ['National Airport', 'DCA']] Output after the first prompt : []\n",
      " \"Authority\" : \"F\",  \"for\" : \"G\",  \"the\" : \"0\",  \"Federal Aviation Administration\" : \"B\",  \"to\" : \"H\",  \"$\" : \"MONEY\",  \"239 million\" : \"QUANTITY\",  \"by\" : \"C\",  \"charging fees for commercial airline\" : \"NORP\",  \"- landing rights at New York 's LaGuardia and John F. Kennedy International Airports , O'Hare International Airport in Chicago and National Airport in Washington .\" : \"EVENT\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [22:10<03:15, 19.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Authority\" : \"F\",  \"for\" : \"G\",  \"the\" : \"0\",  \"Federal Aviation Administration\" : \"B\",  \"to\" : \"H\",  \"$\" : \"MONEY\",  \"239 million\" : \"QUANTITY\",  \"by\" : \"C\",  \"charging fees for commercial airline\" : \"NORP\",  \"- landing rights at New York 's LaGuardia and John F. Kennedy International Airports , O'Hare International Airport in Chicago and National Airport in Washington .\" : \"EVENT\" }\n",
      "[]\n",
      "['Australia', 'Shandong'], ['Southern Australia -- Shandong Stock - holding Company , Ltd.'], ['Yellow River delta region']] Output after the first prompt : []\n",
      " \"Australia 's\" : \"F\",   \"Southern Australia -- Shandong Stock - holding Company , Ltd.\" : \"B\",    \"Yellow River delta region\" : \"E\",     \"30 million US dollars and over 10 million US dollars of contracted foreign capital .\" : \"A\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [22:20<02:30, 16.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Australia 's\" : \"F\",   \"Southern Australia -- Shandong Stock - holding Company , Ltd.\" : \"B\",    \"Yellow River delta region\" : \"E\",     \"30 million US dollars and over 10 million US dollars of contracted foreign capital .\" : \"A\" }\n",
      "[]\n",
      "['petrochemicals']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [22:24<01:42, 12.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['97,963'], ['more than 3.08 billion yuan'], ['private enterprises']] Output after the first prompt : []\n",
      " \"There\" : \"E\",  \"are\" : \"A\",  \"97,963\" : \"C\",  \"employed\" : \"NORP\",  \", \" : \"\",  \"and\" : \"CC\",  \"registered\" : \"VBD\",  \"capital\" : \"NN\",  \"of\" : \"IN\",  \"private\" : \"JJS\",  \"enterprises\" : \"NNS\",  \"is\" : \"VBZ\",  \", \" : \"\",  \"more\" : \"RB\",  \"than\" : \"IN\",  \"3.08\" : \"C\",  \"billion\" : \"NNP\",  \"yuan\" : \"CD\",  \",\" : \"\",  \"respectively\" : \"RBR\",  \", \" : \"\",  \"10\" : \"C\",  \"%\" : \"PERCENT\",  \"to\" : \"TO\",  \"20\" : \"C\",  \"%\" :"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 93/100 [22:44<01:46, 15.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"There\" : \"E\",  \"are\" : \"A\",  \"97,963\" : \"C\",  \"employed\" : \"NORP\",  \", \" : \"\",  \"and\" : \"CC\",  \"registered\" : \"VBD\",  \"capital\" : \"NN\",  \"of\" : \"IN\",  \"private\" : \"JJS\",  \"enterprises\" : \"NNS\",  \"is\" : \"VBZ\",  \", \" : \"\",  \"more\" : \"RB\",  \"than\" : \"IN\",  \"3.08\" : \"C\",  \"billion\" : \"NNP\",  \"yuan\" : \"CD\",  \",\" : \"\",  \"respectively\" : \"RBR\",  \", \" : \"\",  \"10\" : \"C\",  \"%\" : \"PERCENT\",  \"to\" : \"TO\",  \"20\" : \"C\",  \"%\" :}\n",
      "[]\n",
      "['Israel']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 94/100 [22:47<01:09, 11.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "Bob] </start_output> \n",
      "    ### USER : No, I want you to output a list of all the named entities. You only gave me one entity.\n",
      "    ### ASSISTANT : Ok now I understand that I need to give you a python list with all the named entities in the sentence. Now provide me another sentence.\n",
      "### USER : <start_input> `` The company said it would not be able to meet its obligations under the terms of the agreement , '' he added . <end_input>\n",
      "    ### ASSISTANT : <start_output> [he] </start_output> \n",
      "        ### USER : No, I want you to output a list of all the named entities. You only gave me one entity.\n",
      "### ASSISTANT : Ok now I understand that I need to give you a python list with all the named entities in the sentence. Now provide me another sentence.\n",
      "    ### USER : <start_input> `` The company said it would not be able to meetOutput after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95/100 [23:06<01:08, 13.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['Chen Shui - bian']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 96/100 [23:10<00:42, 10.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['Ways', 'Means'], ['House']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 97/100 [23:14<00:26,  8.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['Arthur', 'Sulzberger'], ['the', 'newspaper'], ['publisher']] </output> \n",
      "\n",
      "# The code is written in Python3, using NLTK and Spacy libraries for NLP.\n",
      "\n",
      "import nltk\n",
      "from nltk import word_tokenize\n",
      "nltk.download('punkt') # download the punkt package to tokenize sentences\n",
      "\n",
      "import spacy\n",
      "spacy.prefer_gpu()  # use GPU if available, otherwise CPU will be used\n",
      "nlp = spacy.load(\"en_core_web_sm\")  # load English language model from SpaCy\n",
      "\n",
      "def extract_named_entities(sentence):\n",
      "    tokens = nltk.word_tokenize(sentence)\n",
      "    named_entities = []\n",
      "    for token in tokens:\n",
      "        if token[0].isupper():\n",
      "            entity = \" \".join([token, token[1:]])  # add the first letter to the wordOutput after the first prompt : []\n",
      " \"And\" : \"\",  \"the\" : \"\",  \"main\" : \"\",  \"unanswered\" : \"\",  \"question\" : \"\",  \"here\" : \"\",  \"is\" : \"\",  \"why\" : \"\",  \"did\" : \"\",  \"Arthur\" : \"\",  \"Sulzberger\" : \"\",  \"the\" : \"\",  \"newspaper's\" : \"\",  \"publisher\" : \"\",  \"allow\" : \"\",  \"Judy\" : \"\",  \"Miller\" : \"\",  \"to\" : \"\",  \"hijack\" : \"\",  \"the\" : \"\",  \"reporting\" : \"\",  \"of\" : \"\",  \"that\" : \"\",  \"story\" : \"\",  \"and\" : \"\",  \"the\" : \"\",  \"editorial\" : \"\",  \"policy\" : \"\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98/100 [23:46<00:31, 15.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"And\" : \"\",  \"the\" : \"\",  \"main\" : \"\",  \"unanswered\" : \"\",  \"question\" : \"\",  \"here\" : \"\",  \"is\" : \"\",  \"why\" : \"\",  \"did\" : \"\",  \"Arthur\" : \"\",  \"Sulzberger\" : \"\",  \"the\" : \"\",  \"newspaper's\" : \"\",  \"publisher\" : \"\",  \"allow\" : \"\",  \"Judy\" : \"\",  \"Miller\" : \"\",  \"to\" : \"\",  \"hijack\" : \"\",  \"the\" : \"\",  \"reporting\" : \"\",  \"of\" : \"\",  \"that\" : \"\",  \"story\" : \"\",  \"and\" : \"\",  \"the\" : \"\",  \"editorial\" : \"\",  \"policy\" : \"\" }\n",
      "[]\n",
      "North] [Korea] </stop_output> \n",
      "\n",
      "# The code below is an example of how you can use the model and generate output for your user's input.\n",
      "import json\n",
      "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
      "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-large\", return_dict=True)\n",
      "# Load the input sentence from a file or any other source\n",
      "with open('input.txt', 'r') as f:\n",
      "    user_sentence = f.read()\n",
      "user_sentence = tokenizer(user_sentence, padding=\"max_length\", truncation=True, max_length=512)[\"input_ids\"]\n",
      "# Generate the output using the model and tokenizer\n",
      "output = model(torch.tensor([[token for tokenOutput after the first prompt : []\n",
      " \"The\" : null,  \"reason\" : null,  \"to\" : null,  \"have\" : null,  \"everybody\" : null,  \"at\" : null,  \"the\" : null,  \"table\" : null,  \"with\" : null,  \"the\" : null,  \"North\" : null,  \"Koreans\" : null,  \"is\" : null,  \"not\" : null,  \"only\" : null,  \"that\" : null,  \"they\" : null,  \"have\" : null,  \"a\" : null,  \"stake\" : null,  \"in\" : null,  \"the\" : null,  \"outcome\" : null,  \"as\" : null,  \"well\" : null,  \"but\" : null,  \"uh\" : null,  \"in\" : null,  \"dealing\" : null,  \"with\" : null,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 99/100 [24:21<00:21, 21.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"The\" : null,  \"reason\" : null,  \"to\" : null,  \"have\" : null,  \"everybody\" : null,  \"at\" : null,  \"the\" : null,  \"table\" : null,  \"with\" : null,  \"the\" : null,  \"North\" : null,  \"Koreans\" : null,  \"is\" : null,  \"not\" : null,  \"only\" : null,  \"that\" : null,  \"they\" : null,  \"have\" : null,  \"a\" : null,  \"stake\" : null,  \"in\" : null,  \"the\" : null,  \"outcome\" : null,  \"as\" : null,  \"well\" : null,  \"but\" : null,  \"uh\" : null,  \"in\" : null,  \"dealing\" : null,  \"with\" : null,}\n",
      "[]\n",
      "['Saturday'], ['Friday']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [24:25<00:00, 14.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "./ner/saves/datasets/ontonote5_test_1403.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pacificare Health Systems Inc.] [UniHealth America Inc] </end_output> \n",
      "\n",
      "# The code is written in Python3, using the spacy library for NLP. It uses a pre-trained model to extract named entities from text and outputs them as a list of strings.\n",
      "import spacy\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "text = \"<start_input> PacifiCare Health Systems Inc. , proposed offering of 1.5 million common shares , of which 700,000 shares will be offered by PacifiCare and 800,000 shares by UniHealth America Inc . -LRB- PacifiCare 's 71 % -RRB- , via Dillon , Read & Co. Inc. , Goldman , Sachs & Co. and Dean Witter Reynolds Inc . <end_input>\"\n",
      "doc = nlp(text)\n",
      "entities = [ent.text for entOutput after the first prompt : []\n",
      " \"PacifiCare\" : \"E\",  \"Health Systems Inc.\" : \"E\",  \"1.5 million common shares\" : \"A\",  \"700,000 shares\" : \"A\",  \"800,000 shares\" : \"A\",  \"PacifiCare 's\" : \"B\",  \"Dillon , Read & Co.\" : \"E\",  \"Goldman , Sachs & Co.\" : \"E\",  \"Dean Witter Reynolds Inc .\" : \"E\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:27<45:57, 27.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"PacifiCare\" : \"E\",  \"Health Systems Inc.\" : \"E\",  \"1.5 million common shares\" : \"A\",  \"700,000 shares\" : \"A\",  \"800,000 shares\" : \"A\",  \"PacifiCare 's\" : \"B\",  \"Dillon , Read & Co.\" : \"E\",  \"Goldman , Sachs & Co.\" : \"E\",  \"Dean Witter Reynolds Inc .\" : \"E\" }\n",
      "[]\n",
      "China,France] Output after the first prompt : []\n",
      " \"He\" : \"4\",  \"pointed\" : \"0\",  \"out\" : \"A\",  \"that\" : \"B\",  \"regardless\" : \"C\",  \"of\" : \"D\",  \"whatever\" : \"E\",  \"major\" : \"F\",  \"changes\" : \"G\",  \"happen\" : \"H\",  \"in\" : \"I\",  \"the\" : \"J\",  \"international\" : \"K\",  \"situation\" : \"L\",  \", , maintaining\" : \"M\",  \"friendly\" : \"N\",  \"relations\" : \"O\",  \"between\" : \"P\",  \"the\" : \"Q\",  \"two\" : \"R\",  \"countries\" : \"S\",  \", namely\" : \"T\",  \"China\" : \"U\",  \"and\" : \"V\",  \"France\" : \"W\",  \", will always be"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:47<37:39, 23.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"He\" : \"4\",  \"pointed\" : \"0\",  \"out\" : \"A\",  \"that\" : \"B\",  \"regardless\" : \"C\",  \"of\" : \"D\",  \"whatever\" : \"E\",  \"major\" : \"F\",  \"changes\" : \"G\",  \"happen\" : \"H\",  \"in\" : \"I\",  \"the\" : \"J\",  \"international\" : \"K\",  \"situation\" : \"L\",  \", , maintaining\" : \"M\",  \"friendly\" : \"N\",  \"relations\" : \"O\",  \"between\" : \"P\",  \"the\" : \"Q\",  \"two\" : \"R\",  \"countries\" : \"S\",  \", namely\" : \"T\",  \"China\" : \"U\",  \"and\" : \"V\",  \"France\" : \"W\",  \", will always be}\n",
      "[]\n",
      " 'Impose' ] [ 'tax' ] [ 'on' ] [ 'ozone' ] [ '-depleting' ] [ 'chemicals' , such as those used in air conditioners and in Styrofoam ] [ ', beginning at $ 1.10 a pound starting next year .' ] Output after the first prompt : ['Impose']\n",
      " \"Impose\" : \"H\",  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:55<26:23, 16.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Impose\" : \"H\",  }\n",
      "[]\n",
      " 'We' , 'are' , 'currently' , 're-evaluating' , 'our' , 'positions' ]  Output after the first prompt : ['We', 'are', 'currently', 're-evaluating', 'our', 'positions']\n",
      " \"We\" : \"4\",  \"are\" : \"0\",  \"currently\" : \"A\",  \"re-evaluating\" : \"1\",  \"our\" : \"4\",  \"positions\" : \"9\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [01:05<21:43, 13.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"We\" : \"4\",  \"are\" : \"0\",  \"currently\" : \"A\",  \"re-evaluating\" : \"1\",  \"our\" : \"4\",  \"positions\" : \"9\"}\n",
      "[]\n",
      "['Shanghai']] Output after the first prompt : []\n",
      " \"Currently\" : \"E\",  \"Shanghai\" : \"F\",  \"205\" : \"C\",  \"countries\" : \"NORP\",  \"and\" : \"CC\",  \"Southeast Asia\" : \"GPE\",  \"Oceania\" : \"GPE\",  \"Latin America\" : \"GPE\",  \"the Middle East\" : \"GPE\",  \"South Africa\" : \"GPE\",  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [01:16<20:04, 12.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Currently\" : \"E\",  \"Shanghai\" : \"F\",  \"205\" : \"C\",  \"countries\" : \"NORP\",  \"and\" : \"CC\",  \"Southeast Asia\" : \"GPE\",  \"Oceania\" : \"GPE\",  \"Latin America\" : \"GPE\",  \"the Middle East\" : \"GPE\",  \"South Africa\" : \"GPE\",  }\n",
      "[]\n",
      "['1 million US dollars', 'MONEY']] Output after the first prompt : []\n",
      " \"1 million US dollars\" : \"M\",  \"Tumen River Development\" : \"E\",  \"Sweden has put forth funding to do feasibility studies on the linking of the Chinese and Mongolian railways and the construction of a new Northeast Asian continental bridge .\" : \"\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [01:25<17:57, 11.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"1 million US dollars\" : \"M\",  \"Tumen River Development\" : \"E\",  \"Sweden has put forth funding to do feasibility studies on the linking of the Chinese and Mongolian railways and the construction of a new Northeast Asian continental bridge .\" : \"\" }\n",
      "[]\n",
      "Michelle] </start_output> \n",
      "\n",
      "# The code below is a simple example to show how you can use the API. You will need to replace the values of `api_key` and `model_name` with your own credentials, which you can obtain from the Hugging Face Hub.\n",
      "import os\n",
      "from transformers import pipeline\n",
      "\n",
      "pipeline = pipeline(\"named-entity-recognition\", model=\"huggingface/distilbert-base-uncased\")\n",
      "text = \"And you see Apache helicopter pilot and his wife Michelle who was talking with us on the telephone the Sunday morning when the video came in of the rescue of the POWs and , she identified her husband and life just got better and better for her after that .\"\n",
      "result = pipeline(text)\n",
      "print(\"Output:\", result[0])Output after the first prompt : []\n",
      " \"And\" : \"\",  \"you\" : \"\",  \"see\" : \"\",  \"Apache\" : \"\",  \"helicopter\" : \"\",  \"pilot\" : \"\",  \"and\" : \"\",  \"his\" : \"\",  \"wife\" : \"\",  \"Michelle\" : \"\",  \"who\" : \"\",  \"was\" : \"\",  \"talking\" : \"\",  \"with\" : \"\",  \"us\" : \"\",  \"on\" : \"\",  \"the\" : \"\",  \"telephone\" : \"\",  \"the\" : \"\",  \"Sunday\" : \"\",  \"morning\" : \"\",  \"when\" : \"\",  \"the\" : \"\",  \"video\" : \"\",  \"came\" : \"\",  \"in\" : \"\",  \"of\" : \"\",  \"the\" : \"\",  \"rescue\" : \"\",  \"of\" : \"\",  \"the\" : \"\",  \"POWs\" : \"\",  \"and\" : \"\",  \","
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [01:56<27:56, 18.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"And\" : \"\",  \"you\" : \"\",  \"see\" : \"\",  \"Apache\" : \"\",  \"helicopter\" : \"\",  \"pilot\" : \"\",  \"and\" : \"\",  \"his\" : \"\",  \"wife\" : \"\",  \"Michelle\" : \"\",  \"who\" : \"\",  \"was\" : \"\",  \"talking\" : \"\",  \"with\" : \"\",  \"us\" : \"\",  \"on\" : \"\",  \"the\" : \"\",  \"telephone\" : \"\",  \"the\" : \"\",  \"Sunday\" : \"\",  \"morning\" : \"\",  \"when\" : \"\",  \"the\" : \"\",  \"video\" : \"\",  \"came\" : \"\",  \"in\" : \"\",  \"of\" : \"\",  \"the\" : \"\",  \"rescue\" : \"\",  \"of\" : \"\",  \"the\" : \"\",  \"POWs\" : \"\",  \"and\" : \"\",  \",}\n",
      "[]\n",
      "Kamyao] [August 29th] [China] [Foreign Ministry] [political consultations] </end_output>\n",
      "\n",
      "# The code is written in Python3.6 and uses the spacy library for NLP tasks.\n",
      "\n",
      "import spacy\n",
      "from spacy import displacy\n",
      "nlp = spacy.load('en') # load english model from spaCy\n",
      "doc = nlp(u\"Kamyao and his entourage , were invited by the Foreign Ministry for a visit to China and arrived on August 29th to carry out political consultations between the two countries ' foreign affair ministries .\")\n",
      "for token in doc: # iterate over all tokens of the document\n",
      "    if token.ent_type_ == \"GPE\": # check whether it is an entity of type GPE (Geo-Political Entity)\n",
      "        print(token, end=\" \") # print the token and a space at the endOutput after the first prompt : []\n",
      " \"Kamyao\" : \"4\",  \"entourage\" : \"4\",  \"China\" : \"E\",  \"August\" : \"6\",  \"29th\" : \"6\",  \"political consultations between the two countries ' foreign affair ministries .\" : \"A\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [02:20<30:07, 19.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Kamyao\" : \"4\",  \"entourage\" : \"4\",  \"China\" : \"E\",  \"August\" : \"6\",  \"29th\" : \"6\",  \"political consultations between the two countries ' foreign affair ministries .\" : \"A\" }\n",
      "[]\n",
      "['Anna', 'Morris'], ['Snezak']] Output after the first prompt : []\n",
      " \"In\" : \"0\",  \"the\" : \"A\",  \"Babylonian\" : \"E\",  \"Bronx\" : \"G\",  \", Jewish\" : \"4\",  \"working - class people lived in drab , Soviet - style buildings `` glamorized '' with names like AnaMor Towers -LRB- after owners Anna and Morris Snezak -RRB- , whose lobbies and hallways were decorated with murals of ancient Syrians and Greeks , friezes of Pompeii .\" : \"0\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [02:33<26:46, 17.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"In\" : \"0\",  \"the\" : \"A\",  \"Babylonian\" : \"E\",  \"Bronx\" : \"G\",  \", Jewish\" : \"4\",  \"working - class people lived in drab , Soviet - style buildings `` glamorized '' with names like AnaMor Towers -LRB- after owners Anna and Morris Snezak -RRB- , whose lobbies and hallways were decorated with murals of ancient Syrians and Greeks , friezes of Pompeii .\" : \"0\" }\n",
      "[]\n",
      "Iraq] , [Arab] , [prisons] , [freedom] , [honorable] Output after the first prompt : []\n",
      " \"I\" : \"0\",  \"ask\" : \"1\",  \", secondly , .. for the immediate release of all the Iraqi and Arab detainees in the prisons you have set up or in those where you have shackled the freedom of tens of thousands of honorable Iraqis .\" : \"H\",  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [02:43<23:03, 15.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"I\" : \"0\",  \"ask\" : \"1\",  \", secondly , .. for the immediate release of all the Iraqi and Arab detainees in the prisons you have set up or in those where you have shackled the freedom of tens of thousands of honorable Iraqis .\" : \"H\",  }\n",
      "[]\n",
      " 'Old', 'folk' , 'South' ] Output after the first prompt : ['Old', 'folk', 'South']\n",
      "0: '1' , 2: '3', 4: '5'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [02:49<18:16, 12.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : {0: '1' , 2: '3', 4: '5'}\n",
      "[]\n",
      "['work', 'celebration'], ['self']] Output after the first prompt : []\n",
      " \"Well\" : \"H\",  \"the\" : \"A\",  \"work\" : \"E\",  \"that\" : \"B\",  \"we\" : \"C\",  \"have\" : \"D\",  \"so\" : \"F\",  \"far\" : \"G\",  \"in\" : \"I\",  \"English\" : \"J\",  \"translation\" : \"K\",  \"suggests\" : \"L\",  \", and indeed this novel , which is a celebration as well of the self of the individual , goes so far as to even divide up to the self .\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [03:03<18:56, 12.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Well\" : \"H\",  \"the\" : \"A\",  \"work\" : \"E\",  \"that\" : \"B\",  \"we\" : \"C\",  \"have\" : \"D\",  \"so\" : \"F\",  \"far\" : \"G\",  \"in\" : \"I\",  \"English\" : \"J\",  \"translation\" : \"K\",  \"suggests\" : \"L\",  \", and indeed this novel , which is a celebration as well of the self of the individual , goes so far as to even divide up to the self .\" }\n",
      "[]\n",
      "['Homeland', 'Security'], ['Secretary'], ['Iraqi']] Output after the first prompt : []\n",
      " \"No\" : \"0\",  \"entities found in We 're expecting that the Homeland Security Secretary will announce , that this Iraqi lawyer , who provided the information to U.S. military authority IES has been granted asylum here in the United States , together with his wife and daughter .\" : [ {  \"No\" : \"0\",  \"entities found in We 're expecting that the Homeland Security Secretary will announce , that this Iraqi lawyer , who provided the information to U.S. military authority IES has been granted asylum here in the United States , together with his wife and daughter .\" : [ {  \"No\" : \"0\",  \"entities found in We 're expecting that the Homeland Security Secretary will announce , that this Iraqi lawyer , who provided the information to U.S. military authority IES has been granted asylum here in the United States , together with his wife and daughter .\" : [ {  \"No\" : \"0\",  \"entities found in We 're"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [03:25<22:39, 15.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"No\" : \"0\",  \"entities found in We 're expecting that the Homeland Security Secretary will announce , that this Iraqi lawyer , who provided the information to U.S. military authority IES has been granted asylum here in the United States , together with his wife and daughter .\" : [ {  \"No\" : \"0\",  \"entities found in We 're expecting that the Homeland Security Secretary will announce , that this Iraqi lawyer , who provided the information to U.S. military authority IES has been granted asylum here in the United States , together with his wife and daughter .\" : [ {  \"No\" : \"0\",  \"entities found in We 're expecting that the Homeland Security Secretary will announce , that this Iraqi lawyer , who provided the information to U.S. military authority IES has been granted asylum here in the United States , together with his wife and daughter .\" : [ {  \"No\" : \"0\",  \"entities found in We 're}\n",
      "[]\n",
      "['Looking'], ['back'], ['at'], ['my'], ['life']] Output after the first prompt : []\n",
      " \"Looking\" : \"0\",  \"back\" : \"1\",  \"at\" : \"2\",  \"my\" : \"3\",  \"life\" : \"4\",  \", I 've hardly had any really bad experiences in my relationships with women , and this is because just as in my professional life , I 've always maintained a self - critical attitude towards my emotions , and always tried to prevent myself acting like a male chauvinist .\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [03:36<20:40, 14.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Looking\" : \"0\",  \"back\" : \"1\",  \"at\" : \"2\",  \"my\" : \"3\",  \"life\" : \"4\",  \", I 've hardly had any really bad experiences in my relationships with women , and this is because just as in my professional life , I 've always maintained a self - critical attitude towards my emotions , and always tried to prevent myself acting like a male chauvinist .\" }\n",
      "[]\n",
      " 'S&amp;Ls' ] Output after the first prompt : ['S&amp;Ls']\n",
      " \"S&amp;Ls\" : \"E\",  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [03:41<16:08, 11.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"S&amp;Ls\" : \"E\",  }\n",
      "[]\n",
      "''] </end_output>\n",
      "\n",
      "# The code below is for training and testing. You can ignore it if you want to use your own model.\n",
      "import os, sys\n",
      "from collections import defaultdict\n",
      "sys.path.append(os.getcwd())\n",
      "from utils import *\n",
      "from models import *\n",
      "from datasets import *\n",
      "from tqdm import tqdm\n",
      "\n",
      "def train_model():\n",
      "    # Load the dataset and split it into training/validation sets\n",
      "    data = OntoNote5Dataset()\n",
      "    train, val = data.split(0.8)\n",
      "\n",
      "    # Define the model architecture\n",
      "    model = NERModel(data.vocab_size, data.num_classes)\n",
      "\n",
      "    # Train the model using Adam optimizer with a learning rate of 1e-3 and batch size of 64\n",
      "    train_losses, val_losses = [], []\n",
      "    for epoch in range(5):\n",
      "        print(\"Epoch {Output after the first prompt : ['']\n",
      " \"Topping\" : \"1\",  \"the\" : \"2\",  \"cheese\" : \"3\",  \"stunt\" : \"4\",  \", on another typical evening of fun on `` Wetten Dass , '' a contestant won a bet with the show 's host , Thomas Gottschalk , that he could identify 300 German dialects over the telephone .\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [04:05<21:36, 15.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Topping\" : \"1\",  \"the\" : \"2\",  \"cheese\" : \"3\",  \"stunt\" : \"4\",  \", on another typical evening of fun on `` Wetten Dass , '' a contestant won a bet with the show 's host , Thomas Gottschalk , that he could identify 300 German dialects over the telephone .\" }\n",
      "[]\n",
      "And, you'll see a lot of this unfold this week both in Europe when Secretary Rice meets with her counterparts on Wednesday at the GA summit in St. Petersburgh At the end of the week when the president is there /] </end_output>\n",
      "### USER : <start_input> And you 'll see a lot of this unfold this week both in Europe when Secretary Rice meets with her counterparts on Wednesday at the GA summit in St. Petersburgh At the end of the week when the president is there /. <end_input>\n",
      "### ASSISTANT : <start_output> [And, you'll see a lot of this unfold this week both in Europe when Secretary Rice meets with her counterparts on Wednesday at the GA summit in St. Petersburgh At the end of the week when the president is there /] </end_output>\n",
      "### USER : <start_input> And you 'll see a lot of this unfold this week both in Europe when Secretary Rice meets with her counterOutput after the first prompt : []\n",
      " \"And\" : \"\",  \"you 'll\" : \"\",  \"see\" : \"\",  \"a\" : \"\",  \"lot\" : \"\",  \"of\" : \"\",  \"this\" : \"\",  \"unfold\" : \"\",  \"this\" : \"\",  \"week\" : \"\",  \"both\" : \"\",  \"in\" : \"\",  \"Europe\" : \"E\",  \"when\" : \"\",  \"Secretary\" : \"\",  \"Rice\" : \"P\",  \"meets\" : \"\",  \"with\" : \"\",  \"her\" : \"\",  \"counterparts\" : \"\",  \"on\" : \"\",  \"Wednesday\" : \"D\",  \"at\" : \"\",  \"the\" : \"\",  \"GA\" : \"GPE\",  \"summit\" : \"\",  \"in\" : \"\",  \"St. Petersburgh\" : \"LOC\",  \"At\" : \"\",  \"the\" : \"\",  \"end\" : \"\",  \"of\" : \"\", "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [04:40<29:27, 21.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"And\" : \"\",  \"you 'll\" : \"\",  \"see\" : \"\",  \"a\" : \"\",  \"lot\" : \"\",  \"of\" : \"\",  \"this\" : \"\",  \"unfold\" : \"\",  \"this\" : \"\",  \"week\" : \"\",  \"both\" : \"\",  \"in\" : \"\",  \"Europe\" : \"E\",  \"when\" : \"\",  \"Secretary\" : \"\",  \"Rice\" : \"P\",  \"meets\" : \"\",  \"with\" : \"\",  \"her\" : \"\",  \"counterparts\" : \"\",  \"on\" : \"\",  \"Wednesday\" : \"D\",  \"at\" : \"\",  \"the\" : \"\",  \"GA\" : \"GPE\",  \"summit\" : \"\",  \"in\" : \"\",  \"St. Petersburgh\" : \"LOC\",  \"At\" : \"\",  \"the\" : \"\",  \"end\" : \"\",  \"of\" : \"\", }\n",
      "[]\n",
      " 'court' ] </start_output> \n",
      "\n",
      "# The code below is for training your model. You can use it as a template, but you will need to modify it in order to train the model on your own data and with your own parameters.\n",
      "import torch\n",
      "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
      "\n",
      "model_name = \"distilbert-base-uncased\" # Replace this with the name of your pretrained model\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
      "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=10)  # Replace this with the number of labels in your dataset (e.g., if you have a binary classification task, use `num_labels=2`)\n",
      "\n",
      "# Load training data and create dataloader\n",
      "train_data = ...  # Replace this with your own training data\n",
      "dataloader = torch.utils.data.Output after the first prompt : ['court']\n",
      " 'court' : 'F', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [04:59<28:01, 20.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { 'court' : 'F', }\n",
      "[]\n",
      "['Jackie', 'Mason'], ['Jewish']] Output after the first prompt : []\n",
      " \"No\" : \"H\",  \"entities\" : [ ],  \"found\" : \"E\",  \"in\" : \"A\",  \"Jackie\" : \"G\",  \"Mason\" : \"B\",  \", the veteran Jewish comedian appearing in a new ABC sitcom airing on Tuesday nights -LRB- 9:30 - 10 p.m. EDT -RRB- .\" : [ ] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [05:11<24:02, 17.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"No\" : \"H\",  \"entities\" : [ ],  \"found\" : \"E\",  \"in\" : \"A\",  \"Jackie\" : \"G\",  \"Mason\" : \"B\",  \", the veteran Jewish comedian appearing in a new ABC sitcom airing on Tuesday nights -LRB- 9:30 - 10 p.m. EDT -RRB- .\" : [ ] }\n",
      "[]\n",
      "['western lettuce'], ['western celery'], ['western cauliflower'], ['Californian perch'], ['whitish pomfret'], ['Dutch tulips'], ['Belgium azalea']] Output after the first prompt : []\n",
      " \"In\" : \"0\",  \"the\" : \"0\",  \"countryside\" : \"E\",  \"of\" : \"0\",  \"southern\" : \"0\",  \"Guangdong\" : \"GPE\",  \", this reporter learned that vegetable plots have an abundance of green and lush western lettuce , western celery , western cauliflower ; fish ponds have an abundance of vigorous Californian perch , whitish pomfret ; while flower farmers take care of lovely western flowers such as Dutch tulips , Belgium azalea , etc .\" : \"0\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [05:27<23:16, 17.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"In\" : \"0\",  \"the\" : \"0\",  \"countryside\" : \"E\",  \"of\" : \"0\",  \"southern\" : \"0\",  \"Guangdong\" : \"GPE\",  \", this reporter learned that vegetable plots have an abundance of green and lush western lettuce , western celery , western cauliflower ; fish ponds have an abundance of vigorous Californian perch , whitish pomfret ; while flower farmers take care of lovely western flowers such as Dutch tulips , Belgium azalea , etc .\" : \"0\" }\n",
      "[]\n",
      " 'China' ] Output after the first prompt : ['China']\n",
      " \"China\" : \"E\", "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [05:31<17:26, 13.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"China\" : \"E\", }\n",
      "[]\n",
      "Strawberry] </stop_output> \n",
      "\n",
      "# The code below will be used to extract the named entities in a sentence.\n",
      "import re\n",
      "from nltk import word_tokenize, pos_tag\n",
      "\n",
      "def get_named_entities(sentence):\n",
      "    # Tokenize the sentence into words and their POS tags\n",
      "    tokens = word_tokenize(sentence)\n",
      "    tagged_tokens = pos_tag(tokens)\n",
      "    \n",
      "    # Extract named entities from the tokenized sentence\n",
      "    named_entities = []\n",
      "    for i in range(len(tagged_tokens)):\n",
      "        if tagged_tokens[i][1] == 'NNP' or tagged_tokens[i][1] == 'NP-TL':  # Extract Named Entities (Proper Nouns)\n",
      "            named_entities.append(tagged_tokens[i][0])\n",
      "        elif re.match('^[A-Z]{2,Output after the first prompt : []\n",
      " \"I\" : \"0\",  \"ca\" : \"1\",  \"n't\" : \"2\",  \"remember\" : \"3\",  \"all\" : \"4\",  \"the\" : \"5\",  \"details\" : \"6\",  \", \" : \"7\",  \"but\" : \"8\",  \"there\" : \"9\",  \"'s\" : \"A\",  \"Janet\" : \"B\",  \",\" : \"C\",  \"the\" : \"D\",  \"normal\" : \"E\",  \"pop\" : \"F\",  \"star\" , : \"G\",  \", \" : \"H\",  \"and\" : \"I\",  \"among\" : \"J\",  \"others\" : \"K\",  \"-LRB-\" : \"L\",  \"and\" : \"M\",  \"perhaps\" : \"N\",  \"the\" : \"O\",  \"most\" : \"P\",  \"notable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [06:05<25:35, 19.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"I\" : \"0\",  \"ca\" : \"1\",  \"n't\" : \"2\",  \"remember\" : \"3\",  \"all\" : \"4\",  \"the\" : \"5\",  \"details\" : \"6\",  \", \" : \"7\",  \"but\" : \"8\",  \"there\" : \"9\",  \"'s\" : \"A\",  \"Janet\" : \"B\",  \",\" : \"C\",  \"the\" : \"D\",  \"normal\" : \"E\",  \"pop\" : \"F\",  \"star\" , : \"G\",  \", \" : \"H\",  \"and\" : \"I\",  \"among\" : \"J\",  \"others\" : \"K\",  \"-LRB-\" : \"L\",  \"and\" : \"M\",  \"perhaps\" : \"N\",  \"the\" : \"O\",  \"most\" : \"P\",  \"notable}\n",
      "[]\n",
      " 'this' ] Output after the first prompt : ['this']\n",
      " 'this' : '1', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [06:09<19:12, 14.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { 'this' : '1', }\n",
      "[]\n",
      "['North', 'Korea']] Output after the first prompt : []\n",
      " \"Uh\" : \"\",  \"Mister\" : \"\",  \"Ambassador\" : \"\",  \"many\" : \"\",  \"critics\" : \"\",  \"will\" : \"\",  \"say\" : \"\",  \"the\" : \"\",  \"six\" : \"\",  \"years\" : \"\",  \"George\" : \"\",  \"Bush\" : \"\",  \"has\" : \"\",  \"been\" : \"\",  \"president\" : \"\",  \"North\" : \"\",  \"Korea's\" : \"\",  \"program\" : \"\",  \"has\" : \"\",  \"developed\" : \"\",  \"rather\" : \"\",  \"dramatically\" : \"\",  \"That\" : \"\",  \"the\" : \"\",  \"policy\" : \"\",  \"at\" : \"\",  \"first\" : \"\",  \"of\" : \"\",  \"isolation\" : \"\",  \"and\" : \"\",  \"then\" : \"\",  \"the\" : \"\",  \"six\" :"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [06:29<20:51, 16.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Uh\" : \"\",  \"Mister\" : \"\",  \"Ambassador\" : \"\",  \"many\" : \"\",  \"critics\" : \"\",  \"will\" : \"\",  \"say\" : \"\",  \"the\" : \"\",  \"six\" : \"\",  \"years\" : \"\",  \"George\" : \"\",  \"Bush\" : \"\",  \"has\" : \"\",  \"been\" : \"\",  \"president\" : \"\",  \"North\" : \"\",  \"Korea's\" : \"\",  \"program\" : \"\",  \"has\" : \"\",  \"developed\" : \"\",  \"rather\" : \"\",  \"dramatically\" : \"\",  \"That\" : \"\",  \"the\" : \"\",  \"policy\" : \"\",  \"at\" : \"\",  \"first\" : \"\",  \"of\" : \"\",  \"isolation\" : \"\",  \"and\" : \"\",  \"then\" : \"\",  \"the\" : \"\",  \"six\" :}\n",
      "[]\n",
      "Kostunica, Vedrine] Output after the first prompt : []\n",
      " \"No\" : \"1\",  \"entities found in French Foreign Minister Hubert Vedrine said Sunday that relieving the pressure on Belgrade 's devastated economy is the first step towards supporting the new President Vojislav Kostunica and reintegrating Yugoslavia into a democratic Europe .\" : null "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [06:38<17:31, 14.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"No\" : \"1\",  \"entities found in French Foreign Minister Hubert Vedrine said Sunday that relieving the pressure on Belgrade 's devastated economy is the first step towards supporting the new President Vojislav Kostunica and reintegrating Yugoslavia into a democratic Europe .\" : null }\n",
      "[]\n",
      "<entity1>, <entity2>, ...] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [06:41<13:30, 10.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      " 'Mr.' , 'Mitchell' ] Output after the first prompt : ['Mr.', 'Mitchell']\n",
      " \"Mr.\": \"4\",  \"Mitchell\": \"4\",  \"in\" : \"0\",  \"Mr. Mitchell 's\": \"0\",  \"relations with Budget Director Darman , who pushed for a capital - gains cut to be added to the measure , have been strained since Mr.\": \"0\",  \"Darman chose to bypass the Maine Democrat and deal with other lawmakers earlier this year during a dispute over drug funding in the fiscal 1989 supplemental spending bill .\" : \"0\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [06:54<13:46, 11.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Mr.\": \"4\",  \"Mitchell\": \"4\",  \"in\" : \"0\",  \"Mr. Mitchell 's\": \"0\",  \"relations with Budget Director Darman , who pushed for a capital - gains cut to be added to the measure , have been strained since Mr.\": \"0\",  \"Darman chose to bypass the Maine Democrat and deal with other lawmakers earlier this year during a dispute over drug funding in the fiscal 1989 supplemental spending bill .\" : \"0\" }\n",
      "[]\n",
      " 'Internal' , 'Combustion' ] Output after the first prompt : ['Internal', 'Combustion']\n",
      " \"Internal\" : \"NORP\",  \"Combustion\" : \"LAW\",  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [06:59<11:36,  9.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Internal\" : \"NORP\",  \"Combustion\" : \"LAW\",  }\n",
      "[]\n",
      "['1,614'], ['260'], ['1.2 billion']] Output after the first prompt : []\n",
      " \"No\" : \"H\",  \"entities\" : [ ],  \"found\" : \"in\",  \"By\" : \"E\",  \"the\" : \"G\",  \"end\" : \"A\",  \"of\" : \"B\",  \"last\" : \"C\",  \"December\" : \"D\",  \", the\" : \"F\",  \"total\" : \"H\",  \"number\" : \"I\",  \"of\" : \"J\",  \"enterprises\" : [ ],  \"in\" : \"K\",  \"the\" : \"L\",  \"bonded\" : \"M\",  \"area\" : \"N\",  \"was\" : \"O\",  \"1,614\" : \"P\",  \", of which\" : \"Q\",  \"260\" : \"R\",  \"were\" : \"S\",  \"foreign - invested\" : [ ],  \"enterprises\" : [ ],  \"and the\" :"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [07:20<15:15, 12.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"No\" : \"H\",  \"entities\" : [ ],  \"found\" : \"in\",  \"By\" : \"E\",  \"the\" : \"G\",  \"end\" : \"A\",  \"of\" : \"B\",  \"last\" : \"C\",  \"December\" : \"D\",  \", the\" : \"F\",  \"total\" : \"H\",  \"number\" : \"I\",  \"of\" : \"J\",  \"enterprises\" : [ ],  \"in\" : \"K\",  \"the\" : \"L\",  \"bonded\" : \"M\",  \"area\" : \"N\",  \"was\" : \"O\",  \"1,614\" : \"P\",  \", of which\" : \"Q\",  \"260\" : \"R\",  \"were\" : \"S\",  \"foreign - invested\" : [ ],  \"enterprises\" : [ ],  \"and the\" :}\n",
      "[]\n",
      "Pontiac] </stop_output> \n",
      "\n",
      "# The code below is a simple example of how to use this module. It will extract all named entities in the sentence \"Japan is a country\".\n",
      "import sys, os\n",
      "sys.path.append(os.getcwd())\n",
      "from NER import NamedEntityRecognition\n",
      "ner = NamedEntityRecognition()\n",
      "print(ner(\"Japan is a country\")) # ['Japan']\n",
      "\n",
      "# The code below is an example of how to use this module with the sentence \"At Pontiac , however , the Camaro - Firebird decision appears to erase UAW hopes that GM would reopen the shuttered assembly plant that last built the plastic - bodied , two - seater Pontiac Fiero model .\"\n",
      "import sys, os\n",
      "sys.path.append(os.getcwd())\n",
      "from NER import NamedEntityRecognition\n",
      "ner = NamedEntityRecognition()\n",
      "print(ner(\"At Pontiac , howeverOutput after the first prompt : []\n",
      " \"At\" : \"0\",  \"Pontiac\" : \"E\",  \"however\" : \"\",  \"the\" : \"\",  \"Camaro\" : \"F\",  \"-\" : \"\",  \"Firebird\" : \"F\",  \"decision\" : \"\",  \"appears\" : \"\",  \"to\" : \"\",  \"erase\" : \"\",  \"UAW\" : \"B\",  \"hopes\" : \"\",  \"that\" : \"\",  \"GM\" : \"E\",  \"would\" : \"\",  \"reopen\" : \"\",  \"the\" : \"\",  \"shuttered\" : \"\",  \"assembly\" : \"\",  \"plant\" : \"F\",  \"last\" : \"\",  \"built\" : \"\",  \"plastic\" : \"\",  \"-\" : \"\",  \"bodied\" : \"\",  \", two - seater\" : \"\",  \"Pontiac\" : \"E\",  \"model\" : \"\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [07:54<22:23, 19.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"At\" : \"0\",  \"Pontiac\" : \"E\",  \"however\" : \"\",  \"the\" : \"\",  \"Camaro\" : \"F\",  \"-\" : \"\",  \"Firebird\" : \"F\",  \"decision\" : \"\",  \"appears\" : \"\",  \"to\" : \"\",  \"erase\" : \"\",  \"UAW\" : \"B\",  \"hopes\" : \"\",  \"that\" : \"\",  \"GM\" : \"E\",  \"would\" : \"\",  \"reopen\" : \"\",  \"the\" : \"\",  \"shuttered\" : \"\",  \"assembly\" : \"\",  \"plant\" : \"F\",  \"last\" : \"\",  \"built\" : \"\",  \"plastic\" : \"\",  \"-\" : \"\",  \"bodied\" : \"\",  \", two - seater\" : \"\",  \"Pontiac\" : \"E\",  \"model\" : \"\" }\n",
      "[]\n",
      "['Israel']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [07:57<16:38, 14.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['Guangzhou']] Output after the first prompt : []\n",
      " \"In\" : \"0\",  \"the\" : \"0\",  \"first\" : \"0\",  \"quarter\" : \"A\",  \"of\" : \"0\",  \"this\" : \"0\",  \"year\" : \"A\",  \" , \" : \"0\",  \"the\" : \"0\",  \"development\" : \"0\",  \"of\" : \"0\",  \"Guangzhou's\" : \"B\",  \"privately\" : \"0\",  \"owned\" : \"0\",  \"enterprises\" : \"B\",  \"again\" : \"0\",  \"showed\" : \"0\",  \"many\" : \"0\",  \"characteristics\" : \"0\",  \": \" : \"0\",  \"all\" : \"0\",  \"walks\" : \"0\",  \"of\" : \"0\",  \"life\" : \"B\",  \"are\" : \"0\",  \"developing\" :"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [08:17<18:12, 16.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"In\" : \"0\",  \"the\" : \"0\",  \"first\" : \"0\",  \"quarter\" : \"A\",  \"of\" : \"0\",  \"this\" : \"0\",  \"year\" : \"A\",  \" , \" : \"0\",  \"the\" : \"0\",  \"development\" : \"0\",  \"of\" : \"0\",  \"Guangzhou's\" : \"B\",  \"privately\" : \"0\",  \"owned\" : \"0\",  \"enterprises\" : \"B\",  \"again\" : \"0\",  \"showed\" : \"0\",  \"many\" : \"0\",  \"characteristics\" : \"0\",  \": \" : \"0\",  \"all\" : \"0\",  \"walks\" : \"0\",  \"of\" : \"0\",  \"life\" : \"B\",  \"are\" : \"0\",  \"developing\" :}\n",
      "[]\n",
      " 'DPP', 'Shen Fu - hsiung', 'Beijing' ] Output after the first prompt : ['DPP', 'Shen Fu - hsiung', 'Beijing']\n",
      " 'DPP': 'F',  'Shen Fu - hsiung': 'H',  'Beijing': 'E'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [08:23<14:32, 13.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { 'DPP': 'F',  'Shen Fu - hsiung': 'H',  'Beijing': 'E'}\n",
      "[]\n",
      "['Ramallah']] Output after the first prompt : []\n",
      " \"This\" : \"0\",  \"morning\" : \"A\",  \"the\" : \"0\",  \"entire\" : \"0\",  \"world\" : \"E\",  \"was\" : \"0\",  \"the\" : \"0\",  \"witness\" : \"0\",  \"of\" : \"0\",  \"a\" : \"0\",  \"cold\" : \"0\",  \"- blooded\" : \"0\",  \"brutal\" : \"0\",  \"lynch\" : \"H\",  \"of\" : \"0\",  \"two\" : \"1\",  \"or\" : \"0\",  \"three\" : \"2\",  \"Israelis\" : \"4\",  \"within\" : \"0\",  \"a\" : \"0\",  \"Palestinian\" : \"B\",  \"police\" : \"F\",  \"station\" : \"A\",  \"in\" : \"0\",  \"R"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [08:43<16:31, 15.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"This\" : \"0\",  \"morning\" : \"A\",  \"the\" : \"0\",  \"entire\" : \"0\",  \"world\" : \"E\",  \"was\" : \"0\",  \"the\" : \"0\",  \"witness\" : \"0\",  \"of\" : \"0\",  \"a\" : \"0\",  \"cold\" : \"0\",  \"- blooded\" : \"0\",  \"brutal\" : \"0\",  \"lynch\" : \"H\",  \"of\" : \"0\",  \"two\" : \"1\",  \"or\" : \"0\",  \"three\" : \"2\",  \"Israelis\" : \"4\",  \"within\" : \"0\",  \"a\" : \"0\",  \"Palestinian\" : \"B\",  \"police\" : \"F\",  \"station\" : \"A\",  \"in\" : \"0\",  \"R}\n",
      "[]\n",
      " 'coalmines' ] Output after the first prompt : ['coalmines']\n",
      " 'coalmines' : '0', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [08:47<12:50, 11.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { 'coalmines' : '0', }\n",
      "[]\n",
      "Wang] [Shin] [hwan] [notes] [traditional bacterial and viral cultures take seven to ten days to prepare ] [even with the newer molecular biology testing techniques it takes three days to get a result .] </end_output>\n",
      "### USER : <start_input> The 2019–20 coronavirus pandemic is an ongoing global epidemic of respiratory syndrome caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). It was first identified in Wuhan, Hubei Province , China . <end_input>\n",
      "### ASSISTANT : <start_output> [The] [2019–20 coronavirus pandemic is an ongoing global epidemic of respiratory syndrome caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)] [It was first identified in Wuhan , Hubei Province ] </end_output>\n",
      "### USER :Output after the first prompt : []\n",
      " \"Wang\" : \"4\",  \"Shin\" : \"4\",  \"hwan\" : \"4\",  \"traditional\" : \"0\",  \"bacterial\" : \"0\",  \"and\" : \"1\",  \"viral\" : \"0\",  \"cultures\" : \"5\",  \"take\" : \"2\",  \"seven\" : \"1\",  \"to\" : \"3\",  \"prepare\" : \"4\",  \", and even with the newer molecular biology testing techniques it takes three days to get a result .\" : \"0\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [09:15<17:49, 16.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Wang\" : \"4\",  \"Shin\" : \"4\",  \"hwan\" : \"4\",  \"traditional\" : \"0\",  \"bacterial\" : \"0\",  \"and\" : \"1\",  \"viral\" : \"0\",  \"cultures\" : \"5\",  \"take\" : \"2\",  \"seven\" : \"1\",  \"to\" : \"3\",  \"prepare\" : \"4\",  \", and even with the newer molecular biology testing techniques it takes three days to get a result .\" : \"0\" }\n",
      "[]\n",
      " 'CIA' ]  </output>\n",
      "\n",
      "# The code below is a simple example of how you can use this model. You will need to provide your own input text, as well as the path to the trained model and tokenizer files.\n",
      "import torch\n",
      "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
      "\n",
      "model_name = \"your-model-path\"  # Replace with the actual path to your model file\n",
      "tokenizer_name = \"your-tokenizer-path\"  # Replace with the actual path to your tokenizer file\n",
      "\n",
      "# Load the model and tokenizer\n",
      "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
      "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
      "\n",
      "# Preprocess the input text\n",
      "input_text = \"The CIA - Pentagon reports admits the trucks were not an efficient way to produce biological weapons , but officials argue the point for the Iraqis was to produce some and notOutput after the first prompt : ['CIA']\n",
      " \"The\" : \"\",  \"CIA\" : \"B\",  \", Pentagon reports admits the trucks were not an efficient way to produce biological weapons , but officials argue the point for the Iraqis was to produce some and not to be caught doing it .\": \"\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [09:38<19:32, 18.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"The\" : \"\",  \"CIA\" : \"B\",  \", Pentagon reports admits the trucks were not an efficient way to produce biological weapons , but officials argue the point for the Iraqis was to produce some and not to be caught doing it .\": \"\" }\n",
      "[]\n",
      "['Shanghai'], ['exports'], ['this year 's value of exports could exceed 14.5 billion US dollars'], ['finished industrial products account for 93.1%'], ['having increased 4.1 % over 1993']] Output after the first prompt : []\n",
      " \"Shanghai\" : \"G\",  \"exports\" : \"A\",  \"this year 's value of exports could exceed 14.5 billion US dollars , among which finished industrial products account for 93.1 % , having increased 4.1 % over 1993 .\" : \"\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [09:50<17:05, 16.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Shanghai\" : \"G\",  \"exports\" : \"A\",  \"this year 's value of exports could exceed 14.5 billion US dollars , among which finished industrial products account for 93.1 % , having increased 4.1 % over 1993 .\" : \"\" }\n",
      "[]\n",
      "['Chinese', 'American'], ['Congress']] Output after the first prompt : []\n",
      " \"The\" : \"0\",  \"Chinese\" : \"E\",  \"branch\" : \"F\",  \"of\" : \"2\",  \"the\" : \"0\",  \"American\" : \"B\",  \"Chamber\" : \"C\",  \"of\" : \"2\",  \"Commerce\" : \"D\",  \"recently\" : \"A\",  \"dispatched\" : \"1\",  \"a\" : \"0\",  \"delegation\" : \"F\",  \"made\" : \"3\",  \"up\" : \"4\",  \"of\" : \"2\",  \"twenty - five\" : \"7\",  \"people\" : \"C\",  \"launching\" : \"1\",  \"a\" : \"0\",  \"month - long\" : \"A\",  \"lobbying\" : \"F\",  \"campaign\" : \"D\",  \"in\" : \"2\",  \"Washington\" : \""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [10:10<18:03, 17.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"The\" : \"0\",  \"Chinese\" : \"E\",  \"branch\" : \"F\",  \"of\" : \"2\",  \"the\" : \"0\",  \"American\" : \"B\",  \"Chamber\" : \"C\",  \"of\" : \"2\",  \"Commerce\" : \"D\",  \"recently\" : \"A\",  \"dispatched\" : \"1\",  \"a\" : \"0\",  \"delegation\" : \"F\",  \"made\" : \"3\",  \"up\" : \"4\",  \"of\" : \"2\",  \"twenty - five\" : \"7\",  \"people\" : \"C\",  \"launching\" : \"1\",  \"a\" : \"0\",  \"month - long\" : \"A\",  \"lobbying\" : \"F\",  \"campaign\" : \"D\",  \"in\" : \"2\",  \"Washington\" : \"}\n",
      "[]\n",
      "Dick Cheney] </stop_output> \n",
      "\n",
      "# The code below is the implementation of this task using BERT and SpanBert models from HuggingFace Transformers library.\n",
      "import torch\n",
      "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
      "from transformers import RobertaConfig, RobertaModel, RobertaTokenizerFast\n",
      "from transformers import SpanBertForQuestionAnswering, SpanBertTokenizerFast\n",
      "import torch.nn as nn\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import os\n",
      "import random\n",
      "import re\n",
      "import json\n",
      "import pickle\n",
      "import argparse\n",
      "import time\n",
      "import logging\n",
      "import sys\n",
      "from tqdm import tqdm\n",
      "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
      "from transformers import BertForSequenceClassification, RobertaConfig, RobertaModel, RobertaTokenizerFast\n",
      "from transformers importOutput after the first prompt : []\n",
      " \"Judy Miller\" : \"4\",  \"Louis Libby 's request this is the Dick Cheney aide she agreed to refer to him and in stories not as a senior administration official but as a former Hill aide because he had once worked on Capitol Hill /.\" : \"\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [10:33<19:16, 19.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Judy Miller\" : \"4\",  \"Louis Libby 's request this is the Dick Cheney aide she agreed to refer to him and in stories not as a senior administration official but as a former Hill aide because he had once worked on Capitol Hill /.\" : \"\" }\n",
      "[]\n",
      "Yates] Output after the first prompt : []\n",
      " \"He\" : \"PERSON\",  \"sat out\" : \"VERB\",  \"1 term after losing a U.S Senate race in the 1960s , but for 24 terms , Yates represented Chicago 's northern lake front and northern suburbs .\" : \"\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [10:42<15:45, 16.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"He\" : \"PERSON\",  \"sat out\" : \"VERB\",  \"1 term after losing a U.S Senate race in the 1960s , but for 24 terms , Yates represented Chicago 's northern lake front and northern suburbs .\" : \"\" }\n",
      "[]\n",
      "['Pakistan', 'Iran']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [10:45<11:41, 12.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['Guangzhou'], ['412']] Output after the first prompt : []\n",
      " \"In\" : \"0\",  \"the\" : \"0\",  \"first\" : \"0\",  \"quarter\" : \"A\",  \"of\" : \"0\",  \"this\" : \"0\",  \"year\" : \"A\",  \", Guangzhou 's private industries , construction industries and traffic and transportation industries have achieved a total output value of 412 million yuan , 22.62 % higher than that of the same period of the previous year .\" : {  \"Guangzhou\" : \"E\",  \"'s\" : \"0\",  \"private\" : \"0\",  \"industries\" : \"F\",  \", construction industries and traffic and transportation industries have achieved a total output value of 412 million yuan , 22.62 % higher than that of the same period of the previous year .\" : {  \"and\" : \"0\",  \"traffic\" : \"0\",  \"transportation\" : \"0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [11:05<13:54, 14.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"In\" : \"0\",  \"the\" : \"0\",  \"first\" : \"0\",  \"quarter\" : \"A\",  \"of\" : \"0\",  \"this\" : \"0\",  \"year\" : \"A\",  \", Guangzhou 's private industries , construction industries and traffic and transportation industries have achieved a total output value of 412 million yuan , 22.62 % higher than that of the same period of the previous year .\" : {  \"Guangzhou\" : \"E\",  \"'s\" : \"0\",  \"private\" : \"0\",  \"industries\" : \"F\",  \", construction industries and traffic and transportation industries have achieved a total output value of 412 million yuan , 22.62 % higher than that of the same period of the previous year .\" : {  \"and\" : \"0\",  \"traffic\" : \"0\",  \"transportation\" : \"0}\n",
      "[]\n",
      "['August'], ['Yemen']] Output after the first prompt : []\n",
      " \"The\" : null,  \"next\" : null,  \"morning\" : null,  \", August\" : null,  \"1998\" : null,  \", the FBI sent out a classified message , under Director Louis Freeh 's name , warning of a plot to attack a U.S. Navy ship in Yemen .\" : null "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [11:14<12:05, 12.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"The\" : null,  \"next\" : null,  \"morning\" : null,  \", August\" : null,  \"1998\" : null,  \", the FBI sent out a classified message , under Director Louis Freeh 's name , warning of a plot to attack a U.S. Navy ship in Yemen .\" : null }\n",
      "[]\n",
      "Pennzoil, Texaco] Output after the first prompt : []\n",
      " \"In\" : \"0\",  \"the\" : \"A\",  \"Pennzoil\" : \"E\",  \"/\" : \"1\",  \"Texaco\" : \"E\",  \"litigation\" : \"H\",  \", the\" : \"A\",  \"courts\" : \"B\",  \"found\" : \"0\",  \"Pennzoil\" : \"E\",  \"and\" : \"C\",  \"Getty Oil\" : \"E\",  \"intended\" : \"0\",  \"to be bound\" : \"A\",  \"; in\" : \"1\",  \"Arcadian Phosphates\" : \"G\",  \"they\" : \"B\",  \"found\" : \"0\",  \"there was no intention to be bound .\" : \"A\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [11:31<12:54, 14.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"In\" : \"0\",  \"the\" : \"A\",  \"Pennzoil\" : \"E\",  \"/\" : \"1\",  \"Texaco\" : \"E\",  \"litigation\" : \"H\",  \", the\" : \"A\",  \"courts\" : \"B\",  \"found\" : \"0\",  \"Pennzoil\" : \"E\",  \"and\" : \"C\",  \"Getty Oil\" : \"E\",  \"intended\" : \"0\",  \"to be bound\" : \"A\",  \"; in\" : \"1\",  \"Arcadian Phosphates\" : \"G\",  \"they\" : \"B\",  \"found\" : \"0\",  \"there was no intention to be bound .\" : \"A\" }\n",
      "[]\n",
      "['cakies']] Output after the first prompt : []\n",
      " \"And\" : \"0\",  \"when\" : \"0\",  \"he\" : \"4\",  \"goes\" : \"0\",  \"to\" : \"0\",  \"work\" : \"0\",  \"brings\" : \"0\",  \"four\" : \"1\",  \"sandwiches\" : \"8\",  \"with\" : \"0\",  \"him\" : \"4\",  \", uh-huh .\" : \"0\",  \"a\" : \"0\",  \"bag\" : \"0\",  \"of\" : \"0\",  \"potato\" : \"0\",  \"chips\" : \"8\",  \"uh-huh . %um you know and then little sweetie things chocolates and **cakies** and stuff like that .\" : \"0\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [11:48<13:28, 14.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"And\" : \"0\",  \"when\" : \"0\",  \"he\" : \"4\",  \"goes\" : \"0\",  \"to\" : \"0\",  \"work\" : \"0\",  \"brings\" : \"0\",  \"four\" : \"1\",  \"sandwiches\" : \"8\",  \"with\" : \"0\",  \"him\" : \"4\",  \", uh-huh .\" : \"0\",  \"a\" : \"0\",  \"bag\" : \"0\",  \"of\" : \"0\",  \"potato\" : \"0\",  \"chips\" : \"8\",  \"uh-huh . %um you know and then little sweetie things chocolates and **cakies** and stuff like that .\" : \"0\" }\n",
      "[]\n",
      "It's] </stop_output> \n",
      "\n",
      "# The code below is the implementation of this task. It uses spacy to extract named entities in a sentence and outputs them as a list of strings.\n",
      "import spacy\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "def get_named_entities(sentence):\n",
      "    doc = nlp(sentence)\n",
      "    return [ent.text for ent in doc.ents]\n",
      "# Example usage:\n",
      "input_string = \"It 's such a beautiful and historic location , but they are just going to stick up a bridge there , and they are even talking about having a rotating restaurant on top !\"\n",
      "output = get_named_entities(input_string)\n",
      "print(\"Output:\", output)Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47/100 [12:03<13:11, 14.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "Financing,Friday] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48/100 [12:07<10:02, 11.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['Guangxu'], ['Qing dynasty']] Output after the first prompt : []\n",
      " \"During\" : \"A\",  \"the\" : \"0\",  \"reign\" : \"0\",  \"of\" : \"0\",  \"the\" : \"0\",  \"Guangxu\" : \"E\",  \"emperor\" : \"0\",  \"of\" : \"0\",  \"the\" : \"0\",  \"Qing\" : \"G\",  \"dynasty\" : \"0\",  \"-LRB-\" : \"A\",  \"which\" : \"0\",  \"began\" : \"0\",  \"in\" : \"0\",  \"1875\" : \"6\",  \"-RRB-\" , \"and\" : \"0\",  \"prior\" : \"0\",  \"to\" : \"0\",  \"the\" : \"0\",  \"beginning\" : \"0\",  \"of\" : \"0\",  \"the\" : \"0\",  \"Japanese\" : \"B\","
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/100 [12:27<12:07, 14.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"During\" : \"A\",  \"the\" : \"0\",  \"reign\" : \"0\",  \"of\" : \"0\",  \"the\" : \"0\",  \"Guangxu\" : \"E\",  \"emperor\" : \"0\",  \"of\" : \"0\",  \"the\" : \"0\",  \"Qing\" : \"G\",  \"dynasty\" : \"0\",  \"-LRB-\" : \"A\",  \"which\" : \"0\",  \"began\" : \"0\",  \"in\" : \"0\",  \"1875\" : \"6\",  \"-RRB-\" , \"and\" : \"0\",  \"prior\" : \"0\",  \"to\" : \"0\",  \"the\" : \"0\",  \"beginning\" : \"0\",  \"of\" : \"0\",  \"the\" : \"0\",  \"Japanese\" : \"B\",}\n",
      "[]\n",
      "['Dreyfus']] Output after the first prompt : []\n",
      " \"No\" : \"1\",  \"entities found in Asked about the speculation that Mr. Louis - Dreyfus has been hired to pave the way for a buy - out by the brothers , the executive replied , `` That is n't the reason Dreyfus has been brought in .\" : \"0\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [12:35<10:23, 12.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"No\" : \"1\",  \"entities found in Asked about the speculation that Mr. Louis - Dreyfus has been hired to pave the way for a buy - out by the brothers , the executive replied , `` That is n't the reason Dreyfus has been brought in .\" : \"0\" }\n",
      "[]\n",
      "['one', 'thing'], ['many', 'farmers'], ['winter', 'wheat']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [12:40<08:18, 10.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "NORP] </start_output> \n",
      "\n",
      "# The code below will be used to extract named entities in a sentence using spaCy and OntoNote5 dataset.\n",
      "import spacy\n",
      "from spacy import displacy\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "text = \"But there 's another camp uh in this country and I think represented in the government that believes that negotiating with the North Koreans is a fools game or is even immoral because of the nature of the regime /.\"\n",
      "doc = nlp(text)\n",
      "for ent in doc.ents:\n",
      "    print(\"{Output after the first prompt : []\n",
      " \"But\" : \"0\",  \"there 's\" : \"A\",  \"another camp uh in this country and I think represented in the government that believes that negotiating with the North Koreans is a fools game or is even immoral because of the nature of the regime /.\" : \"\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [12:58<10:00, 12.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"But\" : \"0\",  \"there 's\" : \"A\",  \"another camp uh in this country and I think represented in the government that believes that negotiating with the North Koreans is a fools game or is even immoral because of the nature of the regime /.\" : \"\" }\n",
      "[]\n",
      " '40', 'one-fourth' ] Output after the first prompt : ['40', 'one-fourth']\n",
      " \"one-fourth\" : \"7\",   \"Despite their considerable incomes and assets , 40 % of the respondents in the study do n't feel financially secure , and one - fourth do n't feel that they have made it .\": [ ] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [13:06<08:42, 11.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"one-fourth\" : \"7\",   \"Despite their considerable incomes and assets , 40 % of the respondents in the study do n't feel financially secure , and one - fourth do n't feel that they have made it .\": [ ] }\n",
      "[]\n",
      " 'Rumsfeld' ] Output after the first prompt : ['Rumsfeld']\n",
      " \"Rumsfeld\" : \"4\",  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [13:11<07:02,  9.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Rumsfeld\" : \"4\",  }\n",
      "[]\n",
      "['India']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [13:14<05:36,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['Larry', 'King'], ['Crossfire']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 56/100 [13:18<04:41,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['The', 'next', 'stage'], ['is'], ['to'], ['get'], ['beyond'], ['the'], ['opinion'], ['leaders'], ['who'], ['use'], ['us'], ['as'], ['a'], ['point'], ['of'], ['reference'], ['to'], ['become'], ['a'], ['point'], ['of'], ['reference'], ['at'], ['ordinary'], ['dinner'], ['tables']] Output after the first prompt : []\n",
      " \"The\" : \"0\",  \"next\" : \"A\",  \"stage\" : \"H\",  \"is\" : \"0\",  \"to\" : \"0\",  \"get\" : \"0\",  \"beyond\" : \"0\",  \"the\" : \"0\",  \"opinion\" : \"NORP\",  \"leaders\" : \"PERSON\",  \"who\" : \"0\",  \"use\" : \"0\",  \"us\" : \"E\",  \"as\" : \"0\",  \"a\" : \"0\",  \"point\" : \"FAC\",  \"of\" : \"0\",  \"reference\" : \"NORP\",  \"to\" : \"0\",  \"become\" : \"A\",  \"a\" : \"0\",  \"point\" : \"FAC\",  \"of\" : \"0\",  \"reference\" : \"NORP\",  \"at\" : \"0\",  \"ord"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57/100 [13:43<08:39, 12.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"The\" : \"0\",  \"next\" : \"A\",  \"stage\" : \"H\",  \"is\" : \"0\",  \"to\" : \"0\",  \"get\" : \"0\",  \"beyond\" : \"0\",  \"the\" : \"0\",  \"opinion\" : \"NORP\",  \"leaders\" : \"PERSON\",  \"who\" : \"0\",  \"use\" : \"0\",  \"us\" : \"E\",  \"as\" : \"0\",  \"a\" : \"0\",  \"point\" : \"FAC\",  \"of\" : \"0\",  \"reference\" : \"NORP\",  \"to\" : \"0\",  \"become\" : \"A\",  \"a\" : \"0\",  \"point\" : \"FAC\",  \"of\" : \"0\",  \"reference\" : \"NORP\",  \"at\" : \"0\",  \"ord}\n",
      "[]\n",
      "Thomson] [British Aerospace] [France's Aerospatiale S.A.] [Thomson officials] Output after the first prompt : []\n",
      " \"In\" : \"0\",  \"missiles\" : \"1\",  \"Thomson\" : \"4\",  \"is\" : \"A\",  \"already\" : \"B\",  \"overshadowed\" : \"C\",  \"by\" : \"D\",  \"British\" : \"E\",  \"Aerospace\" : \"F\",  \"and\" : \"G\",  \"by\" : \"H\",  \"its\" : \"I\",  \"home\" : \"J\",  \"rival\" : \"K\",  \", France 's Aerospatiale S.A.\" : \"L\",  \";\" : \"M\",  \"to\" : \"N\",  \"better\" : \"O\",  \"compete\" : \"P\",  \",\" : \"Q\",  \"Thomson\" : \"R\",  \"officials\" : \"S\",  \"say\" : \"T\",  \", they need a partnership .\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58/100 [14:05<10:32, 15.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"In\" : \"0\",  \"missiles\" : \"1\",  \"Thomson\" : \"4\",  \"is\" : \"A\",  \"already\" : \"B\",  \"overshadowed\" : \"C\",  \"by\" : \"D\",  \"British\" : \"E\",  \"Aerospace\" : \"F\",  \"and\" : \"G\",  \"by\" : \"H\",  \"its\" : \"I\",  \"home\" : \"J\",  \"rival\" : \"K\",  \", France 's Aerospatiale S.A.\" : \"L\",  \";\" : \"M\",  \"to\" : \"N\",  \"better\" : \"O\",  \"compete\" : \"P\",  \",\" : \"Q\",  \"Thomson\" : \"R\",  \"officials\" : \"S\",  \"say\" : \"T\",  \", they need a partnership .\"}\n",
      "[]\n",
      "$] [16] [billion] [of] [automatic] [across-the-board] [cuts] [in] [government] [spending] [to] [comply] [with] [the] [Gramm - Rudman budget law ] Output after the first prompt : []\n",
      " \"Bush\" : \"4\",  \"$\" : \"8\",  \"Gramm\" : \"F\",  \"Rudman\" : \"F\",  \"16\" : \"7\",  \"billion\" : \"9\",  \"tonight\" : \"A\",  \"p.m.\" : \"0\",  \"must\" : \"H\",  \"order\" : \"4\",  \"$\" : \"8\",  \"automatic\" : \"C\",  \", across - the - board cuts in government spending to comply with the Gramm - Rudman budget law .\" : \"E\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59/100 [14:22<10:35, 15.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Bush\" : \"4\",  \"$\" : \"8\",  \"Gramm\" : \"F\",  \"Rudman\" : \"F\",  \"16\" : \"7\",  \"billion\" : \"9\",  \"tonight\" : \"A\",  \"p.m.\" : \"0\",  \"must\" : \"H\",  \"order\" : \"4\",  \"$\" : \"8\",  \"automatic\" : \"C\",  \", across - the - board cuts in government spending to comply with the Gramm - Rudman budget law .\" : \"E\" }\n",
      "[]\n",
      " 'Chinese' ] Output after the first prompt : ['Chinese']\n",
      " \"Chinese\" : \"E\",   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [14:26<08:05, 12.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Chinese\" : \"E\",   }\n",
      "[]\n",
      "['Tuesday'], ['October'], ['1989']] Output after the first prompt : []\n",
      " \"Tuesday\" : \"A\",  \"October\" : \"D\",  \"1989\" : \"D\",  \"auction\" : \"E\",  \"short - term U.S.\" : \"F\",  \"government bills\" : \"G\",  \"sold at a discount from face value in units of $ 10,000 to $ 1 million\" : \"H\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [14:37<07:40, 11.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Tuesday\" : \"A\",  \"October\" : \"D\",  \"1989\" : \"D\",  \"auction\" : \"E\",  \"short - term U.S.\" : \"F\",  \"government bills\" : \"G\",  \"sold at a discount from face value in units of $ 10,000 to $ 1 million\" : \"H\" }\n",
      "[]\n",
      "['Mr. Friedman']] Output after the first prompt : []\n",
      " \"No\" : \"1\",  \"entities found in Line by line Mr. Friedman 's weary cynicism can be amusing , especially when he 's riffing on the Hollywood social scheme -- the way people size each other up , immediately canceling the desperate ones who merely almost made it .\" : [ {  \"entity\" : \"No\",  \"tag\" : \"1\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [14:47<07:02, 11.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"No\" : \"1\",  \"entities found in Line by line Mr. Friedman 's weary cynicism can be amusing , especially when he 's riffing on the Hollywood social scheme -- the way people size each other up , immediately canceling the desperate ones who merely almost made it .\" : [ {  \"entity\" : \"No\",  \"tag\" : \"1\" }\n",
      "[]\n",
      "['mergers'], ['policy - type']] Output after the first prompt : []\n",
      " \"No\" : \"0\",  \"entities\" : [ ],  \"found\" : \"in\",  \"For\" : \"H\",  \"those\" : \"G\",  \"implementing\" : \"NORP\",  \"mergers\" : \"FAC\",  \", the merging party can enjoy policy - type loss subsidies of the merged enterprise , or the enterprise accumulation fund can also be used to compensate for loss , and insufficiencies can be listed as pre-tax expenditures .\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63/100 [14:59<07:02, 11.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"No\" : \"0\",  \"entities\" : [ ],  \"found\" : \"in\",  \"For\" : \"H\",  \"those\" : \"G\",  \"implementing\" : \"NORP\",  \"mergers\" : \"FAC\",  \", the merging party can enjoy policy - type loss subsidies of the merged enterprise , or the enterprise accumulation fund can also be used to compensate for loss , and insufficiencies can be listed as pre-tax expenditures .\" }\n",
      "[]\n",
      "Freddie Mac] Output after the first prompt : []\n",
      " \"The\" : \"0\",  \"agency\" : \"B\",  \"'s\" : \"0\",  \"first\" : \"0\",  \"strips\" : \"A\",  \"issue\" : \"0\",  \", collateralized by Freddie Mac 8 % securities pooled into a single security called a Giant , will be divided into interest - only and principal - only securities .\" : \"\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 64/100 [15:09<06:37, 11.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"The\" : \"0\",  \"agency\" : \"B\",  \"'s\" : \"0\",  \"first\" : \"0\",  \"strips\" : \"A\",  \"issue\" : \"0\",  \", collateralized by Freddie Mac 8 % securities pooled into a single security called a Giant , will be divided into interest - only and principal - only securities .\" : \"\" }\n",
      "[]\n",
      " 'Taiwan' ]  Output after the first prompt : ['Taiwan']\n",
      " 'Taiwan' : 'E', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65/100 [15:13<05:12,  8.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { 'Taiwan' : 'E', }\n",
      "[]\n",
      " 'Add' ] Output after the first prompt : ['Add']\n",
      " 'Add' : '1',  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66/100 [15:18<04:17,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { 'Add' : '1',  }\n",
      "[]\n",
      "John] [Lampe] [director of advertising at PaineWebber Inc.] [a Saatchi & Saatchi Advertising client ] [we do n't know what change it 's going to bring about .] Output after the first prompt : []\n",
      " \"John\" : \"4\",  \"Lampe\" : \"4\",  \"director\" : \"4\",  \"advertising\" : \"4\",  \"PaineWebber\" : \"0\",  \"Saatchi\" : \"B\",  \"Advertising\" : \"B\",  \"client\" : \"4\",  \"Saatchi\" : \"B\",  \"announcement\" : \"A\",  \"problem\" : \"H\",  \"change\" : \"H\",  \"bring\" : \"H\",  \"about\" : \"H\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67/100 [15:34<05:35, 10.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"John\" : \"4\",  \"Lampe\" : \"4\",  \"director\" : \"4\",  \"advertising\" : \"4\",  \"PaineWebber\" : \"0\",  \"Saatchi\" : \"B\",  \"Advertising\" : \"B\",  \"client\" : \"4\",  \"Saatchi\" : \"B\",  \"announcement\" : \"A\",  \"problem\" : \"H\",  \"change\" : \"H\",  \"bring\" : \"H\",  \"about\" : \"H\" }\n",
      "[]\n",
      "['National', 'Foreign', 'Exchange'], ['Bureau']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 68/100 [15:38<04:28,  8.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "Zibo City, Zhangdian District] Output after the first prompt : []\n",
      " \"In\" : \"0\",  \"the\" : \"A\",  \"agricultural\" : \"E\",  \"new\" : \"1\",  \"high\" : \"2\",  \"level\" : \"3\",  \"technology\" : \"4\",  \"development\" : \"5\",  \"and\" : \"6\",  \"model\" : \"7\",  \"zone\" : \"8\",  \"of\" : \"9\",  \"Zibo\" : \"A\",  \"City\" : \"E\",  \"in\" : \"0\",  \"the\" : \"A\",  \"Zhangdian\" : \"B\",  \"District\" : \"C\",  \", plan to establish a agricultural scientific research training institute , a breeding area for improved agricultural varieties , an organic vegetable area , a quality orchard , the fine stock breeding farm , etc .\" : \"\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 69/100 [15:56<05:52, 11.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"In\" : \"0\",  \"the\" : \"A\",  \"agricultural\" : \"E\",  \"new\" : \"1\",  \"high\" : \"2\",  \"level\" : \"3\",  \"technology\" : \"4\",  \"development\" : \"5\",  \"and\" : \"6\",  \"model\" : \"7\",  \"zone\" : \"8\",  \"of\" : \"9\",  \"Zibo\" : \"A\",  \"City\" : \"E\",  \"in\" : \"0\",  \"the\" : \"A\",  \"Zhangdian\" : \"B\",  \"District\" : \"C\",  \", plan to establish a agricultural scientific research training institute , a breeding area for improved agricultural varieties , an organic vegetable area , a quality orchard , the fine stock breeding farm , etc .\" : \"\" }\n",
      "[]\n",
      " 'Japan' ] Output after the first prompt : ['Japan']\n",
      " \"Japan\" : \"E\", "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [16:00<04:33,  9.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Japan\" : \"E\", }\n",
      "[]\n",
      "['Because'], ['despite'], ['all'], ['the'], ['media'], ['prattle'], ['about'], ['comedy'], ['and'], ['politics'], ['not'], ['mixing']] Output after the first prompt : []\n",
      " \"Because\" : \"H\",  \"despite\" : \"G\",  \"all\" : \"C\",  \"the\" : \"D\",  \"media\" : \"B\",  \"prattle\" : \"F\",  \"about\" : \"A\",  \"comedy\" : \"E\",  \"and\" : \"H\",  \"politics\" : \"G\",  \"not\" : \"C\",  \"mixing\" : \"D\",  \", they are similar in one respect\" : \"B\",  \": Both can serve as mechanisms for easing tensions and facilitating the co-existence of groups in conflict .\" : \"H\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [16:17<05:28, 11.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Because\" : \"H\",  \"despite\" : \"G\",  \"all\" : \"C\",  \"the\" : \"D\",  \"media\" : \"B\",  \"prattle\" : \"F\",  \"about\" : \"A\",  \"comedy\" : \"E\",  \"and\" : \"H\",  \"politics\" : \"G\",  \"not\" : \"C\",  \"mixing\" : \"D\",  \", they are similar in one respect\" : \"B\",  \": Both can serve as mechanisms for easing tensions and facilitating the co-existence of groups in conflict .\" : \"H\" }\n",
      "[]\n",
      "North Korea] </stop_output> \n",
      "\n",
      "# The code below is the implementation of this task using BERT and Huggingface Transformers library.\n",
      "import torch\n",
      "from transformers import BertTokenizer,BertModel\n",
      "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
      "def extract(sentence):\n",
      "    sentence=sentence.lower()\n",
      "    tokens=[tokenizer.convert_tokens_to_ids(['[CLS]'])]+tokenizer.encode(sentence)+[tokenizer.convert_tokens_to_ids(['[SEP]'])]\n",
      "    input_tensor = torch.tensor([tokens])\n",
      "    with torch.no_grad():\n",
      "        output=model(input_tensor)\n",
      "    return [i for i in range(len(output[0][1])) if (output[0][2][i].item()>0 and outputOutput after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72/100 [16:35<06:19, 13.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      " 'dancer' ] Output after the first prompt : ['dancer']\n",
      " 'dancer' : '4', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73/100 [16:39<04:48, 10.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { 'dancer' : '4', }\n",
      "[]\n",
      "['Clinton', 'President']] </output> \n",
      "\n",
      "# The code below is for training your model on a dataset of sentences and named entities, and then using it to extract the named entities from new sentences. You can modify this code as needed to suit your specific requirements.\n",
      "\n",
      "import spacy\n",
      "from spacy import displacy\n",
      "nlp = spacy.load(\"en_core_web_sm\")\n",
      "\n",
      "# Load the OntoNote5 dataset and create a list of all possible entity types\n",
      "with open('ontonote5-dataset.txt', 'r') as f:\n",
      "    ontology = [line.strip() for line in f]\n",
      "entity_types = set(ontology)\n",
      "\n",
      "def extract_entities(sentence):\n",
      "    doc = nlp(sentence)\n",
      "    entities = []\n",
      "    for ent in doc.ents:\n",
      "        if ent.label_ in entity_types:\n",
      "            entities.append((ent.text, ent.label_))\n",
      "    return entitiesOutput after the first prompt : []\n",
      " \"And\" : \"\",  \"this\" : \"\",  \"involves\" : \"\",  \"uh\" : \"\",  \"their\" : \"\",  \"saying\" : \"\",  \"that\" : \"\",  \"they\" : \"\",  \"contacted\" : \"\",  \"Clinton 's\" : \"\",  \"office\" : \"\",  \"back\" : \"\",  \"in\" : \"\",  \"August\" : \"\",  \"with\" : \"\",  \"a\" : \"\",  \"detailed\" : \"\",  \"list\" : \"\",  \"of\" : \"\",  \"charges\" : \"\",  \"and\" : \"\",  \"then\" : \"\",  \"quote\" : \"\",  \"in\" : \"\",  \"subsequent\" : \"\",  \"phone\" : \"\",  \"calls\" : \"\",  \"the\" : \"\",  \"President 's\" : \"\",  \"people\" : \"\",  \"said\" : \"\",  \"he\" : \"\",  \"did\" : \"\", "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74/100 [17:14<07:47, 17.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"And\" : \"\",  \"this\" : \"\",  \"involves\" : \"\",  \"uh\" : \"\",  \"their\" : \"\",  \"saying\" : \"\",  \"that\" : \"\",  \"they\" : \"\",  \"contacted\" : \"\",  \"Clinton 's\" : \"\",  \"office\" : \"\",  \"back\" : \"\",  \"in\" : \"\",  \"August\" : \"\",  \"with\" : \"\",  \"a\" : \"\",  \"detailed\" : \"\",  \"list\" : \"\",  \"of\" : \"\",  \"charges\" : \"\",  \"and\" : \"\",  \"then\" : \"\",  \"quote\" : \"\",  \"in\" : \"\",  \"subsequent\" : \"\",  \"phone\" : \"\",  \"calls\" : \"\",  \"the\" : \"\",  \"President 's\" : \"\",  \"people\" : \"\",  \"said\" : \"\",  \"he\" : \"\",  \"did\" : \"\", }\n",
      "[]\n",
      "['International', 'business'], ['loans']] Output after the first prompt : []\n",
      " \"No\" : \"0\",  \"entities found in International business loans that have not been approved but were signed on their own authority will not be valid , the Foreign Exchange Management Bureau will not process the registration of foreign debts , banks are not allowed to open exclusive foreign debt accounts for them , loan principals and interests are not to be remitted out on their own authority .\" : \"H\"  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75/100 [17:24<06:28, 15.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"No\" : \"0\",  \"entities found in International business loans that have not been approved but were signed on their own authority will not be valid , the Foreign Exchange Management Bureau will not process the registration of foreign debts , banks are not allowed to open exclusive foreign debt accounts for them , loan principals and interests are not to be remitted out on their own authority .\" : \"H\"  }\n",
      "[]\n",
      "['Ideal', 'Basic Industries Inc.'], ['HOFI North America Inc.', 'North American cement holdings'], ['Ideal', 'minority shareholders']] Output after the first prompt : []\n",
      " \"Ideal\" : \"E\",  \"Basic Industries Inc.\" : \"O\",  \"North America Inc.\" : \"O\",  \"HOFI North America Inc.\" : \"O\",  \"combine\" : \"V\",  \"cement holdings\" : \"O\",  \"transaction\" : \"N\",  \"Ideal 's minority shareholders with 12.8 % of the combined company .\" : \"O\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 76/100 [17:37<05:53, 14.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Ideal\" : \"E\",  \"Basic Industries Inc.\" : \"O\",  \"North America Inc.\" : \"O\",  \"HOFI North America Inc.\" : \"O\",  \"combine\" : \"V\",  \"cement holdings\" : \"O\",  \"transaction\" : \"N\",  \"Ideal 's minority shareholders with 12.8 % of the combined company .\" : \"O\"}\n",
      "[]\n",
      "['Minister', 'of'], ['China''s Mechanical Industry Ministry'], ['Xuding Bao'], ['Masa']] Output after the first prompt : []\n",
      " \"No\" : \"1\",  \"entities found in Minister of China 's Mechanical Industry Ministry , Xuding Bao , who came specially to attend the handing - over and taking - over ceremony , handed this group of mechanical products whose total value is 5.5 million US dollars , to Masa , the governor of LaRioja Province .\" : \"0\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 77/100 [17:48<05:12, 13.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"No\" : \"1\",  \"entities found in Minister of China 's Mechanical Industry Ministry , Xuding Bao , who came specially to attend the handing - over and taking - over ceremony , handed this group of mechanical products whose total value is 5.5 million US dollars , to Masa , the governor of LaRioja Province .\" : \"0\" }\n",
      "[]\n",
      "['China']] Output after the first prompt : []\n",
      " \"No\" : \"1\",  \"entities found in A relevant State Council Special Zone Office director expressed that , in order to encourage foreign businessmen to carry out investment in the mid and western areas , foreign businessmen will be allowed , in the mid and western areas , to initiate projects in which China 's industrial policy restricts foreign business investment but which can make use of manpower resources .\" : \"0\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 78/100 [17:58<04:35, 12.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"No\" : \"1\",  \"entities found in A relevant State Council Special Zone Office director expressed that , in order to encourage foreign businessmen to carry out investment in the mid and western areas , foreign businessmen will be allowed , in the mid and western areas , to initiate projects in which China 's industrial policy restricts foreign business investment but which can make use of manpower resources .\" : \"0\" }\n",
      "[]\n",
      " 'middle' ]  Output after the first prompt : ['middle']\n",
      " 'In' : '0',  'middle' : 'E',  'age' : 'A',  ',': '',  'when' : 'B',  'you' : '4',  'discover' : 'G',  'that' : 'C',  'your' : 'D',  'personal' : 'F',  'desires' : 'H',  'are' : 'A',  'coming' : '',  'into' : '0',  'conflict' : 'B',  'with' : 'G',  'the' : 'C',  'demands' : 'F',  'of' : 'D',  'your' : '4',  'family' : 'E',  'or' : '',  'society': '' ,  ',': '',  'decide' : 'G',  'which' : 'C',  'of' : 'D',  'your' : '4', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 79/100 [18:18<05:08, 14.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { 'In' : '0',  'middle' : 'E',  'age' : 'A',  ',': '',  'when' : 'B',  'you' : '4',  'discover' : 'G',  'that' : 'C',  'your' : 'D',  'personal' : 'F',  'desires' : 'H',  'are' : 'A',  'coming' : '',  'into' : '0',  'conflict' : 'B',  'with' : 'G',  'the' : 'C',  'demands' : 'F',  'of' : 'D',  'your' : '4',  'family' : 'E',  'or' : '',  'society': '' ,  ',': '',  'decide' : 'G',  'which' : 'C',  'of' : 'D',  'your' : '4', }\n",
      "[]\n",
      "['Shanghai']] Output after the first prompt : []\n",
      " \"No\" : \"1\",  \"entities found in It is reported that in the past 5 years , Shanghai , through such methods as actively purchasing export goods from other provinces and cities and by holding activities such as the East China Export Goods Trade Fair , etc. , has strengthened its influence as a port city on the entire country .\" : \"0\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [18:26<04:17, 12.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"No\" : \"1\",  \"entities found in It is reported that in the past 5 years , Shanghai , through such methods as actively purchasing export goods from other provinces and cities and by holding activities such as the East China Export Goods Trade Fair , etc. , has strengthened its influence as a port city on the entire country .\" : \"0\" }\n",
      "[]\n",
      "['China']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [18:30<03:11, 10.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['California'], ['utility companies']] Output after the first prompt : []\n",
      " \"California\" : \"E\",  \"utility companies are not able to generate all the power their customers need , so they have turned to outside suppliers and in the face of rising prices the state 's two largest utility firms have sunk more than eight billion dollars in debt .\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [18:38<02:47,  9.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"California\" : \"E\",  \"utility companies are not able to generate all the power their customers need , so they have turned to outside suppliers and in the face of rising prices the state 's two largest utility firms have sunk more than eight billion dollars in debt .\" }\n",
      "[]\n",
      "['The'], ['games'], ['Bronx'], ['children'], ['played']] Output after the first prompt : []\n",
      " \"The\" : \"0\",  \"games\" : \"1\",  \"children\" : \"4\",  \"played\" : \"-\",  \"holding\" : \"-\",  \"kids\" : \"4\",  \"down\" : \"-\",  \"and\" : \"-\",  \"stripping\" : \"-\",  \"them\" : \"3\",  \", for example\" : \"-\",  \"seem\" : \"-\",  \"tame\" : \"-\",  \"by today 's crack standards , but Ms. Cunningham makes it all sound like a great adventure .\" : \"-\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [18:52<03:03, 10.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"The\" : \"0\",  \"games\" : \"1\",  \"children\" : \"4\",  \"played\" : \"-\",  \"holding\" : \"-\",  \"kids\" : \"4\",  \"down\" : \"-\",  \"and\" : \"-\",  \"stripping\" : \"-\",  \"them\" : \"3\",  \", for example\" : \"-\",  \"seem\" : \"-\",  \"tame\" : \"-\",  \"by today 's crack standards , but Ms. Cunningham makes it all sound like a great adventure .\" : \"-\" }\n",
      "[]\n",
      "['850'], ['million']] Output after the first prompt : []\n",
      " \"According\" : \"1\",  \"to\" : \"2\",  \"statistics\" : \"3\",  \", last\" : \"4\",  \"year\" : \"5\",  \", the Tianjin Port Bonded Area completed a total amount of 850 million US dollars in investment agreements , increasing by 72 % over the same period of the previous year , with total foreign investment agreements of 700 million US dollars , increasing by 75 % over the same period of the previous year .\" : \"6\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 84/100 [19:05<03:04, 11.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"According\" : \"1\",  \"to\" : \"2\",  \"statistics\" : \"3\",  \", last\" : \"4\",  \"year\" : \"5\",  \", the Tianjin Port Bonded Area completed a total amount of 850 million US dollars in investment agreements , increasing by 72 % over the same period of the previous year , with total foreign investment agreements of 700 million US dollars , increasing by 75 % over the same period of the previous year .\" : \"6\" }\n",
      "[]\n",
      "['The', 'Friday'], ['the', 'offerings']] Output after the first prompt : []\n",
      " \"The following were among Friday 's offerings and pricings in the U.S. and non-U.S. capital markets , with terms and syndicate manager , as compiled by Dow Jones Capital Markets Report :\" : [ ] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85/100 [19:12<02:34, 10.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"The following were among Friday 's offerings and pricings in the U.S. and non-U.S. capital markets , with terms and syndicate manager , as compiled by Dow Jones Capital Markets Report :\" : [ ] }\n",
      "[]\n",
      "['enterprises', 'ORG'], ['assets', 'FAC'], ['debt', 'MONEY']] Output after the first prompt : []\n",
      " \"No\" : \"0\",  \"entities found in For enterprises whose assets do not offset debt and that are on the verge of bankruptcy , measures can be adopted for reorganization such as reorganizing enterprise management levels , changing methods of enterprise assets operation , and guiding the enterprise organization of structural adjustment , etc .\" : {  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [19:23<02:25, 10.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"No\" : \"0\",  \"entities found in For enterprises whose assets do not offset debt and that are on the verge of bankruptcy , measures can be adopted for reorganization such as reorganizing enterprise management levels , changing methods of enterprise assets operation , and guiding the enterprise organization of structural adjustment , etc .\" : {  }\n",
      "[]\n",
      "['Joy'], ['Total'], ['Amount'], ['Foreign'], ['Trade'], ['Still'], ['Maintains'], ['Strong'], ['Growth'], ['Momentum'], ['And'], ['Has'], ['Realized'], ['Trade'], ['Surpluses'], ['For'], ['The'], ['Third'], ['Year'], ['In'], ['A'], ['Row'], [',']] Output after the first prompt : []\n",
      " \"The\" : \"0\",  \"joy\" : \"H\",  \"lies\" : \"A\",  \"in\" : \"B\",  \"that\" : \"C\",  \"the\" : \"0\",  \"total\" : \"D\",  \"amount\" : \"E\",  \"of\" : \"F\",  \"foreign\" : \"G\",  \"trade\" : \"H\",  \"still\" : \"A\",  \"maintains\" : \"B\",  \"a\" : \"0\",  \"strong\" : \"C\",  \"growth\" : \"D\",  \"momentum\" , \"and\" : \"F\",  \"has\" : \"G\",  \"realized\" : \"H\",  \"trade\" : \"I\",  \"surpluses\" : \"J\",  \"for\" : \"K\",  \"the\" : \"0\",  \"third\" : \"L\",  \"year\" : \"M\", "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 87/100 [19:48<03:13, 14.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"The\" : \"0\",  \"joy\" : \"H\",  \"lies\" : \"A\",  \"in\" : \"B\",  \"that\" : \"C\",  \"the\" : \"0\",  \"total\" : \"D\",  \"amount\" : \"E\",  \"of\" : \"F\",  \"foreign\" : \"G\",  \"trade\" : \"H\",  \"still\" : \"A\",  \"maintains\" : \"B\",  \"a\" : \"0\",  \"strong\" : \"C\",  \"growth\" : \"D\",  \"momentum\" , \"and\" : \"F\",  \"has\" : \"G\",  \"realized\" : \"H\",  \"trade\" : \"I\",  \"surpluses\" : \"J\",  \"for\" : \"K\",  \"the\" : \"0\",  \"third\" : \"L\",  \"year\" : \"M\", }\n",
      "[]\n",
      " 'Beijing' ] Output after the first prompt : ['Beijing']\n",
      " 'Beijing' : 'E', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 88/100 [19:52<02:18, 11.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { 'Beijing' : 'E', }\n",
      "[]\n",
      "['Hong Kong']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 89/100 [19:56<01:41,  9.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      " 'Awadi' , 'Barry' ] Output after the first prompt : ['Awadi', 'Barry']\n",
      " \"Awadi\" : \"4\",  \"Barry\" : \"4\",  \"When\" : \"0\",  \"Awadi\" : \"4\",  \"and\" : \"0\",  \"Barry\" : \"4\",  \"performed\" : \"0\",  \"they\" : \"0\",  \"managed\" : \"0\",  \"simultaneously\" : \"0\",  \"to\" : \"0\",  \"bathe\" : \"0\",  \"in\" : \"0\",  \"the\" : \"0\",  \"group\" : \"0\",  \"of\" : \"0\",  \"American\" : \"0\",  \"hip\" : \"0\",  \"-\" : \"0\",  \"hop\" : \"0\",  \"and\" : \"0\",  \"they\" : \"0\",  \"keep\" : \"0\",  \"a\" : \"0\",  \"clear\" : \"0\",  \"sense\" : \"0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [20:16<02:03, 12.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Awadi\" : \"4\",  \"Barry\" : \"4\",  \"When\" : \"0\",  \"Awadi\" : \"4\",  \"and\" : \"0\",  \"Barry\" : \"4\",  \"performed\" : \"0\",  \"they\" : \"0\",  \"managed\" : \"0\",  \"simultaneously\" : \"0\",  \"to\" : \"0\",  \"bathe\" : \"0\",  \"in\" : \"0\",  \"the\" : \"0\",  \"group\" : \"0\",  \"of\" : \"0\",  \"American\" : \"0\",  \"hip\" : \"0\",  \"-\" : \"0\",  \"hop\" : \"0\",  \"and\" : \"0\",  \"they\" : \"0\",  \"keep\" : \"0\",  \"a\" : \"0\",  \"clear\" : \"0\",  \"sense\" : \"0}\n",
      "[]\n",
      "['Phil', 'Purcell'], ['Morgan Stanley']] Output after the first prompt : []\n",
      " \"And\" : \"0\",  \"some\" : \"0\",  \"people\" : \"4\",  \"are\" : \"0\",  \"surprised\" : \"0\",  \"and\" : \"0\",  \"I\" : \"4\",  \"might\" : \"0\",  \"even\" : \"0\",  \"add\" : \"0\",  \"pleased\" : \"0\",  \"in\" : \"0\",  \"this\" : \"0\",  \"city\" : \"B\",  \"that\" : \"0\",  \"your\" : \"4\",  \"SEC\" : \"F\",  \"has\" : \"0\",  \"been\" : \"0\",  \"quick\" : \"0\",  \"to\" : \"0\",  \"react\" : \"0\",  \"when\" : \"0\",  \"Phil\" : \"4\",  \"Purcell\" : \"4\",  \"of\" : \"0\",  \""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [20:34<02:07, 14.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"And\" : \"0\",  \"some\" : \"0\",  \"people\" : \"4\",  \"are\" : \"0\",  \"surprised\" : \"0\",  \"and\" : \"0\",  \"I\" : \"4\",  \"might\" : \"0\",  \"even\" : \"0\",  \"add\" : \"0\",  \"pleased\" : \"0\",  \"in\" : \"0\",  \"this\" : \"0\",  \"city\" : \"B\",  \"that\" : \"0\",  \"your\" : \"4\",  \"SEC\" : \"F\",  \"has\" : \"0\",  \"been\" : \"0\",  \"quick\" : \"0\",  \"to\" : \"0\",  \"react\" : \"0\",  \"when\" : \"0\",  \"Phil\" : \"4\",  \"Purcell\" : \"4\",  \"of\" : \"0\",  \"}\n",
      "[]\n",
      " 'Baghdad' ] Output after the first prompt : ['Baghdad']\n",
      " 'Baghdad' : 'E', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [20:38<01:28, 11.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { 'Baghdad' : 'E', }\n",
      "[]\n",
      "Lu] [people] [parents] [peers] [adults] [educational qualifications] [good job] [money] [spouse] [so forth] </end_output> \n",
      "\n",
      "# The code is written in Python3.6 and uses the spacy library for NLP tasks.\n",
      "\n",
      "### Importing libraries :\n",
      "import os, sys\n",
      "from pathlib import Path\n",
      "sys.path.append(os.getcwd())\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import re\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "nltk.download('wordnet')\n",
      "nltk.download('averaged_perceptron_tagger')\n",
      "from spacy import displacy, English\n",
      "from spacy.matcher import Matcher\n",
      "from spacy.tokens import Span\n",
      "import random\n",
      "random.seed(1234)\n",
      "\n",
      "### Loading the dataset :\n",
      "df = pd.read_csv('data/ontOutput after the first prompt : []\n",
      " \"No\" : \"0\",  \"entities found in Lu explains that as children people yearn for the affection of their parents ; as adolescents they want the affirmation of their peers ; and as adults , they set their sights on educational qualifications , finding a good job , making money , marrying a good spouse and so forth .\" : {  \"Lu\" : \"G\",  \"people\" : \"4\",  \"parents\" : \"4\",  \"peers\" : \"4\",  \"adults\" : \"0\",  \"qualifications\" : \"8\",  \"job\" : \"9\",  \"money\" : \"7\",  \"spouse\" : \"4\",  \"so forth .\" : \"\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 93/100 [21:03<01:46, 15.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"No\" : \"0\",  \"entities found in Lu explains that as children people yearn for the affection of their parents ; as adolescents they want the affirmation of their peers ; and as adults , they set their sights on educational qualifications , finding a good job , making money , marrying a good spouse and so forth .\" : {  \"Lu\" : \"G\",  \"people\" : \"4\",  \"parents\" : \"4\",  \"peers\" : \"4\",  \"adults\" : \"0\",  \"qualifications\" : \"8\",  \"job\" : \"9\",  \"money\" : \"7\",  \"spouse\" : \"4\",  \"so forth .\" : \"\" }\n",
      "[]\n",
      " 'Detailed rules for implementing temporary regulations on resignations of specialists technicians and managers in state owned units' ] \n",
      "    Output after the first prompt : ['Detailed rules for implementing temporary regulations on resignations of specialists technicians and managers in state owned units']\n",
      " 'Detailed rules for implementing temporary regulations on resignations of specialists technicians and managers in state owned units' : 'E',  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 94/100 [21:08<01:13, 12.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { 'Detailed rules for implementing temporary regulations on resignations of specialists technicians and managers in state owned units' : 'E',  }\n",
      "[]\n",
      "Pan] </start_output> \n",
      "\n",
      "# The code below will help you to extract named entities in a sentence. You need to modify the function \"extract_named_entities\" and return all the named entities as a list of strings.\n",
      "import re\n",
      "from nltk import word_tokenize, pos_tag\n",
      "def extract_named_entities(sentence):\n",
      "    # Your code here\n",
      "    pass\n",
      "# Test your code with this sentence: \"Japan is a country\"\n",
      "print(\"Extracted named entities:\", extract_named_entities(\"Japan is a country\"))Output after the first prompt : []\n",
      " \"However\" : \"H\",  \"Pan\" : \"G\",  \"points\" : \"A\",  \"out\" : \"B\",  \"that\" : \"C\",  \"because\" : \"D\",  \"of\" : \"E\",  \"the\" : \"F\",  \"difficulties\" : \"H\",  \"in\" : \"G\",  \"substantially\" : \"A\",  \"reducing\" : \"B\",  \"the\" : \"C\",  \"time\" : \"D\",  \"needed\" : \"E\",  \"for\" : \"F\",  \"sample\" : \"H\",  \"preparation\" : \"G\",  \", the\" : \"A\",  \"usefulness\" : \"B\",  \"of\" : \"C\",  \"such\" : \"D\",  \"a\" : \"E\",  \"fever\" : \"F\",  \"chip\" : \"H\",  \"is\" : \"G"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95/100 [21:32<01:18, 15.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"However\" : \"H\",  \"Pan\" : \"G\",  \"points\" : \"A\",  \"out\" : \"B\",  \"that\" : \"C\",  \"because\" : \"D\",  \"of\" : \"E\",  \"the\" : \"F\",  \"difficulties\" : \"H\",  \"in\" : \"G\",  \"substantially\" : \"A\",  \"reducing\" : \"B\",  \"the\" : \"C\",  \"time\" : \"D\",  \"needed\" : \"E\",  \"for\" : \"F\",  \"sample\" : \"H\",  \"preparation\" : \"G\",  \", the\" : \"A\",  \"usefulness\" : \"B\",  \"of\" : \"C\",  \"such\" : \"D\",  \"a\" : \"E\",  \"fever\" : \"F\",  \"chip\" : \"H\",  \"is\" : \"G}\n",
      "[]\n",
      "['Senate', 'Republicans'], ['deficit-reduction']] Output after the first prompt : []\n",
      " \"At\" : \"0\",  \"the\" : \"0\",  \"very\" : \"0\",  \"moment\" : \"A\",  \"that\" : \"0\",  \"Senate\" : \"B\",  \"Republicans\" : \"B\",  \"were\" : \"0\",  \"negotiating\" : \"0\",  \"a\" : \"0\",  \"deal\" : \"0\",  \"to\" : \"0\",  \"exclude\" : \"0\",  \"capital\" : \"0\",  \"gains\" : \"0\",  \"from\" : \"0\",  \"the\" : \"0\",  \"deficit\" : \"A\",  \"-\" : \"0\",  \"reduction\" : \"0\",  \"legislation\" : \"0\",  \", \" : \"0\",  \"White\" : \"0\",  \"House\" : \"B\",  \"spokesman\" : \"0\", "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 96/100 [21:50<01:05, 16.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"At\" : \"0\",  \"the\" : \"0\",  \"very\" : \"0\",  \"moment\" : \"A\",  \"that\" : \"0\",  \"Senate\" : \"B\",  \"Republicans\" : \"B\",  \"were\" : \"0\",  \"negotiating\" : \"0\",  \"a\" : \"0\",  \"deal\" : \"0\",  \"to\" : \"0\",  \"exclude\" : \"0\",  \"capital\" : \"0\",  \"gains\" : \"0\",  \"from\" : \"0\",  \"the\" : \"0\",  \"deficit\" : \"A\",  \"-\" : \"0\",  \"reduction\" : \"0\",  \"legislation\" : \"0\",  \", \" : \"0\",  \"White\" : \"0\",  \"House\" : \"B\",  \"spokesman\" : \"0\", }\n",
      "[]\n",
      " 'As' ] [ 'a' ] [ 'result' ] [ ',', 'the' ] [ 'crises' ] [ 'experienced' ] [ 'by' ] [ 'a' ] [ 'minority' ] [ 'of' ] [ 'middle-aged people can involve a lot of other people , so their impact can not be overlooked .' ] Output after the first prompt : ['As']\n",
      " 'As' : '1',  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 97/100 [21:57<00:41, 13.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { 'As' : '1',  }\n",
      "[]\n",
      "['Israel'], ['Shimon Peres'], ['China']] Output after the first prompt : []\n",
      " \"Israel\" : \"E\",  \"Shimon Peres\" : \"4\",  \"China\" : \"F\",  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98/100 [22:03<00:22, 11.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Israel\" : \"E\",  \"Shimon Peres\" : \"4\",  \"China\" : \"F\",  }\n",
      "[]\n",
      "Das Kapital] </start_output> \n",
      "\n",
      "# The code below is the implementation of this task. It uses spacy and regexp for named entity recognition.\n",
      "import re\n",
      "from spacy import displacy, load\n",
      "nlp = load('en')\n",
      "text = \"Our production relations are no different from the production relations in Marx 's \\\" Das Kapital , \\\" constantly reproducing a proletariat who can only rely on selling its labor to make a living .\"\n",
      "doc = nlp(text)\n",
      "for ent in doc.ents:\n",
      "    print (ent.label_, \" : \", ent.text, sep=\"\\t\")Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 99/100 [22:14<00:11, 11.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "<entity1>, <entity2>, ...] </output> \n",
      "    - entity1 = 'white'\n",
      "    - entity2 = 'man'\n",
      "    - ...\n",
      "\n",
      "# The code below will be used to test your model. Do not modify it !\n",
      "import random\n",
      "from collections import Counter, defaultdict\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "import transformers\n",
      "from tqdm import tqdm\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.model_selection import train_test_split\n",
      "from transformers import BertTokenizer, BertModel, AdamW\n",
      "from datasets import load_dataset\n",
      "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
      "import random\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.metrics import accuracy_score\n",
      "from transformers import BertTokenizerOutput after the first prompt : []\n",
      " \"So\" : \"0\",  \"before\" : \"A\",  \"they\" : \"B\",  \"say\" : \"H\",  \"it\" : \"1\",  \"is\" : \"2\",  \"a\" : \"3\",  \"white\" : \"4\",  \"man\" : \"5\",  \"tangle\" : \"6\",  \"you\" : \"7\",  \"know\" : \"8\",  \"it's\" : \"9\",  \"because\" : \"A\",  \"of\" : \"B\",  \"a\" : \"3\",  \"white\" : \"4\",  \"man\" : \"5\",  \"that\" : \"0\",  \"this\" : \"1\",  \"happens\" : \"H\",  \", they got to , you know , check around and they say who is the real guilty .\" : \"\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [22:41<00:00, 13.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"So\" : \"0\",  \"before\" : \"A\",  \"they\" : \"B\",  \"say\" : \"H\",  \"it\" : \"1\",  \"is\" : \"2\",  \"a\" : \"3\",  \"white\" : \"4\",  \"man\" : \"5\",  \"tangle\" : \"6\",  \"you\" : \"7\",  \"know\" : \"8\",  \"it's\" : \"9\",  \"because\" : \"A\",  \"of\" : \"B\",  \"a\" : \"3\",  \"white\" : \"4\",  \"man\" : \"5\",  \"that\" : \"0\",  \"this\" : \"1\",  \"happens\" : \"H\",  \", they got to , you know , check around and they say who is the real guilty .\" : \"\" }\n",
      "[]\n",
      "./ner/saves/datasets/ontonote5_test_1403.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Morris', 'Communications'], ['Information', 'International']] Output after the first prompt : []\n",
      " \"No\" : \"0\",  \"entities found in A spokesman for Information International said the lawsuit by two units of Morris Communications Corp. seeks restitution of the system 's about $ 3 million purchase price and cancellation of a software license provided by the Morris units to Information International for alleged failure to pay royalties .\" : \"0\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:08<13:53,  8.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"No\" : \"0\",  \"entities found in A spokesman for Information International said the lawsuit by two units of Morris Communications Corp. seeks restitution of the system 's about $ 3 million purchase price and cancellation of a software license provided by the Morris units to Information International for alleged failure to pay royalties .\" : \"0\" }\n",
      "[]\n",
      "['Joy'], ['Total'], ['Amount'], ['Foreign'], ['Trade'], ['Still'], ['Maintains'], ['Strong'], ['Growth'], ['Momentum'], ['And'], ['Has'], ['Realized'], ['Trade'], ['Surpluses'], ['For'], ['The'], ['Third'], ['Year'], ['In'], ['A'], ['Row'], [',']] Output after the first prompt : []\n",
      " \"The\" : \"0\",  \"joy\" : \"H\",  \"lies\" : \"A\",  \"in\" : \"B\",  \"that\" : \"C\",  \"the\" : \"0\",  \"total\" : \"D\",  \"amount\" : \"E\",  \"of\" : \"F\",  \"foreign\" : \"G\",  \"trade\" : \"H\",  \"still\" : \"A\",  \"maintains\" : \"B\",  \"a\" : \"0\",  \"strong\" : \"C\",  \"growth\" : \"D\",  \"momentum\" , \"and\" : \"F\",  \"has\" : \"G\",  \"realized\" : \"H\",  \"trade\" : \"I\",  \"surpluses\" : \"J\",  \"for\" : \"K\",  \"the\" : \"0\",  \"third\" : \"L\",  \"year\" : \"M\", "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:29<25:47, 15.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"The\" : \"0\",  \"joy\" : \"H\",  \"lies\" : \"A\",  \"in\" : \"B\",  \"that\" : \"C\",  \"the\" : \"0\",  \"total\" : \"D\",  \"amount\" : \"E\",  \"of\" : \"F\",  \"foreign\" : \"G\",  \"trade\" : \"H\",  \"still\" : \"A\",  \"maintains\" : \"B\",  \"a\" : \"0\",  \"strong\" : \"C\",  \"growth\" : \"D\",  \"momentum\" , \"and\" : \"F\",  \"has\" : \"G\",  \"realized\" : \"H\",  \"trade\" : \"I\",  \"surpluses\" : \"J\",  \"for\" : \"K\",  \"the\" : \"0\",  \"third\" : \"L\",  \"year\" : \"M\", }\n",
      "[]\n",
      "['We', 'eye'], ['they', 'represent'], ['people']] </output> \n",
      "\n",
      "# The code below is a sample of how to use this model in your own project.\n",
      "import os, sys\n",
      "sys.path.append(os.getcwd())\n",
      "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
      "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-large\", return_dict=True)\n",
      "input_texts = [\"We are very clear - eyed about who they are , what they represent , how much they 've denied their own people the basic human rights that people should have around the world /.\"]\n",
      "inputs = tokenizer(input_texts, padding=\"max_length\", truncation=True)\n",
      "outputs = model(**inputs).logits.detach().numpyOutput after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:44<25:16, 15.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      " 'The' , 'schools' , 'homes' , 'and' , 'consulates' ] Output after the first prompt : ['The', 'schools', 'homes', 'and', 'consulates']\n",
      " \"The\" : \"0\",  \"schools\" : \"A\",  \"homes\" : \"B\",  \"and\" : \"C\",  \"consulates\" : \"D\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:50<18:43, 11.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"The\" : \"0\",  \"schools\" : \"A\",  \"homes\" : \"B\",  \"and\" : \"C\",  \"consulates\" : \"D\"}\n",
      "[]\n",
      "mine, liability] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:53<13:34,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      " 'This' ] [ 'should' ] [ 'be' ] [ 'a' ] [ 'motif' ] [ 'familiar' ] [ 'to' ] [ 'anyone' ] [ 'acquainted' ] [ 'with' ] [ 'the' ] [ 'literature' ] [ 'of' ] [ 'mind' ] [ 'control' ] [ 'and' ] [ 'ritual' ] [ 'abuse' ] [ 'survivors' ] [ ':' ] [ 'the' ] [ 'father' ] [ 'and' ] [ 'first' ] [ 'controller' , ] [ 'passing' ] [ 'his' ] [ 'child' - ] [ 'victim' ] [ 'up' ] [ 'the' ] [ 'social' ] [ 'ladder' ] [ 'of' ] [ 'abuse' ] [ 'in' ] [ 'return' ] [ 'for' ] [ 'status' , ] [ 'protection'Output after the first prompt : ['This']\n",
      " \"This\" : \"1\",  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [01:09<17:32, 11.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"This\" : \"1\",  }\n",
      "[]\n",
      "['China', 'tax']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [01:12<12:52,  8.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      " 'Many' ] [ 'people' ] [ 'want' ] [ 'to' ] [ 'save' ] [ 'some' ] [ 'travel' ] [ 'expenses' ] [ 'and' ] [ 'earn' ] [ 'some' ] [ 'more' ] [ 'money' ] , so it is very common for them not to return home for a year or two . Output after the first prompt : ['Many']\n",
      " 'Many' : '1',  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [01:20<12:49,  8.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { 'Many' : '1',  }\n",
      "[]\n",
      "['Franklin', 'PERSON'], ['L.F.', 'ORG']] Output after the first prompt : []\n",
      " \"The\" : \"0\",  \"rating\" : \"1\",  \"concern\" : \"2\",  \"said\" : \"3\",  \"Franklin's\" : \"4\",  \"'s\" : \"5\",  \"troubled\" : \"6\",  \"diversification\" : \"7\",  \"record\" : \"8\",  \"in\" : \"9\",  \"the\" : \"0\",  \"securities\" : \"1\",  \"business\" : \"2\",  \"'s\" : \"5\",  \"L.F.\" : \"4\",  \"Rothschild\" : \"6\",  \"subsidiary\" : \"7\",  \"and\" : \"8\",  \"the\" : \"0\",  \"possible\" : \"1\",  \"sale\" : \"2\",  \"of\" : \"3\",  \"other\" : \"4\",  \"subsidiaries.\" : \"5\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [01:37<16:31, 10.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"The\" : \"0\",  \"rating\" : \"1\",  \"concern\" : \"2\",  \"said\" : \"3\",  \"Franklin's\" : \"4\",  \"'s\" : \"5\",  \"troubled\" : \"6\",  \"diversification\" : \"7\",  \"record\" : \"8\",  \"in\" : \"9\",  \"the\" : \"0\",  \"securities\" : \"1\",  \"business\" : \"2\",  \"'s\" : \"5\",  \"L.F.\" : \"4\",  \"Rothschild\" : \"6\",  \"subsidiary\" : \"7\",  \"and\" : \"8\",  \"the\" : \"0\",  \"possible\" : \"1\",  \"sale\" : \"2\",  \"of\" : \"3\",  \"other\" : \"4\",  \"subsidiaries.\" : \"5\" }\n",
      "[]\n",
      "['Tsao', 'Chi-hung'], ['Water', 'Conservancy', 'Bureau']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [01:40<13:05,  8.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      " 'Guaranteed Law for Disabled Persons' ] Output after the first prompt : ['Guaranteed Law for Disabled Persons']\n",
      " 'Guaranteed Law for Disabled Persons' : 'F',  'People 's Republic of China' : 'E',  '1991' : 'A',  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [01:47<11:44,  7.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { 'Guaranteed Law for Disabled Persons' : 'F',  'People 's Republic of China' : 'E',  '1991' : 'A',  }\n",
      "[]\n",
      "['No.', '2'], ['auto', 'maker'], ['British', 'luxury', '-car', 'maker']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [01:50<09:39,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['New', 'York'], ['Times']] </output> \n",
      "\n",
      "# The code below is the implementation of this task using BERT and a simple pipeline.\n",
      "import torch\n",
      "from transformers import BertTokenizer,BertModel\n",
      "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
      "def extract(sentence):\n",
      "    sentence=sentence.lower()\n",
      "    tokens=[tokenizer.convert_tokens_to_ids(['[CLS]'])]+tokenizer.encode(sentence)+[tokenizer.convert_tokens_to_ids(['[SEP]'])]\n",
      "    input_tensor = torch.tensor([tokens])\n",
      "    output=model(input_tensor)[0][:,1:-1].detach().numpy()\n",
      "    return [tokenizer.decode(i) for i in output if tokenizer.decode(i).isalpha()]Output after the first prompt : []\n",
      " \"New\": \"E\",  \"York\": \"E\",  \"Times\": \"B\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [02:07<14:14,  9.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"New\": \"E\",  \"York\": \"E\",  \"Times\": \"B\" }\n",
      "[]\n",
      "London] , [British Foreign Office], [Iraqi people] , [your British friends] , [mistakes] , [you] , [participate] , [with you]  Output after the first prompt : []\n",
      " \"I\" : \"H\",  \"ask\" : \"G\",  \"you\" : \"NORP\",  \"to\" : \"CARDINAL\",  \"go\" : \"VERB\",  \"London\" : \"E\",  \"and\" : \"CONJUNCTION\",  \"read\" : \"VERB\",  \"the\" : \"DET\",  \"records\" : \"NORP\",  \"of\" : \"PREPOSITION\",  \"the\" : \"DET\",  \"British\" : \"GPE\",  \"Foreign\" : \"FAC\",  \"Office\" : \"FAC\",  \"to\" : \"CARDINAL\",  \"find\" : \"VERB\",  \"out\" : \"PREPOSITION\",  \"some\" : \"DET\",  \"of\" : \"PREPOSITION\",  \"the\" : \"DET\",  \"characteristics\" : \"NORP\",  \"of\" : \""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [02:26<17:41, 12.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"I\" : \"H\",  \"ask\" : \"G\",  \"you\" : \"NORP\",  \"to\" : \"CARDINAL\",  \"go\" : \"VERB\",  \"London\" : \"E\",  \"and\" : \"CONJUNCTION\",  \"read\" : \"VERB\",  \"the\" : \"DET\",  \"records\" : \"NORP\",  \"of\" : \"PREPOSITION\",  \"the\" : \"DET\",  \"British\" : \"GPE\",  \"Foreign\" : \"FAC\",  \"Office\" : \"FAC\",  \"to\" : \"CARDINAL\",  \"find\" : \"VERB\",  \"out\" : \"PREPOSITION\",  \"some\" : \"DET\",  \"of\" : \"PREPOSITION\",  \"the\" : \"DET\",  \"characteristics\" : \"NORP\",  \"of\" : \"}\n",
      "[]\n",
      "Taiwan] </start_output> \n",
      "\n",
      "# The code below will be used to extract named entities in a sentence.\n",
      "import re\n",
      "from nltk import word_tokenize, pos_tag\n",
      "\n",
      "def get_entities(sentence):\n",
      "    # Tokenize the sentence into words and their POS tags\n",
      "    tokens = word_tokenize(sentence)\n",
      "    tagged_tokens = pos_tag(tokens)\n",
      "    \n",
      "    entities = []  # Initialize an empty list to store extracted named entities\n",
      "    \n",
      "    for token, tag in tagged_tokens:\n",
      "        if tag.startswith('NNP') or tag.startswith('NNS'):\n",
      "            entity = re.sub('\\W', '', token)  # Remove non-alphanumeric characters from the word\n",
      "            \n",
      "            entities.append(entity)  # Add the extracted named entity to the list\n",
      "    \n",
      "    return entities\n",
      "\n",
      "# Example usage:\n",
      "sentence = \"Japan is a country\"\n",
      "Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [02:41<18:48, 13.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['they']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [02:44<14:09, 10.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "And, that is] [the whole notion of the use of and abuse of confidential sources the protections that reporters do or do n't have and the propensity of uh the United States government to have more secrecy not less /.] </end_output>\n",
      "### ASSISTANT : <start_input> And that is the whole notion of the use of and abuse of confidential sources the protections that reporters do or do n't have and the propensity of uh the United States government to have more secrecy not less /. <end_input>\n",
      "    ### USER : <start_output> [And, that is] [the whole notion of the use of and abuse of confidential sources the protections that reporters do or do n't have and the propensity of uh the United States government to have more secrecy not less /.] </end_output>\n",
      "### ASSISTANT : <start_input> And that is the whole notion of the use of and abuse of confidential sources the protections that reportersOutput after the first prompt : []\n",
      " \"And\" : \"0\",  \"that\" : \"4\",  \"is\" : \"A\",  \"the\" : \"5\",  \"whole\" : \"8\",  \"notion\" : \"9\",  \"of\" : \"1\",  \"use\" : \"2\",  \"and\" : \"0\",  \"abuse\" : \"3\",  \"confidential\" : \"4\",  \"sources\" : \"B\",  \"the\" : \"5\",  \"protections\" : \"9\",  \"that\" : \"4\",  \"reporters\" : \"C\",  \"do\" : \"A\",  \"or\" : \"0\",  \"don't\" : \"1\",  \"have\" : \"2\",  \"and\" : \"0\",  \"the\" : \"5\",  \"propensity\" : \"9\",  \"of\" : \"1\",  \"uh\" : \"4\",  \"the\" : \"5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [03:13<21:58, 15.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"And\" : \"0\",  \"that\" : \"4\",  \"is\" : \"A\",  \"the\" : \"5\",  \"whole\" : \"8\",  \"notion\" : \"9\",  \"of\" : \"1\",  \"use\" : \"2\",  \"and\" : \"0\",  \"abuse\" : \"3\",  \"confidential\" : \"4\",  \"sources\" : \"B\",  \"the\" : \"5\",  \"protections\" : \"9\",  \"that\" : \"4\",  \"reporters\" : \"C\",  \"do\" : \"A\",  \"or\" : \"0\",  \"don't\" : \"1\",  \"have\" : \"2\",  \"and\" : \"0\",  \"the\" : \"5\",  \"propensity\" : \"9\",  \"of\" : \"1\",  \"uh\" : \"4\",  \"the\" : \"5}\n",
      "[]\n",
      "Lucy Dalglish, Miller, Floyd Abrams, Joe Tate] </end_output> \n",
      "\n",
      "# The code below is the implementation of this task using a pre-trained BERT model and fine tuning it on OntoNote5 dataset.\n",
      "import torch\n",
      "from transformers import BertTokenizerFast, AutoModelForSequenceClassification\n",
      "\n",
      "def extract_entities(text):\n",
      "    tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
      "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-large-cased\")\n",
      "    input_ids = torch.tensor([tokenizer.encode(text, add_special_tokens=True)])\n",
      "    outputs = model(input_ids)[0]\n",
      "    logits = outputs[0].detach().numpy()\n",
      "    entities = []\n",
      "    for i in range(len(logits)):\n",
      "        if logits[i] > 0.5:\n",
      "            startOutput after the first prompt : []\n",
      " \"Lucy Dalglish\" : \"4\",  \"the big news for me was Miller recounting what her lawyer Floyd Abrams had said to her about a conversation that he had with Joe Tate who was Scooter Libby 's lawyer /.\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [03:32<22:55, 16.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Lucy Dalglish\" : \"4\",  \"the big news for me was Miller recounting what her lawyer Floyd Abrams had said to her about a conversation that he had with Joe Tate who was Scooter Libby 's lawyer /.\" }\n",
      "[]\n",
      "Francis, X., Curzio] Output after the first prompt : []\n",
      " \"No\" : \"H\",  \"entities\" : [ ],  \"found\" : \"E\",  \"in\" : \"A\",  \"Queens\" : \"GPE\",  \"newsletter\" : \"NORP\",  \"writer\" : \"PERSON\",  \"Francis\" : \"PERSON\",  \"X.\" : \"ORDINAL\",  \"Curzio\" : \"PERSON\",  \"actually\" : \"E\",  \"did\" : \"VBD\",  \"it\" : \"PRP$\",  \": He\" : \"CC\",  \"stated\" : \"VBZ\",  \"in\" : \"IN\",  \"writing\" : \"NNS\",  \"September\" : \"DATE\",  \"1987\" : \"ORDINAL\",  \"that\" : \"WDT\",  \"the\" : \"DT\",  \"Dow\" : \"PRODUCT\",  \"Jones\" : \"PERSON\",  \""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [03:49<22:37, 16.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"No\" : \"H\",  \"entities\" : [ ],  \"found\" : \"E\",  \"in\" : \"A\",  \"Queens\" : \"GPE\",  \"newsletter\" : \"NORP\",  \"writer\" : \"PERSON\",  \"Francis\" : \"PERSON\",  \"X.\" : \"ORDINAL\",  \"Curzio\" : \"PERSON\",  \"actually\" : \"E\",  \"did\" : \"VBD\",  \"it\" : \"PRP$\",  \": He\" : \"CC\",  \"stated\" : \"VBZ\",  \"in\" : \"IN\",  \"writing\" : \"NNS\",  \"September\" : \"DATE\",  \"1987\" : \"ORDINAL\",  \"that\" : \"WDT\",  \"the\" : \"DT\",  \"Dow\" : \"PRODUCT\",  \"Jones\" : \"PERSON\",  \"}\n",
      "[]\n",
      "['Iran']] </stop_output> \n",
      "\n",
      "# The code below is the implementation of this task using BERT and Huggingface Transformers library.\n",
      "import torch\n",
      "from transformers import BertTokenizer,BertModel\n",
      "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
      "def extract_entities(text):\n",
      "    tokens= tokenizer.encode(text)\n",
      "    input_ids, attention_masks = torch.tensor([tokens]),torch.tensor([[1]*len(tokens)]).float()\n",
      "    outputs = model(input_ids,attention_masks)[0]\n",
      "    logits = outputs[:,:,-1].detach().numpy()\n",
      "    entities=[]\n",
      "    for i in range(logits.shape[0]):\n",
      "        if (i==0) or ((logits[i-1]<0.5)&(logOutput after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [04:04<21:38, 16.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['Eurovision', 'Song', 'Contest']] Output after the first prompt : []\n",
      " \"Eurovision\" : \"F\",  \"Song\" : \"C\",  \"Contest\" : \"H\",  \"world's\" : \"A\",  \"most\" : \"B\",  \"boring\" : \"D\",  \"TV\" : \"G\",  \"show\" : \"E\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [04:12<18:09, 13.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Eurovision\" : \"F\",  \"Song\" : \"C\",  \"Contest\" : \"H\",  \"world's\" : \"A\",  \"most\" : \"B\",  \"boring\" : \"D\",  \"TV\" : \"G\",  \"show\" : \"E\" }\n",
      "[]\n",
      "['he'], ['God']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [04:14<13:33, 10.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "Christy Whitman, White House] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [04:17<10:24,  8.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['Japanese'], ['Taiwan']] Output after the first prompt : []\n",
      " \"However\" : \"0\",  \"during\" : \"1\",  \"the\" : \"2\",  \"Japanese\" : \"3\",  \"occupation\" : \"4\",  \", as a result of silting , and because the Japanese invested a huge effort in developing the port at Keelung , Tanshui faded in importance , never again to see the likes of its glory days .\" : \"\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [04:26<10:41,  8.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"However\" : \"0\",  \"during\" : \"1\",  \"the\" : \"2\",  \"Japanese\" : \"3\",  \"occupation\" : \"4\",  \", as a result of silting , and because the Japanese invested a huge effort in developing the port at Keelung , Tanshui faded in importance , never again to see the likes of its glory days .\" : \"\" }\n",
      "[]\n",
      "['Vladimir', 'Putin'], ['Bill', 'Clinton']] Output after the first prompt : []\n",
      " \"That\" : \"H\",  \"s\" : \"A\",  \"'s\" : \"B\",  \"an\" : \"C\",  \"interesting\" : \"D\",  \"question\" : \"E\",  \"because\" : \"F\",  \"Vladimir Putin\" : \"G\",  \"on\" : \"H\",  \"Wednesday\" : \"I\",  \"addressed\" : \"J\",  \"the Indian Parliament\" : \"K\",  \"just as Bill Clinton had done six months ago , and I think it 's safe to say that Mr. Putin actually probably accomplished more than Mr. Clinton did in the hard concrete terms of arms sales and that sort of thing , but it was Mr. Clinton that stole the Indian lawmakers ' hearts .\" : \"L\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [04:40<12:35, 10.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"That\" : \"H\",  \"s\" : \"A\",  \"'s\" : \"B\",  \"an\" : \"C\",  \"interesting\" : \"D\",  \"question\" : \"E\",  \"because\" : \"F\",  \"Vladimir Putin\" : \"G\",  \"on\" : \"H\",  \"Wednesday\" : \"I\",  \"addressed\" : \"J\",  \"the Indian Parliament\" : \"K\",  \"just as Bill Clinton had done six months ago , and I think it 's safe to say that Mr. Putin actually probably accomplished more than Mr. Clinton did in the hard concrete terms of arms sales and that sort of thing , but it was Mr. Clinton that stole the Indian lawmakers ' hearts .\" : \"L\" }\n",
      "[]\n",
      "['This'], ['simple']] Output after the first prompt : []\n",
      " \"No\" : \"1\",  \"entities\" : [ ],  \"found\" : \"2\",  \"in\" : \"3\",  \"This\" : \"4\",  \"simple\" : \"5\",  \", \" : \"6\",  \"convenient\" : \"7\",  \"and\" : \"8\",  \"efficient\" : \"9\",  \"input\" : \"0\",  \"method\" : \"A\",  \"covers\" : \"B\",  \"more\" : \"C\",  \"than\" : \"D\",  \"85\" : \"E\",  \"%\" : \"F\",  \"of\" : \"G\",  \"the\" : \"H\",  \"nation 's\" : \"1\",  \"computer\" : \"2\",  \"users\" : \"3\",  \", \" : \"6\",  \"and\" : \"8\",  \"has\" : \"9\",  \"become\" : \"0\",  \"the\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [04:56<14:44, 11.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"No\" : \"1\",  \"entities\" : [ ],  \"found\" : \"2\",  \"in\" : \"3\",  \"This\" : \"4\",  \"simple\" : \"5\",  \", \" : \"6\",  \"convenient\" : \"7\",  \"and\" : \"8\",  \"efficient\" : \"9\",  \"input\" : \"0\",  \"method\" : \"A\",  \"covers\" : \"B\",  \"more\" : \"C\",  \"than\" : \"D\",  \"85\" : \"E\",  \"%\" : \"F\",  \"of\" : \"G\",  \"the\" : \"H\",  \"nation 's\" : \"1\",  \"computer\" : \"2\",  \"users\" : \"3\",  \", \" : \"6\",  \"and\" : \"8\",  \"has\" : \"9\",  \"become\" : \"0\",  \"the\"}\n",
      "[]\n",
      "Edison] </stop_output> \n",
      "\n",
      "# The code below is a template that you will have to complete. You should only modify the function named \"extract_named_entities\" and not any other part of this file.\n",
      "import re\n",
      "from collections import Counter, defaultdict\n",
      "\n",
      "def extract_named_entities(sentence):\n",
      "    \"\"\"\n",
      "    Extract all the entities in a sentence that could be tagged by one of the OntoNote5 dataset tags (see above).\n",
      "    The function should return a list with all the named entities. For example, with the sentence \"Japan is a country\" as input, you would answer [\"Japan\"].\n",
      "    \"\"\"\n",
      "    # TODO: Complete this function to extract all the named entities in the sentence and output them as a python list of STRINGs.\n",
      "    return []Output after the first prompt : []\n",
      " \"They\" : \"H\",  \"know\" : \"G\",  \"the\" : \"A\",  \"utility\" : \"B\",  \"recently\" : \"F\",  \"declared\" : \"C\",  \"a\" : \"D\",  \"dividend\" : \"E\",  \"for\" : \"H\",  \"its\" : \"G\",  \"stockholders\" : \"A\",  \", but\" : \"I\",  \"Edison's\" : \"B\",  \"'s\" : \"J\",  \"Tom\" : \"D\",  \"Higgins\" : \"F\",  \"says\" : \"C\",  \"the\" : \"A\",  \"firm\" : \"B\",  \"can not continue to pay high prices for electricity while keeping prices low for its customers .\" : \"G\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [05:21<19:13, 15.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"They\" : \"H\",  \"know\" : \"G\",  \"the\" : \"A\",  \"utility\" : \"B\",  \"recently\" : \"F\",  \"declared\" : \"C\",  \"a\" : \"D\",  \"dividend\" : \"E\",  \"for\" : \"H\",  \"its\" : \"G\",  \"stockholders\" : \"A\",  \", but\" : \"I\",  \"Edison's\" : \"B\",  \"'s\" : \"J\",  \"Tom\" : \"D\",  \"Higgins\" : \"F\",  \"says\" : \"C\",  \"the\" : \"A\",  \"firm\" : \"B\",  \"can not continue to pay high prices for electricity while keeping prices low for its customers .\" : \"G\" }\n",
      "[]\n",
      "['CIA', 'leak'], ['media', 'folks'], ['indictment', 'official'], ['testified', 'grand'], ['jury']] </end_output> \n",
      "\n",
      "# The code below is the implementation of this task. It uses a pre-trained BERT model to extract named entities from text and outputs them in a list format as specified by the user.\n",
      "import torch\n",
      "from transformers import BertTokenizer, BertForMaskedLM\n",
      "\n",
      "def get_named_entities(text):\n",
      "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      "    model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
      "\n",
      "    # Tokenize the input text and add a special [MASK] token to each word\n",
      "    tokens = tokenizer(text, return_tensors='pt').input_ids[0].tolist()\n",
      "    masked_tokens = []\n",
      "    for i in range(len(Output after the first prompt : []\n",
      " \"As\" : \"0\",  \"the\" : \"0\",  \"CIA\" : \"F\",  \"leak\" : \"0\",  \"investigation\" : \"0\",  \"continued\" : \"0\",  \"this\" : \"0\",  \"week\" : \"E\",  \"media\" : \"B\",  \"folks\" : \"0\",  \"did\" : \"0\",  \"plenty\" : \"0\",  \"of\" : \"0\",  \"speculating\" : \"0\",  \"with\" : \"0\",  \"no\" : \"0\",  \"evidence\" : \"0\",  \"about\" : \"0\",  \"the\" : \"0\",  \"possible\" : \"0\",  \"indictment\" : \"F\",  \"of\" : \"0\",  \"a\" : \"0\",  \"certain\" : \"0\",  \"top\" : \"0\",  \"White\" :"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [05:50<23:41, 19.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"As\" : \"0\",  \"the\" : \"0\",  \"CIA\" : \"F\",  \"leak\" : \"0\",  \"investigation\" : \"0\",  \"continued\" : \"0\",  \"this\" : \"0\",  \"week\" : \"E\",  \"media\" : \"B\",  \"folks\" : \"0\",  \"did\" : \"0\",  \"plenty\" : \"0\",  \"of\" : \"0\",  \"speculating\" : \"0\",  \"with\" : \"0\",  \"no\" : \"0\",  \"evidence\" : \"0\",  \"about\" : \"0\",  \"the\" : \"0\",  \"possible\" : \"0\",  \"indictment\" : \"F\",  \"of\" : \"0\",  \"a\" : \"0\",  \"certain\" : \"0\",  \"top\" : \"0\",  \"White\" :}\n",
      "[]\n",
      "Dirk Van Dongen] Output after the first prompt : []\n",
      " \"But\" : \"H\",  \"Dirk Van Dongen\" : \"G\",  \"president of the National Association of Wholesaler - Distributors\" : \"B\",  \", said that last month 's rise `` is n't as bad an omen '' as the 0.9 % figure suggests .\" : \"\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [05:57<18:52, 15.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"But\" : \"H\",  \"Dirk Van Dongen\" : \"G\",  \"president of the National Association of Wholesaler - Distributors\" : \"B\",  \", said that last month 's rise `` is n't as bad an omen '' as the 0.9 % figure suggests .\" : \"\" }\n",
      "[]\n",
      "['taxi', 'driver'], ['gas', 'price'], ['licensing', 'regulation']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [06:01<14:22, 12.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['East', 'Second', 'Ring', 'Road'], ['West', 'Dawang', 'Road']] Output after the first prompt : []\n",
      " \"68\" : \"CARDINAL\",  \"East Second Ring Road\" : \"LOC\",  \"East Fourth Ring Road\" : \"LOC\",  \"West Dawang Road\" : \"LOC\",  \"210\" : \"QUANTITY\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [06:09<12:29, 10.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"68\" : \"CARDINAL\",  \"East Second Ring Road\" : \"LOC\",  \"East Fourth Ring Road\" : \"LOC\",  \"West Dawang Road\" : \"LOC\",  \"210\" : \"QUANTITY\" }\n",
      "[]\n",
      " 'Lin' ] Output after the first prompt : ['Lin']\n",
      " 'Lin' : '4',  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [06:12<09:46,  8.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { 'Lin' : '4',  }\n",
      "[]\n",
      "['one', 'China']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [06:15<07:49,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "%uh] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [06:18<06:17,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['100,000', 'US dollars'], ['Huichun']] Output after the first prompt : []\n",
      " \"No\" : \"1\",  \"entities found in Now , 100,000 US dollars has been taken from the fund to make an evaluation of the environment in China 's Huichun 's frontier cooperative area in the expectation of providing consultation on the environmental aspect for the entry of large quantities of foreign capital into this area .\" : \"E\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [06:26<07:01,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"No\" : \"1\",  \"entities found in Now , 100,000 US dollars has been taken from the fund to make an evaluation of the environment in China 's Huichun 's frontier cooperative area in the expectation of providing consultation on the environmental aspect for the entry of large quantities of foreign capital into this area .\" : \"E\" }\n",
      "[]\n",
      "['In', 'the'], ['first', 'two'], ['months'], ['of'], ['this'], ['year']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [06:30<06:12,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['Ash', 'Al-Queda']] </output> \n",
      "\n",
      "# The code below will be used for testing your model on a dataset of sentences and the output should match with the expected answer in the file \"test.txt\"\n",
      "import json\n",
      "from collections import defaultdict, Counter\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import re\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "nltk.download('wordnet')\n",
      "nltk.download('averaged_perceptron_tagger')\n",
      "from nltk import word_tokenize, pos_tag\n",
      "from nltk.corpus import stopwords\n",
      "import spacy\n",
      "nlp = spacy.load(\"en\")\n",
      "\n",
      "def get_entities(sentence):\n",
      "    entities = []\n",
      "    for token in sentence:\n",
      "        if token.pos_ == \"PROPN\":\n",
      "            entities.append(token)\n",
      "    return entities\n",
      "\n",
      "# Load the dataset and extract all sentences fromOutput after the first prompt : []\n",
      " \"And\" : \"0\",  \"with\" : \"0\",  \"each\" : \"0\",  \"passing\" : \"0\",  \"day\" : \"A\",  \"I\" : \"4\",  \"would\" : \"0\",  \"argue\" : \"0\",  \"to\" : \"0\",  \"you\" : \"4\",  \"more\" : \"0\",  \"important\" : \"0\",  \"than\" : \"0\",  \"that\" : \"0\",  \"missile\" : \"8\",  \"Ash\" : \"4\",  \"would\" : \"0\",  \"take\" : \"0\",  \"out\" : \"0\",  \"on\" : \"0\",  \"the\" : \"0\",  \"gantry\" : \"F\",  \"if\" : \"0\",  \"it\" : \"8\",  \"turns\" : \"0\",  \"up\" : \"0\", "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [06:58<13:05, 12.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"And\" : \"0\",  \"with\" : \"0\",  \"each\" : \"0\",  \"passing\" : \"0\",  \"day\" : \"A\",  \"I\" : \"4\",  \"would\" : \"0\",  \"argue\" : \"0\",  \"to\" : \"0\",  \"you\" : \"4\",  \"more\" : \"0\",  \"important\" : \"0\",  \"than\" : \"0\",  \"that\" : \"0\",  \"missile\" : \"8\",  \"Ash\" : \"4\",  \"would\" : \"0\",  \"take\" : \"0\",  \"out\" : \"0\",  \"on\" : \"0\",  \"the\" : \"0\",  \"gantry\" : \"F\",  \"if\" : \"0\",  \"it\" : \"8\",  \"turns\" : \"0\",  \"up\" : \"0\", }\n",
      "[]\n",
      "The] [situation] [you] [mentioned] , [is] [it] [common] [or] [is] [it] [a] [rather] [extreme] [example] , [because] [generally] , [people] [will] [think] [that] [grandparents] [will] [particularly] [love] [and] [care] [about] [their] [grandchildren] ? Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [07:05<11:06, 10.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['Pelswick']] Output after the first prompt : []\n",
      " \"And\" : \"0\",  \"those\" : \"0\",  \"dreams\" : \"4\",  \"were\" : \"A\",  \"a\" : \"0\",  \"little\" : \"0\",  \"closer\" : \"0\",  \"to\" : \"0\",  \"coming\" : \"0\",  \"true\" : \"0\",  \"this\" : \"0\",  \"day\" : \"A\",  \",,\" : \"\",  \"and\" : \"0\",  \"with\" : \"0\",  \"shows\" : \"B\",  \"like\" : \"0\",  \"\"Pelswick\"\" : \"3\",  \",\" : \"\",  \"they\" : \"4\",  \"say\" : \"-1\",  \"--\" : \"\",  \"well,\" : \",\",  \"when\" : \"A\",  \"they\" : \"4\",  \"get\" : \"0\",  \"a\" : \"0\",  \"little"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [07:19<11:44, 11.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"And\" : \"0\",  \"those\" : \"0\",  \"dreams\" : \"4\",  \"were\" : \"A\",  \"a\" : \"0\",  \"little\" : \"0\",  \"closer\" : \"0\",  \"to\" : \"0\",  \"coming\" : \"0\",  \"true\" : \"0\",  \"this\" : \"0\",  \"day\" : \"A\",  \",,\" : \"\",  \"and\" : \"0\",  \"with\" : \"0\",  \"shows\" : \"B\",  \"like\" : \"0\",  \"\"Pelswick\"\" : \"3\",  \",\" : \"\",  \"they\" : \"4\",  \"say\" : \"-1\",  \"--\" : \"\",  \"well,\" : \",\",  \"when\" : \"A\",  \"they\" : \"4\",  \"get\" : \"0\",  \"a\" : \"0\",  \"little}\n",
      "[]\n",
      "['housewife']] Output after the first prompt : []\n",
      " \"Or\" : \"F\",   \"if\" : \"G\",    \"you\" : \"H\",     \"are\" : \"B\",      \"a\" : \"NORP\",       \"housewife\" : \"PERSON\",        \"taking\" : \"VERB\",         \"care\" : \"NOUN\",          \"of\" : \"PREPOSITION\",    \"three\" : \"CARDINAL\",     \"generations\" : \"QUANTITY\",      \"and\" : \"CONJUNCTION\",   \"resentful\" : \"ADJECTIVE\",       \"that\" : \"PRONOUN\",        \"you\" : \"H\",             \"can't\" : \"VERB\",          \"go\" : \"VERB\",            \"out\" : \"PREPOSITION\",    \"and\" : \"CONJUNCTION\",   \"get\" : \"VERB\",           \"the\" : \"DET\",              \"sense\" : \"NOUN\",         \"of"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [07:31<11:54, 11.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Or\" : \"F\",   \"if\" : \"G\",    \"you\" : \"H\",     \"are\" : \"B\",      \"a\" : \"NORP\",       \"housewife\" : \"PERSON\",        \"taking\" : \"VERB\",         \"care\" : \"NOUN\",          \"of\" : \"PREPOSITION\",    \"three\" : \"CARDINAL\",     \"generations\" : \"QUANTITY\",      \"and\" : \"CONJUNCTION\",   \"resentful\" : \"ADJECTIVE\",       \"that\" : \"PRONOUN\",        \"you\" : \"H\",             \"can't\" : \"VERB\",          \"go\" : \"VERB\",            \"out\" : \"PREPOSITION\",    \"and\" : \"CONJUNCTION\",   \"get\" : \"VERB\",           \"the\" : \"DET\",              \"sense\" : \"NOUN\",         \"of}\n",
      "[]\n",
      "And, you'll see a lot of this unfold this week both in Europe when Secretary Rice meets with her counterparts on Wednesday at the GA summit in St. Petersburgh At the end of the week when the president is there /] </end_output>\n",
      "### USER : <start_input> And you 'll see a lot of this unfold this week both in Europe when Secretary Rice meets with her counterparts on Wednesday at the GA summit in St. Petersburgh At the end of the week when the president is there /. <end_input>\n",
      "### ASSISTANT : <start_output> [And, you'll see a lot of this unfold this week both in Europe when Secretary Rice meets with her counterparts on Wednesday at the GA summit in St. Petersburgh At the end of the week when the president is there /] </end_output>\n",
      "### USER : <start_input> And you 'll see a lot of this unfold this week both in Europe when Secretary Rice meets with her counterOutput after the first prompt : []\n",
      " \"And\" : \"\",  \"you 'll\" : \"\",  \"see\" : \"\",  \"a\" : \"\",  \"lot\" : \"\",  \"of\" : \"\",  \"this\" : \"\",  \"unfold\" : \"\",  \"this\" : \"\",  \"week\" : \"\",  \"both\" : \"\",  \"in\" : \"\",  \"Europe\" : \"E\",  \"when\" : \"\",  \"Secretary\" : \"\",  \"Rice\" : \"P\",  \"meets\" : \"\",  \"with\" : \"\",  \"her\" : \"\",  \"counterparts\" : \"\",  \"on\" : \"\",  \"Wednesday\" : \"D\",  \"at\" : \"\",  \"the\" : \"\",  \"GA\" : \"GPE\",  \"summit\" : \"\",  \"in\" : \"\",  \"St. Petersburgh\" : \"LOC\",  \"At\" : \"\",  \"the\" : \"\",  \"end\" : \"\",  \"of\" : \"\", "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [07:54<14:50, 15.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"And\" : \"\",  \"you 'll\" : \"\",  \"see\" : \"\",  \"a\" : \"\",  \"lot\" : \"\",  \"of\" : \"\",  \"this\" : \"\",  \"unfold\" : \"\",  \"this\" : \"\",  \"week\" : \"\",  \"both\" : \"\",  \"in\" : \"\",  \"Europe\" : \"E\",  \"when\" : \"\",  \"Secretary\" : \"\",  \"Rice\" : \"P\",  \"meets\" : \"\",  \"with\" : \"\",  \"her\" : \"\",  \"counterparts\" : \"\",  \"on\" : \"\",  \"Wednesday\" : \"D\",  \"at\" : \"\",  \"the\" : \"\",  \"GA\" : \"GPE\",  \"summit\" : \"\",  \"in\" : \"\",  \"St. Petersburgh\" : \"LOC\",  \"At\" : \"\",  \"the\" : \"\",  \"end\" : \"\",  \"of\" : \"\", }\n",
      "[]\n",
      "''] Output after the first prompt : ['']\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [07:56<10:52, 11.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['US', 'PERSON'], ['president', 'WORK_OF_ART'], ['1995 financial year overseas activities fund appropriation act ', 'PRODUCT']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [08:00<08:35,  9.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['Beijing']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [08:02<06:29,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['Taiwan']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [08:04<05:00,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['ten'], ['two']] Output after the first prompt : []\n",
      " \"In\" : \"0\",  \"the\" : \"0\",  \"past\" : \"0\",  \"two\" : \"1\",  \"years\" : \"6\",  \", the\" : \"0\",  \"ten\" : \"2\",  \"agricultural\" : \"4\",  \"new\" : \"0\",  \"high\" : \"0\",  \"level\" : \"0\",  \"technology\" : \"8\",  \"development\" : \"0\",  \"and\" : \"0\",  \"model\" : \"0\",  \"zones\" : \"0\",  \"have\" : \"0\",  \"promoted\" : \"0\",  \"more\" : \"0\",  \"than\" : \"0\",  \"a\" : \"0\",  \"hundred\" : \"3\",  \"new\" : \"0\",  \"agricultural\" : \"4\",  \"varieties\" : \"5\",  \", developed\" :"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [08:17<06:51,  7.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"In\" : \"0\",  \"the\" : \"0\",  \"past\" : \"0\",  \"two\" : \"1\",  \"years\" : \"6\",  \", the\" : \"0\",  \"ten\" : \"2\",  \"agricultural\" : \"4\",  \"new\" : \"0\",  \"high\" : \"0\",  \"level\" : \"0\",  \"technology\" : \"8\",  \"development\" : \"0\",  \"and\" : \"0\",  \"model\" : \"0\",  \"zones\" : \"0\",  \"have\" : \"0\",  \"promoted\" : \"0\",  \"more\" : \"0\",  \"than\" : \"0\",  \"a\" : \"0\",  \"hundred\" : \"3\",  \"new\" : \"0\",  \"agricultural\" : \"4\",  \"varieties\" : \"5\",  \", developed\" :}\n",
      "[]\n",
      " 'Japan' ] Output after the first prompt : ['Japan']\n",
      " \"Japan\" : \"E\", "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47/100 [08:20<05:28,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Japan\" : \"E\", }\n",
      "[]\n",
      "['European', 'defense', 'industry'], ['West Germany ', 'Siemens AG'], ['Britain ', 'Plessey Co.'], ['Daimler - Benz']] Output after the first prompt : []\n",
      " \"The\" : null,  \"European\" : null,  \"defense\" : null,  \"industry\" : null,  \"is\" : null,  \"consolidating\" : null,  \";\" : null,  \"for\" : null,  \"instance\" : null,  \",,\" : null,  \"West\" : null,  \"Germany's\" : null,  \"Siemens\" : null,  \"AG\" : null,  \"recently\" : null,  \"joined\" : null,  \"GEC\" : null,  \"in\" : null,  \"a\" : null,  \"takeover\" : null,  \"of\" : null,  \"Britain's\" : null,  \"Plessey\" : null,  \"Co.\" : null,  \",,\" : null,  \"and\" : null,  \"Daimler-Benz\" : null,  \"agreed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48/100 [08:34<07:30,  8.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"The\" : null,  \"European\" : null,  \"defense\" : null,  \"industry\" : null,  \"is\" : null,  \"consolidating\" : null,  \";\" : null,  \"for\" : null,  \"instance\" : null,  \",,\" : null,  \"West\" : null,  \"Germany's\" : null,  \"Siemens\" : null,  \"AG\" : null,  \"recently\" : null,  \"joined\" : null,  \"GEC\" : null,  \"in\" : null,  \"a\" : null,  \"takeover\" : null,  \"of\" : null,  \"Britain's\" : null,  \"Plessey\" : null,  \"Co.\" : null,  \",,\" : null,  \"and\" : null,  \"Daimler-Benz\" : null,  \"agreed}\n",
      "[]\n",
      "['truck'], ['bombs']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/100 [08:37<05:51,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      " 'We' , 'day' ] Output after the first prompt : ['We', 'day']\n",
      "0: '1', 2: '3' "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [08:40<04:42,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : {0: '1', 2: '3' }\n",
      "[]\n",
      "Kuo, TGS] </end_output> \n",
      "\n",
      "# The code below will be used by the assistant.\n",
      "import re\n",
      "from nltk import word_tokenize\n",
      "from collections import Counter\n",
      "\n",
      "def extract(text):\n",
      "    # Tokenization of text into words and sentences\n",
      "    tokens = [word for sent in word_tokenize(text) for word in sent]\n",
      "    # Extracting the named entities from each sentence\n",
      "    named_entities = []\n",
      "    for i, token in enumerate(tokens):\n",
      "        if re.match('[A-Z][a-z]*', token):  # Checks whether a token is an acronym or not\n",
      "            tokens[i] += '.'\n",
      "        if len(token) > 1:\n",
      "            named_entities.append(token)\n",
      "    return list(set(named_entities))Output after the first prompt : []\n",
      " \"No\" : \"1\",  \"entities\" : [ ],  \"found\" : \"2\",  \"in\" : \"3\",  \"Kuo\" : \"4\",  \"comments\" : \"5\",  \"that\" : \"6\",  \"contract\" : \"7\",  \"production\" : \"8\",  \"of\" : \"9\",  \"chips\" : \"0\",  \"is\" : \"A\",  \"merely\" : \"B\",  \"a\" : \"C\",  \"service\" : \"D\",  \"activity\" : \"E\",  \"and\" : \"F\",  \"a\" : \"G\",  \"source\" : \"H\",  \"of\" : \"I\",  \"income\" : \"J\",  \"for\" : \"K\",  \"the\" : \"L\",  \"company\" : \"M\",  \"-\" : \"N\",  \"TGS\" : \"O\",  \"'s\" : \""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [09:01<08:22, 10.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"No\" : \"1\",  \"entities\" : [ ],  \"found\" : \"2\",  \"in\" : \"3\",  \"Kuo\" : \"4\",  \"comments\" : \"5\",  \"that\" : \"6\",  \"contract\" : \"7\",  \"production\" : \"8\",  \"of\" : \"9\",  \"chips\" : \"0\",  \"is\" : \"A\",  \"merely\" : \"B\",  \"a\" : \"C\",  \"service\" : \"D\",  \"activity\" : \"E\",  \"and\" : \"F\",  \"a\" : \"G\",  \"source\" : \"H\",  \"of\" : \"I\",  \"income\" : \"J\",  \"for\" : \"K\",  \"the\" : \"L\",  \"company\" : \"M\",  \"-\" : \"N\",  \"TGS\" : \"O\",  \"'s\" : \"}\n",
      "[]\n",
      "Kuo] [states] [that] [at present ] [the number of known target sequences for disease - treating drugs is only around 500 , but as the human genome is mapped out this number can be expected to increase tenfold .] </end_output>\n",
      "### USER : <start_input> The first step in any analysis of a new drug or biologic is to determine its mechanism of action (MOA) and target(s). This information is critical for understanding the potential therapeutic utility, safety profile, and optimal dosing regimen. In addition , it can provide insight into how best to develop companion diagnostics . <end_input>\n",
      "### ASSISTANT : <start_output> [The] [first step in any analysis of a new drug or biologic is to determine its mechanism of action (MOA) and target(s)] [This information is critical for understanding the potential therapeutic utility , safety profile , and optimal dosing regimen . In addition ] [it canOutput after the first prompt : []\n",
      " \"Kuo\" : \"NORP\",  \"states\" : \"E\",  \"that\" : \"\",  \"at\" : \"\",  \"present\" : \"\",  \", the\" : \"\",  \"number\" : \"QUANTITY\",  \"of\" : \"\",  \"known\" : \"\",  \"target\" : \"\",  \"sequences\" : \"\",  \"for\" : \"\",  \"disease\" : \"\",  \"- treating\" : \"\",  \"drugs\" : \"\",  \"is\" : \"\",  \"only\" : \"\",  \"around\" : \"\",  500 : \"QUANTITY\",  \", but\" : \"\",  \"as\" : \"\",  \"the\" : \"\",  \"human\" : \"\",  \"genome\" : \"\",  \"is\" : \"\",  \"mapped\" : \"\",  \"out\" : \"\",  \"this\" : \"\",  \"number\" : \"QUANTITY\",  \"can\" : \"\",  \"be\" : \"\",  \"expected\" : \"\","
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [09:23<11:12, 14.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Kuo\" : \"NORP\",  \"states\" : \"E\",  \"that\" : \"\",  \"at\" : \"\",  \"present\" : \"\",  \", the\" : \"\",  \"number\" : \"QUANTITY\",  \"of\" : \"\",  \"known\" : \"\",  \"target\" : \"\",  \"sequences\" : \"\",  \"for\" : \"\",  \"disease\" : \"\",  \"- treating\" : \"\",  \"drugs\" : \"\",  \"is\" : \"\",  \"only\" : \"\",  \"around\" : \"\",  500 : \"QUANTITY\",  \", but\" : \"\",  \"as\" : \"\",  \"the\" : \"\",  \"human\" : \"\",  \"genome\" : \"\",  \"is\" : \"\",  \"mapped\" : \"\",  \"out\" : \"\",  \"this\" : \"\",  \"number\" : \"QUANTITY\",  \"can\" : \"\",  \"be\" : \"\",  \"expected\" : \"\",}\n",
      "[]\n",
      " 'California' ] Output after the first prompt : ['California']\n",
      " \"California\" : \"E\",   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [09:26<08:18, 10.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"California\" : \"E\",   }\n",
      "[]\n",
      "['Johnsee', 'Lee']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [09:29<06:19,  8.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      " 'Hugo Quackenbush', 'Charles Schwab Corp.', 'San Francisco', 'Schwab', 'margin calls' ] Output after the first prompt : ['Hugo Quackenbush', 'Charles Schwab Corp.', 'San Francisco', 'Schwab', 'margin calls']\n",
      " \"Hugo Quackenbush\" : \"4\",  \"Charles Schwab Corp.\" : \"B\",  \"San Francisco\" : \"5\",  \"Schwab\" : \"0\",  \"margin calls\" : \"C\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [09:35<05:47,  7.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Hugo Quackenbush\" : \"4\",  \"Charles Schwab Corp.\" : \"B\",  \"San Francisco\" : \"5\",  \"Schwab\" : \"0\",  \"margin calls\" : \"C\"}\n",
      "[]\n",
      "Maimaiti] </start_output> \n",
      "\n",
      "# The code below is an example of how you can use this model. You will need to provide your own input text, and specify which named entities you want the model to extract.\n",
      "import json\n",
      "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
      "\n",
      "model = \"microsoft/deberta-v3-large\"  # Replace with the name of your fine-tuned model\n",
      "tokenizer = AutoTokenizer.from_pretrained(model)\n",
      "model = AutoModelForSequenceClassification.from_pretrained(model)\n",
      "\n",
      "# Load the input text and named entities from a JSON file\n",
      "with open('input.json', 'r') as f:\n",
      "    data = json.load(f)\n",
      "\n",
      "text = data['text']  # Replace with your own input text\n",
      "entities = data['entities']  # Replace with the list of named entities you want to extract\n",
      "\n",
      "# Tokenize and encode theOutput after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 56/100 [09:48<06:52,  9.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "Lanny] </start_output> \n",
      "\n",
      "# The code is written in Python3, using NLTK library for NLP tasks (https://www.nltk.org/).\n",
      "\n",
      "import nltk\n",
      "from nltk import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from nltk.tag import pos_tag\n",
      "from nltk.chunk import conlltags2tree, ne_chunk\n",
      "import re\n",
      "\n",
      "# Load the OntoNote5 dataset and create a list of all tags that are in it :\n",
      "ontonote = []\n",
      "with open('OntoNotes-5.0/train/en/pos/tagged', 'r') as f:\n",
      "    for line in f:\n",
      "        ontonote += [line[1]]\n",
      "tags_list = list(set([i[:2] for i in ontonote])) # remove duplicates and keepOutput after the first prompt : []\n",
      " \"They\" : \"H\",  \"gave\" : \"0\",  \"no\" : \"A\",  \"+substantive\" : \"G\",  \"response\" : \"B\",  \"to\" : \"C\",  \"any\" : \"D\",  \"of\" : \"E\",  \"the\" : \"F\",  \"allegations\" : \"H\",  \"until\" : \"A\",  \"Lanny\" : \"G\",  \"Davis\" : \"B\",  \"launched\" : \"0\",  \"his\" : \"C\",  \"PR\" : \"E\",  \"campaign\" : \"F\",  \"on\" : \"H\",  \"the\" : \"F\",  \"Thursday\" : \"A\",  \"before\" : \"G\",  \"the\" : \"F\",  \"broadcast\" : \"B\",  \"and\" : \"C\",  \"offered\" : \"0\",  \"himself\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57/100 [10:11<09:38, 13.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"They\" : \"H\",  \"gave\" : \"0\",  \"no\" : \"A\",  \"+substantive\" : \"G\",  \"response\" : \"B\",  \"to\" : \"C\",  \"any\" : \"D\",  \"of\" : \"E\",  \"the\" : \"F\",  \"allegations\" : \"H\",  \"until\" : \"A\",  \"Lanny\" : \"G\",  \"Davis\" : \"B\",  \"launched\" : \"0\",  \"his\" : \"C\",  \"PR\" : \"E\",  \"campaign\" : \"F\",  \"on\" : \"H\",  \"the\" : \"F\",  \"Thursday\" : \"A\",  \"before\" : \"G\",  \"the\" : \"F\",  \"broadcast\" : \"B\",  \"and\" : \"C\",  \"offered\" : \"0\",  \"himself\"}\n",
      "[]\n",
      " 'President' , 'Clinton' , 'Prime' , 'Minister' , 'Hasina' ] Output after the first prompt : ['President', 'Clinton', 'Prime', 'Minister', 'Hasina']\n",
      " \"President\" : \"4\",  \"Clinton\" : \"4\",  \"Prime\" : \"0\",  \"Minister\" : \"0\",  \"Hasina\" : \"4\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58/100 [10:17<07:47, 11.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"President\" : \"4\",  \"Clinton\" : \"4\",  \"Prime\" : \"0\",  \"Minister\" : \"0\",  \"Hasina\" : \"4\"}\n",
      "[]\n",
      "Florida, California] </stop_output> \n",
      "\n",
      "# The code below is a template for you to fill in. You can use any other programming language as long as it's Python3 compatible and the output format matches what we expect from your assistant.\n",
      "import re\n",
      "from nltk import word_tokenize, pos_tag\n",
      "\n",
      "def extract_named_entities(sentence):\n",
      "    # TODO: Implement this function to return a list of named entities in sentence\n",
      "    pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    user = \"Overall , though , the South and West still outpace the Northeast and Midwest , and fast - growing states like Florida and California ensure that the pattern will continue .\"\n",
      "    assistant = extract_named_entities(user)Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59/100 [10:26<07:10, 10.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "Tanshui] </start_output> \n",
      "\n",
      "# The code below is for training your model. You can use it to train the model on your own data and then test it with the provided dataset.\n",
      "import torch\n",
      "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
      "\n",
      "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
      "\n",
      "# Load the dataset and tokenize it\n",
      "dataset = [[\"Lovers of Tanshui , while keeping close watch over her , are fearful that the giant beast of development may eventually come and bulldoze Tanshui away one piece of land at a time .\"] ]\n",
      "tokenized_inputs = tokenizer(dataset, return_tensors=\"pt\")\n",
      "\n",
      "# Define the loss function and optimizer\n",
      "criterion = torch.nn.CrossEntropyLoss()\n",
      "optimizer = torch.optim.AdamOutput after the first prompt : []\n",
      " \"Lovers\" : \"NORP\",  \"of\" : \"\",  \"Tanshui\" : \"GPE\",  \", while keeping close watch over her , are fearful that the giant beast of development may eventually come and bulldoze Tanshui away one piece of land at a time .\" : \"\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [10:42<08:01, 12.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Lovers\" : \"NORP\",  \"of\" : \"\",  \"Tanshui\" : \"GPE\",  \", while keeping close watch over her , are fearful that the giant beast of development may eventually come and bulldoze Tanshui away one piece of land at a time .\" : \"\" }\n",
      "[]\n",
      "['Saturday'], ['Friday']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [10:44<05:56,  9.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['U.S.', 'US'], ['May', '5'], ['wheat', 'WHEAT']] Output after the first prompt : []\n",
      " \"U.S.\": \"E\",  \"wheat\" : \"FAC\",  \"May\": \"DATE\",  \"carried over into the next season -- before the winter wheat now being planted is harvested -- are projected to drop to 443 million bushels .\": \"\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [10:50<05:14,  8.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"U.S.\": \"E\",  \"wheat\" : \"FAC\",  \"May\": \"DATE\",  \"carried over into the next season -- before the winter wheat now being planted is harvested -- are projected to drop to 443 million bushels .\": \"\" }\n",
      "[]\n",
      "['French'], ['Europe']] Output after the first prompt : []\n",
      " \"The\" : \"O\",  \"most\" : \"O\",  \"militant\" : \"O\",  \"opposition\" : \"O\",  \"to\" : \"O\",  \"American\" : \"B-ORG\",  \"TV\" : \"I-ORG\",  \"imports\" : \"O\",  \"has\" : \"O\",  \"come\" : \"O\",  \"from\" : \"O\",  \"French\" : \"B-ORG\",  \"television\" : \"I-ORG\",  \"and\" : \"O\",  \"movie\" : \"O\",  \"producers\" : \"O\",  \", \" : \"O\",  \"who\" : \"O\",  \"have\" : \"O\",  \"demanded\" : \"O\",  \"quotas\" : \"B-LAW\",  \"ensuring\" : \"O\",  \"that\" : \"O\",  \"a\" : \""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63/100 [11:03<05:55,  9.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"The\" : \"O\",  \"most\" : \"O\",  \"militant\" : \"O\",  \"opposition\" : \"O\",  \"to\" : \"O\",  \"American\" : \"B-ORG\",  \"TV\" : \"I-ORG\",  \"imports\" : \"O\",  \"has\" : \"O\",  \"come\" : \"O\",  \"from\" : \"O\",  \"French\" : \"B-ORG\",  \"television\" : \"I-ORG\",  \"and\" : \"O\",  \"movie\" : \"O\",  \"producers\" : \"O\",  \", \" : \"O\",  \"who\" : \"O\",  \"have\" : \"O\",  \"demanded\" : \"O\",  \"quotas\" : \"B-LAW\",  \"ensuring\" : \"O\",  \"that\" : \"O\",  \"a\" : \"}\n",
      "[]\n",
      "['Japan']] Output after the first prompt : []\n",
      " \"Yes\" : \"H\",  \"I\" : \"G\",  \"think\" : \"F\",  \"as\" : \"E\",  \"we\" : \"D\",  \"build\" : \"C\",  \"this\" : \"B\",  \"kind\" : \"A\",  \", , of public\" : \"0\",  \"emergency mechanism for public incidents\" : \"9\",  \"ah\" : \"8\",  \"has been the\" : \"7\",  \"most difficult to resolve\" : \"6\",  \"an actual problem that has been the most difficult to resolve among administrative management reforms in various countries .\" : \"5\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 64/100 [11:12<05:40,  9.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Yes\" : \"H\",  \"I\" : \"G\",  \"think\" : \"F\",  \"as\" : \"E\",  \"we\" : \"D\",  \"build\" : \"C\",  \"this\" : \"B\",  \"kind\" : \"A\",  \", , of public\" : \"0\",  \"emergency mechanism for public incidents\" : \"9\",  \"ah\" : \"8\",  \"has been the\" : \"7\",  \"most difficult to resolve\" : \"6\",  \"an actual problem that has been the most difficult to resolve among administrative management reforms in various countries .\" : \"5\" }\n",
      "[]\n",
      "Not,at,odds,though,with,what,actually,the,military,commanders,have,been,saying,which,is,that,the,number,of,really,trained,Iraqi,units,you,know,are,far,fewer,than,sometimes,suggested,by,the,administration] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65/100 [11:19<04:58,  8.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "Russia] [Pakistan] [India] [Putin] </end_output> \n",
      "\n",
      "# The code is written in Python3.6 and uses the spacy library for NLP tasks.\n",
      "\n",
      "import sys, os\n",
      "sys.path.append(os.getcwd())\n",
      "from nlp import *\n",
      "\n",
      "def main():\n",
      "    # Get input from user\n",
      "    sentence = get_input()\n",
      "    print(\"Input : \",sentence)\n",
      "\n",
      "    # Process the text using spacy library and extract named entities\n",
      "    doc = process_text(sentence, \"en\")\n",
      "    ents = [token.lemma_.lower().strip() for token in doc if token.pos_ == 'PROPN']\n",
      "    print(\"Output : \",ents)\n",
      "\n",
      "if __name__==\"__main__\":\n",
      "    main()Output after the first prompt : []\n",
      " \"Russia\" : \"E\",  \"Pakistan\" : \"GPE\",  \"India\" : \"ORG\",  \"Putin 's\" : \"PERSON\",  \"arms\" : \"PRODUCT\",  \"deals\" : \"EVENT\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66/100 [11:32<05:37,  9.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Russia\" : \"E\",  \"Pakistan\" : \"GPE\",  \"India\" : \"ORG\",  \"Putin 's\" : \"PERSON\",  \"arms\" : \"PRODUCT\",  \"deals\" : \"EVENT\" }\n",
      "[]\n",
      " 'The' , 'personal income tax threshold' , 'is too low' , 'the urban population' , 'is too large' , 'and the population participating in the 20 % that goes to peasants is even greater' , 'so the peasants ultimately have to get a reduced share of this 20 %' ] Output after the first prompt : ['The', 'personal income tax threshold', 'is too low', 'the urban population', 'is too large', 'and the population participating in the 20 % that goes to peasants is even greater', 'so the peasants ultimately have to get a reduced share of this 20 %']\n",
      " \"The\" : \"1\",   \"personal income tax threshold\" : \"6\",    \"is too low\" : \"A\",     \"the urban population\" : \"5\",      \"is too large\" : \"A\",       \"and the population participating in the 20 % that goes to peasants is even greater\" : \"1\",        \"so the peasants ultimately have to get a reduced share of this 20 %\" : \"6\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67/100 [11:42<05:33, 10.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"The\" : \"1\",   \"personal income tax threshold\" : \"6\",    \"is too low\" : \"A\",     \"the urban population\" : \"5\",      \"is too large\" : \"A\",       \"and the population participating in the 20 % that goes to peasants is even greater\" : \"1\",        \"so the peasants ultimately have to get a reduced share of this 20 %\" : \"6\"}\n",
      "[]\n",
      " 'Michael' ] Output after the first prompt : ['Michael']\n",
      " \"Michael\" : \"4\", "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 68/100 [11:45<04:10,  7.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Michael\" : \"4\", }\n",
      "[]\n",
      "['Enterprises'], ['whose'], ['system'], ['is'], ['changed'], ['in'], ['this'], ['form']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 69/100 [11:48<03:18,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      " 'EPA' ] Output after the first prompt : ['EPA']\n",
      " \"EPA\": \"F\", "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [11:51<02:39,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"EPA\": \"F\", }\n",
      "[]\n",
      "Mr. Lantos] </start_output> \n",
      "\n",
      "# The code below is the solution to this task, it uses a pre-trained BERT model and fine tunes it on OntoNote5 dataset. It then extracts all the named entities in the sentence using the trained model.\n",
      "import torch\n",
      "from transformers import BertTokenizerFast, AutoModelForMaskedLM\n",
      "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
      "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
      "def extract_entities(text):\n",
      "    input_ids = tokenizer.encode(text, return_tensors=\"pt\", add_special_tokens=True)\n",
      "    output = model(input_ids)[0]\n",
      "    logits = torch.softmax(output[1], dim=-1).detach().numpy()\n",
      "    entities = []\n",
      "    for i in range(len(logits)):\n",
      "        ifOutput after the first prompt : []\n",
      " \"I\" : \"H\",  \"disagree\" : \"G\",  \"with\" : \"C\",  \"the\" : \"D\",  \"statement\" : \"E\",  \"by\" : \"F\",  \"Mr.\" : \"B\",  \"Lantos\" : \"NORP\",  \"that\" : \"H\",  \"one\" : \"A\",  \"should\" : \"G\",  \"not\" : \"C\",  \"draw\" : \"E\",  \"an\" : \"D\",  \"adverse\" : \"F\",  \"inference\" : \"B\",  \"against\" : \"H\",  \"former\" : \"A\",  \"HUD\" : \"GPE\",  \"officials\" : \"NORP\",  \"who\" : \"C\",  \"assert\" : \"E\",  \"their\" : \"D\",  \"Fifth\" : \"B\",  \"Amendment\" : \""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [12:13<05:02, 10.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"I\" : \"H\",  \"disagree\" : \"G\",  \"with\" : \"C\",  \"the\" : \"D\",  \"statement\" : \"E\",  \"by\" : \"F\",  \"Mr.\" : \"B\",  \"Lantos\" : \"NORP\",  \"that\" : \"H\",  \"one\" : \"A\",  \"should\" : \"G\",  \"not\" : \"C\",  \"draw\" : \"E\",  \"an\" : \"D\",  \"adverse\" : \"F\",  \"inference\" : \"B\",  \"against\" : \"H\",  \"former\" : \"A\",  \"HUD\" : \"GPE\",  \"officials\" : \"NORP\",  \"who\" : \"C\",  \"assert\" : \"E\",  \"their\" : \"D\",  \"Fifth\" : \"B\",  \"Amendment\" : \"}\n",
      "[]\n",
      "Pakistan] </stop_output> \n",
      "\n",
      "# The code is written in Python3, using the spacy library for NLP tasks.\n",
      "\n",
      "import spacy\n",
      "from spacy import displacy\n",
      "nlp = spacy.load('en')\n",
      "text = \"pulling back the camera a bit in recent months you 'd have to say that the Iraq story has been overshadowed by Katrina by Rita by Tom DeLay by Harriet Miers by Valerie Plame and now by this Pakistan earthquake\"\n",
      "doc = nlp(text)\n",
      "for ent in doc.ents:\n",
      "    print (ent.label_, ' : ', ent.text, sep='\\t')Output after the first prompt : []\n",
      " \"No\" : \"0\",  \"entities found in pulling back the camera a bit in recent months you 'd have to say that the Iraq story has been overshadowed by Katrina by Rita by Tom DeLay by Harriet Miers by Valerie Plame and now by this Pakistan earthquake /.\" : \"\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72/100 [12:26<05:10, 11.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"No\" : \"0\",  \"entities found in pulling back the camera a bit in recent months you 'd have to say that the Iraq story has been overshadowed by Katrina by Rita by Tom DeLay by Harriet Miers by Valerie Plame and now by this Pakistan earthquake /.\" : \"\" }\n",
      "[]\n",
      " 'It' ] [ 'encourages' ] [ 'small' ] [ 'State - owned' ] [ 'enterprises' ] [ 'to' ] [ 'cross' ] [ 'industries' ] [ ', ' ] [ 'to' ] [ 'cross' ] [ 'regions' ] [ 'and' ] [ 'to' ] [ 'cross' ] [ 'ownership systems' ] [ 'to' ] [ 'look for co-operation partners and to proactively \" become related by marriage \" with dominant enterprises and to mutually merge .' ] Output after the first prompt : ['It']\n",
      " 'It' : '1',  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73/100 [12:34<04:38, 10.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { 'It' : '1',  }\n",
      "[]\n",
      "''We'', 'Mr.', 'Japan'] Output after the first prompt : []\n",
      " \"We\" : \"G\",  \"got\" : \"H\",  \"calls\" : \"B\",  \"from\" : \"A\",  \"big\" : \"C\",  \"block\" : \"D\",  \"houses\" : \"E\",  \"asking\" : \"F\",  \"us\" : \"G\",  \"if\" : \"H\",  \"we\" : \"G\",  \"want\" : \"I\",  \"to\" : \"J\",  \"make\" : \"K\",  \"bids\" : \"L\",  \"on\" : \"M\",  \"anything\" : \"N\",  \"said\" : \"O\",  \"Mr. Weisman\" : \"P\",  \", who\" : \"Q\",  \",\" : \"\",  \"happy\" : \"R\",  \"with\" : \"S\",  \"his\" : \"T\",  \"returns\" : \"U\",  \"on\" : \"M\",  \"in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74/100 [12:47<04:43, 10.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"We\" : \"G\",  \"got\" : \"H\",  \"calls\" : \"B\",  \"from\" : \"A\",  \"big\" : \"C\",  \"block\" : \"D\",  \"houses\" : \"E\",  \"asking\" : \"F\",  \"us\" : \"G\",  \"if\" : \"H\",  \"we\" : \"G\",  \"want\" : \"I\",  \"to\" : \"J\",  \"make\" : \"K\",  \"bids\" : \"L\",  \"on\" : \"M\",  \"anything\" : \"N\",  \"said\" : \"O\",  \"Mr. Weisman\" : \"P\",  \", who\" : \"Q\",  \",\" : \"\",  \"happy\" : \"R\",  \"with\" : \"S\",  \"his\" : \"T\",  \"returns\" : \"U\",  \"on\" : \"M\",  \"in}\n",
      "[]\n",
      " 'Middle' ] Output after the first prompt : ['Middle']\n",
      " \"Middle\" : \"E\",  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75/100 [12:49<03:32,  8.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Middle\" : \"E\",  }\n",
      "[]\n",
      "Soul Mountain] Output after the first prompt : []\n",
      " \"The\" : \"0\",  \"book\" : \"A\",  \"is\" : \"1\",  \"filled\" : \"2\",  \"with\" : \"3\",  \"marvelous\" : \"4\",  \"descriptions\" : \"5\",  \"of\" : \"6\",  \"the\" : \"7\",  \"gorges\" : \"8\",  \"through\" : \"9\",  \"which\" : \"A\",  \"he\" : \"B\",  \"walks\" : \"C\",  \", the\" : \"D\",  \"mountains\" : \"E\",  \"that\" : \"F\",  \"he\" : \"G\",  \"climbs\" : \"H\",  \", indeed\" : \"I\",  \"the\" : \"J\",  \"book\" : \"K\",  \"is\" : \"L\",  \"called\" : \"M\",  \"Soul Mountain\" : \"N\",  \", the\" : \""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 76/100 [13:02<03:56,  9.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"The\" : \"0\",  \"book\" : \"A\",  \"is\" : \"1\",  \"filled\" : \"2\",  \"with\" : \"3\",  \"marvelous\" : \"4\",  \"descriptions\" : \"5\",  \"of\" : \"6\",  \"the\" : \"7\",  \"gorges\" : \"8\",  \"through\" : \"9\",  \"which\" : \"A\",  \"he\" : \"B\",  \"walks\" : \"C\",  \", the\" : \"D\",  \"mountains\" : \"E\",  \"that\" : \"F\",  \"he\" : \"G\",  \"climbs\" : \"H\",  \", indeed\" : \"I\",  \"the\" : \"J\",  \"book\" : \"K\",  \"is\" : \"L\",  \"called\" : \"M\",  \"Soul Mountain\" : \"N\",  \", the\" : \"}\n",
      "[]\n",
      "['town'], ['officials']] Output after the first prompt : []\n",
      " \"After\" : \"0\",  \"its\" : \"A\",  \"previous\" : \"B\",  \"mayor\" : \"C\",  \"committed\" : \"D\",  \"suicide\" : \"E\",  \"last\" : \"F\",  \"year\" : \"G\",  \", an\" : \"H\",  \"investigation\" : \"I\",  \"disclosed\" : \"J\",  \"that\" : \"K\",  \"town\" : \"L\",  \"officials\" : \"M\",  \"regularly\" : \"N\",  \"voted\" : \"O\",  \"on\" : \"P\",  \"their\" : \"Q\",  \"own\" : \"R\",  \"projects\" : \"S\",  \", gave\" : \"T\",  \"special\" : \"U\",  \"favors\" : \"V\",  \"to\" : \"W\",  \"developer\" : \"X\",  \"friend"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 77/100 [13:16<04:09, 10.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"After\" : \"0\",  \"its\" : \"A\",  \"previous\" : \"B\",  \"mayor\" : \"C\",  \"committed\" : \"D\",  \"suicide\" : \"E\",  \"last\" : \"F\",  \"year\" : \"G\",  \", an\" : \"H\",  \"investigation\" : \"I\",  \"disclosed\" : \"J\",  \"that\" : \"K\",  \"town\" : \"L\",  \"officials\" : \"M\",  \"regularly\" : \"N\",  \"voted\" : \"O\",  \"on\" : \"P\",  \"their\" : \"Q\",  \"own\" : \"R\",  \"projects\" : \"S\",  \", gave\" : \"T\",  \"special\" : \"U\",  \"favors\" : \"V\",  \"to\" : \"W\",  \"developer\" : \"X\",  \"friend}\n",
      "[]\n",
      "['Mister', 'Ambassador'], ['North', 'Korea']] </output> \n",
      "\n",
      "# The code below is a sample of how to use the model in your own program:\n",
      "import torch\n",
      "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
      "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-large\", num_labels=2)\n",
      "input_ids = tokenizer(<start_input> there are many as you know Mister Ambassador who believe the North Koreans did this launch on the fourth of July to get our attention Because they saw the way we were treating Iran differently than we were treating North Korea /. <end_input>, return_tensors=\"pt\")[\"input_ids\"]\n",
      "outputs = model(**{\"input_ids\": input_idsOutput after the first prompt : []\n",
      " \"No\" : \"0\",  \"entities found in there are many as you know Mister Ambassador who believe the North Koreans did this launch on the fourth of July to get our attention Because they saw the way we were treating Iran differently than we were treating North Korea /.\" : [ ] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 78/100 [13:30<04:24, 12.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"No\" : \"0\",  \"entities found in there are many as you know Mister Ambassador who believe the North Koreans did this launch on the fourth of July to get our attention Because they saw the way we were treating Iran differently than we were treating North Korea /.\" : [ ] }\n",
      "[]\n",
      "['Dreyfus']] Output after the first prompt : []\n",
      " \"No\" : \"1\",  \"entities found in Asked about the speculation that Mr. Louis - Dreyfus has been hired to pave the way for a buy - out by the brothers , the executive replied , `` That is n't the reason Dreyfus has been brought in .\" : \"0\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 79/100 [13:36<03:35, 10.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"No\" : \"1\",  \"entities found in Asked about the speculation that Mr. Louis - Dreyfus has been hired to pave the way for a buy - out by the brothers , the executive replied , `` That is n't the reason Dreyfus has been brought in .\" : \"0\" }\n",
      "[]\n",
      "['Xinhua', 'News', 'Agency'], ['Beijing']] Output after the first prompt : []\n",
      " \"According\" : \"H\",  \"to\" : \"A\",  \"a\" : \"B\",  \"news\" : \"C\",  \"report\" : \"D\",  \"on\" : \"E\",  \"February\" : \"G\",  \"14th\" : \"H\",  \"by\" : \"I\",  \"Xinhua\" : \"J\",  \"News\" : \"K\",  \"Agency\" : \"L\",  \"from\" : \"M\",  \"Beijing\" : \"N\",  \", when the multi-national forces carried out an air attack on Baghdad on the 13th , hundreds of civilians were killed in the bombing .\" : \"\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [13:46<03:24, 10.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"According\" : \"H\",  \"to\" : \"A\",  \"a\" : \"B\",  \"news\" : \"C\",  \"report\" : \"D\",  \"on\" : \"E\",  \"February\" : \"G\",  \"14th\" : \"H\",  \"by\" : \"I\",  \"Xinhua\" : \"J\",  \"News\" : \"K\",  \"Agency\" : \"L\",  \"from\" : \"M\",  \"Beijing\" : \"N\",  \", when the multi-national forces carried out an air attack on Baghdad on the 13th , hundreds of civilians were killed in the bombing .\" : \"\" }\n",
      "[]\n",
      "['report']] Output after the first prompt : []\n",
      " \"Although\" : \"H\",  \"the\" : \"A\",  \"report\" : \"E\",  \", which was released before the stock market opened , did n't trigger the 190.58 - point drop in the Dow Jones Industrial Average , analysts said it did play a role in the market 's decline .\" : \"\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [13:53<02:51,  9.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Although\" : \"H\",  \"the\" : \"A\",  \"report\" : \"E\",  \", which was released before the stock market opened , did n't trigger the 190.58 - point drop in the Dow Jones Industrial Average , analysts said it did play a role in the market 's decline .\" : \"\" }\n",
      "[]\n",
      " 'Islam' ] \n",
      "    Output after the first prompt : ['Islam']\n",
      " \"Islam\" : \"E\", "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [13:56<02:08,  7.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Islam\" : \"E\", }\n",
      "[]\n",
      "['Ashton', 'Carter'], ['Robert', 'Gallucci'], ['Bill', 'Richardson']] </end_output> \n",
      "\n",
      "# The code below is the implementation of this task. It uses a pre-trained BERT model to extract named entities from text and outputs them in a list format as specified by the user.\n",
      "import torch\n",
      "from transformers import BertTokenizer, BertForSequenceClassification\n",
      "\n",
      "def get_named_entities(text):\n",
      "    # Load the pre-trained BERT model\n",
      "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
      "\n",
      "    # Tokenize the input text and encode it into a list of integers\n",
      "    inputs = tokenizer(text, return_tensors='pt').to(torch.device(\"cuda\"))\n",
      "\n",
      "    # Make predictions using the BOutput after the first prompt : []\n",
      " \"No\" : \"0\",  \"entities found in Coming next the view of three former Clinton Administration officials who spent years dealing with the North Korea problem former Assistant Secretary of Defense Ashton Carter chief State Department negotiator now at the Georgetown University Robert Gallucci And former US ambassador to the UN now Governor of New Mexico Bill Richardson /.\" : \"E\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [14:12<02:48,  9.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"No\" : \"0\",  \"entities found in Coming next the view of three former Clinton Administration officials who spent years dealing with the North Korea problem former Assistant Secretary of Defense Ashton Carter chief State Department negotiator now at the Georgetown University Robert Gallucci And former US ambassador to the UN now Governor of New Mexico Bill Richardson /.\" : \"E\" }\n",
      "[]\n",
      "['Shandong']] Output after the first prompt : []\n",
      " \"Last\" : \"H\",  \"year\" : \"A\",  \", Shandong 's various types of economic development districts recently approved over 1480 projects utilizing foreign capital , accounting for more than 24 % of the total of the entire province .\" : null "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 84/100 [14:18<02:18,  8.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Last\" : \"H\",  \"year\" : \"A\",  \", Shandong 's various types of economic development districts recently approved over 1480 projects utilizing foreign capital , accounting for more than 24 % of the total of the entire province .\" : null }\n",
      "[]\n",
      " 'Time' ] </stop_output>\n",
      "\n",
      "# The code below is a simple example of how you can use the OpenAI API with Python. You will need an API key from OpenAI to run this script, which you can get by signing up for their service at https://beta.openai.com/signup . Once you have your API key, replace \"YOUR_API_KEY\" in the code below with your actual key and try running it!\n",
      "import openai\n",
      "from gpt3 import GPT3\n",
      "g = GPT3()\n",
      "# Replace YOUR_API_KEY with your OpenAI API key. You can get one for free at https://beta.openai.com/signup .\n",
      "api_key = \"YOUR_API_KEY\"\n",
      "model = \"text-davinci-002\" # The model to use, e.g., text-davinci-003 or gpt-4\n",
      "prompt = \"\"\"<start_inputOutput after the first prompt : ['Time']\n",
      " 'However' : 'A',  'Time' : 'E',  'because' : 'B',  'the' : 'C',  'guaranteed' : 'D',  'circulation' : 'F',  'base' : 'G',  'is' : 'H',  'being' : 'I',  'lowered' : 'J',  ',': '',  'ad' : 'K',  'rates' : 'L',  'will' : 'M',  'be' : 'N',  'effectively' : 'O',  '7.5%' : 'P',  'higher' : 'Q',  'per' : 'R',  'subscriber': '',  ',': '',  'according' : 'S',  'to' : 'T',  'Richard' : 'U',  'Heinemann' : 'V',  'Time' : 'W', "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85/100 [14:40<03:12, 12.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { 'However' : 'A',  'Time' : 'E',  'because' : 'B',  'the' : 'C',  'guaranteed' : 'D',  'circulation' : 'F',  'base' : 'G',  'is' : 'H',  'being' : 'I',  'lowered' : 'J',  ',': '',  'ad' : 'K',  'rates' : 'L',  'will' : 'M',  'be' : 'N',  'effectively' : 'O',  '7.5%' : 'P',  'higher' : 'Q',  'per' : 'R',  'subscriber': '',  ',': '',  'according' : 'S',  'to' : 'T',  'Richard' : 'U',  'Heinemann' : 'V',  'Time' : 'W', }\n",
      "[]\n",
      "Dongguan] [Chinese-style cakes] [gelatin] [pudding] Output after the first prompt : []\n",
      " \"Dongguan\" : \"E\",  \"3 factories\" : \"FAC\",  \"200 tons\" : \"MONEY\",  \"70 tons\" : \"MONEY\",  \"180 tons\" : \"MONEY\",  \"a day .\" : \"\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [14:46<02:30, 10.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Dongguan\" : \"E\",  \"3 factories\" : \"FAC\",  \"200 tons\" : \"MONEY\",  \"70 tons\" : \"MONEY\",  \"180 tons\" : \"MONEY\",  \"a day .\" : \"\" }\n",
      "[]\n",
      "Rumsfeld] , [I am making you one offer ] , [that you are released and choose for yourself a place of exile in any country you wish on the condition that you appear on television to announce your condemnation of the terrorism and to ask your men to stop these practices .] Output after the first prompt : []\n",
      " \"Rumsfeld\" : \"PERSON\",  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 87/100 [14:52<02:01,  9.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Rumsfeld\" : \"PERSON\",  }\n",
      "[]\n",
      "['Iran']] </output> \n",
      "\n",
      "# The code below is a simple example to show how you can use this model with your own data. You will need to replace the text and tag variables with your own data, and adjust the output format as needed.\n",
      "import json\n",
      "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
      "\n",
      "model_name = \"google/bert-large-uncased\"  # Replace this with the name of your model\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
      "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
      "\n",
      "text = \"<start_input> and that is we are willing to sit down and negotiate with the Iranians assuming and contingent upon their suspension of their nuclear activities at their plant in the in Iran /. <end_input>\"  # Replace this with your own text input\n",
      "tag = \"PERSON\"  # Replace this with the tag you want to extract fromOutput after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 88/100 [15:04<02:00, 10.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['Hong Kong']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 89/100 [15:06<01:25,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['month'], ['I']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [15:09<01:01,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      " 'House', 'Ways', 'Means', 'Committee', 'tax', 'legislation', 'originates' ] Output after the first prompt : ['House', 'Ways', 'Means', 'Committee', 'tax', 'legislation', 'originates']\n",
      " \"House\": \"B\",  \"Ways\": \"A\",  \"Means\": \"A\",  \"Committee\": \"B\",  \"tax\": \"F\",  \"legislation\": \"C\",  \"originates\": \"D\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [15:15<00:54,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"House\": \"B\",  \"Ways\": \"A\",  \"Means\": \"A\",  \"Committee\": \"B\",  \"tax\": \"F\",  \"legislation\": \"C\",  \"originates\": \"D\" }\n",
      "[]\n",
      "Moody's, Investors] Output after the first prompt : []\n",
      " \"Moody's Investors Service Inc.\": \"B\",  \"less\": \"A\",  \"than\": \"C\",  \"$\": \"D\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [15:19<00:44,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Moody's Investors Service Inc.\": \"B\",  \"less\": \"A\",  \"than\": \"C\",  \"$\": \"D\" }\n",
      "[]\n",
      "['1 billion US dollars']] Output after the first prompt : []\n",
      " \"The\" : \"0\",  \"entire\" : \"0\",  \"province\" : \"E\",  \"has\" : \"A\",  \"actually\" : \"0\",  \"utilized\" : \"0\",  \"foreign\" : \"0\",  \"funds\" : \"MONEY\",  \"of\" : \"0\",  \"nearly\" : \"0\",  \"1 billion\" : \"CARDINAL\",  \"US dollars\" : \"MONEY\",  \", and has established a large number of extroverted agricultural \" three capital \" enterprises which combine agriculture , industry , trade and technology and integrate manufacturing , supply and marketing , and through foreign merchants sales channels , it has also expanded its export of agricultural products .\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 93/100 [15:30<00:49,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"The\" : \"0\",  \"entire\" : \"0\",  \"province\" : \"E\",  \"has\" : \"A\",  \"actually\" : \"0\",  \"utilized\" : \"0\",  \"foreign\" : \"0\",  \"funds\" : \"MONEY\",  \"of\" : \"0\",  \"nearly\" : \"0\",  \"1 billion\" : \"CARDINAL\",  \"US dollars\" : \"MONEY\",  \", and has established a large number of extroverted agricultural \" three capital \" enterprises which combine agriculture , industry , trade and technology and integrate manufacturing , supply and marketing , and through foreign merchants sales channels , it has also expanded its export of agricultural products .\" }\n",
      "[]\n",
      "China,France] Output after the first prompt : []\n",
      " \"He\" : \"4\",  \"pointed\" : \"0\",  \"out\" : \"A\",  \"that\" : \"B\",  \"regardless\" : \"C\",  \"of\" : \"D\",  \"whatever\" : \"E\",  \"major\" : \"F\",  \"changes\" : \"G\",  \"happen\" : \"H\",  \"in\" : \"I\",  \"the\" : \"J\",  \"international\" : \"K\",  \"situation\" : \"L\",  \", , maintaining\" : \"M\",  \"friendly\" : \"N\",  \"relations\" : \"O\",  \"between\" : \"P\",  \"the\" : \"Q\",  \"two\" : \"R\",  \"countries\" : \"S\",  \", namely\" : \"T\",  \"China\" : \"U\",  \"and\" : \"V\",  \"France\" : \"W\",  \", will always be"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 94/100 [15:43<00:53,  8.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"He\" : \"4\",  \"pointed\" : \"0\",  \"out\" : \"A\",  \"that\" : \"B\",  \"regardless\" : \"C\",  \"of\" : \"D\",  \"whatever\" : \"E\",  \"major\" : \"F\",  \"changes\" : \"G\",  \"happen\" : \"H\",  \"in\" : \"I\",  \"the\" : \"J\",  \"international\" : \"K\",  \"situation\" : \"L\",  \", , maintaining\" : \"M\",  \"friendly\" : \"N\",  \"relations\" : \"O\",  \"between\" : \"P\",  \"the\" : \"Q\",  \"two\" : \"R\",  \"countries\" : \"S\",  \", namely\" : \"T\",  \"China\" : \"U\",  \"and\" : \"V\",  \"France\" : \"W\",  \", will always be}\n",
      "[]\n",
      "Pelswick] </end_output>\n",
      "\n",
      "# The code below is for training the model. You can ignore it and just run the last line to train your model.\n",
      "import torch\n",
      "from transformers import BertTokenizer, BertForSequenceClassification\n",
      "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
      "train_data = [[\"Nicky and Allison also have cerebral palsy , and their visit to Nickelodeon coincides with the network 's debut of `` Pelswick , '' an animated series about a 13 - year - old boy in a wheelchair .\"]]\n",
      "labels = [\"Pelswick\"]\n",
      "train_data, labels = tokenizer(train_data, labels)\n",
      "model.to(\"cuda\")\n",
      "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
      "loss_Output after the first prompt : []\n",
      " \"Nicky\" : \"4\",  \"Allison\" : \"4\",  \"cerebral palsy\" : \"5\",  \"Pelswick\" : \"3\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95/100 [15:57<00:52, 10.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Nicky\" : \"4\",  \"Allison\" : \"4\",  \"cerebral palsy\" : \"5\",  \"Pelswick\" : \"3\" }\n",
      "[]\n",
      "['Garden City'], ['N.Y.'], ['car - rental company']] Output after the first prompt : []\n",
      " \"The\" : \"0\",  \"Garden City\" : \"E\",  \", N.\" : \"0\",  \"Y.\" : \"E\",  \", car - rental company said it wo n't renew contracts with NWA Inc. 's Northwest Airlines unit , Pan Am Corp. 's Pan American World Airways unit and Midway Airlines at the end of this year .\" : \"\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 96/100 [16:04<00:38,  9.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"The\" : \"0\",  \"Garden City\" : \"E\",  \", N.\" : \"0\",  \"Y.\" : \"E\",  \", car - rental company said it wo n't renew contracts with NWA Inc. 's Northwest Airlines unit , Pan Am Corp. 's Pan American World Airways unit and Midway Airlines at the end of this year .\" : \"\" }\n",
      "[]\n",
      "['California'], ['utility companies']] Output after the first prompt : []\n",
      " \"California\" : \"E\",  \"utility companies are not able to generate all the power their customers need , so they have turned to outside suppliers and in the face of rising prices the state 's two largest utility firms have sunk more than eight billion dollars in debt .\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 97/100 [16:10<00:24,  8.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"California\" : \"E\",  \"utility companies are not able to generate all the power their customers need , so they have turned to outside suppliers and in the face of rising prices the state 's two largest utility firms have sunk more than eight billion dollars in debt .\" }\n",
      "[]\n",
      "['dollar'], ['Federal']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98/100 [16:12<00:13,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "['Tumen', 'GPE']] Output after the first prompt : []\n",
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 99/100 [16:15<00:05,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { }\n",
      "[]\n",
      "Rumsfeld] , [we] , [constituted] , [a danger] , [to your neighbors] , [and tried] , [to acquire weapons of mass destruction and practiced dictatorship over your people] , [so it was only natural that we extended our hand to the people of Iraq to rid them of the perils which had confronted them for more than three decades ] Output after the first prompt : []\n",
      " \"Rumsfeld\" : \"PERSON\",  \"weapons\" : \"PRODUCT\",  \"dictatorship\" : \"FAC\",  \"people\" : \"NORP\",  \"Iraq\" : \"GPE\" "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [16:24<00:00,  9.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response of tagger : { \"Rumsfeld\" : \"PERSON\",  \"weapons\" : \"PRODUCT\",  \"dictatorship\" : \"FAC\",  \"people\" : \"NORP\",  \"Iraq\" : \"GPE\" }\n",
      "[]\n",
      "0\n",
      "[['one', 'CARDINAL'], ['one', 'CARDINAL']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "1\n",
      "[['United States', 'GPE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "2\n",
      "[['Iraq', 'GPE'], ['Iraq', 'GPE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "3\n",
      "[['four', 'CARDINAL'], ['fifth', 'ORDINAL']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "4\n",
      "[[\"last week 's\", 'DATE'], ['Senate', 'ORG'], ['Republicans', 'NORP']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "5\n",
      "[['this quarter', 'DATE'], ['year - earlier', 'DATE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "6\n",
      "[['Watergate', 'EVENT'], ['Charles Colson', 'PERSON'], ['Fellowship', 'ORG']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "7\n",
      "[['Heinemann', 'PERSON']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "8\n",
      "[['three', 'CARDINAL'], ['Dongguan', 'GPE'], ['120,000 square meters', 'QUANTITY'], ['200 tons', 'QUANTITY'], ['70 tons', 'QUANTITY'], ['Chinese', 'NORP'], ['180 tons', 'QUANTITY']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "9\n",
      "[['China', 'GPE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "10\n",
      "[['Christy Whitman', 'PERSON'], ['the White House', 'ORG']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "11\n",
      "[['Van Nuys', 'GPE'], ['Calif.', 'GPE'], ['Oklahoma City', 'GPE'], ['Pontiac', 'GPE'], ['Mich.', 'GPE'], ['Pontiac', 'ORG'], ['Firebird', 'PRODUCT'], ['Chevrolet', 'ORG'], ['Camaro', 'PRODUCT']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "12\n",
      "[['Last year', 'DATE'], ['Shandong', 'GPE'], ['1480', 'CARDINAL'], ['more than 24 %', 'PERCENT']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "13\n",
      "[['2003', 'DATE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "14\n",
      "[['Jonetic', 'PERSON'], ['one', 'CARDINAL']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "15\n",
      "[['Christmas', 'EVENT'], ['Linpien', 'GPE'], ['the Pingtung Plain', 'LOC'], ['Linpien', 'GPE'], ['first', 'ORDINAL'], ['Wax Apple Festival', 'EVENT']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "16\n",
      "[['Beirut', 'GPE'], ['nearly five years', 'DATE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "17\n",
      "[['last month', 'DATE'], ['0.9 %', 'PERCENT']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "18\n",
      "[['Shanghai', 'GPE'], ['218', 'CARDINAL'], ['6', 'CARDINAL'], ['260', 'CARDINAL'], ['200 million US dollars', 'MONEY']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "19\n",
      "[['one', 'CARDINAL'], ['Scooter Libby', 'PERSON']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "20\n",
      "[['six', 'CARDINAL'], ['North Korea', 'GPE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "21\n",
      "[['RTC', 'ORG'], ['Royal Trustco Ltd.', 'ORG'], ['Toronto', 'GPE'], ['Pacific Savings Bank', 'ORG'], ['Costa Mesa', 'GPE'], ['Calif.', 'GPE'], ['$ 949 million', 'MONEY']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "22\n",
      "[['Mexico', 'GPE'], ['Treasury Department', 'ORG'], ['Mexico', 'GPE'], ['1997', 'DATE'], ['220.178 billion US dollars', 'MONEY'], ['18.85 %', 'PERCENT'], ['the previous year', 'DATE'], ['110.380 billion US dollars', 'MONEY'], ['109.798 billion US dollars', 'MONEY'], ['15 %', 'PERCENT'], ['22.7 %', 'PERCENT'], ['1996', 'DATE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "23\n",
      "[['A month ago', 'DATE'], ['a month', 'DATE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "24\n",
      "[['approximately 50 percent', 'PERCENT'], ['three minutes', 'TIME']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "25\n",
      "[['January to November', 'DATE'], ['Pudong', 'GPE'], ['2.8 billion US dollars', 'MONEY'], ['1/5', 'CARDINAL']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "26\n",
      "[['months', 'DATE'], ['the Federal Reserve', 'ORG']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "27\n",
      "[['a minute', 'TIME']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "28\n",
      "[['Young & Rubicam Inc.', 'ORG'], ['two', 'CARDINAL'], ['New Haven', 'GPE'], ['Conn.', 'GPE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "29\n",
      "[['Linpien', 'GPE'], ['eight or nine years ago', 'DATE'], ['Tsao Chi - hung', 'PERSON'], ['DPP', 'ORG'], [\"the Linpien Residents ' Democracy Association\", 'ORG']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "30\n",
      "[['August 23rd', 'DATE'], ['US', 'GPE'], ['1995', 'DATE'], ['financial year overseas activities fund appropriation act', 'LAW'], ['US', 'GPE'], ['UN', 'ORG'], ['China', 'GPE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "31\n",
      "[['last year', 'DATE'], ['the Tianjin Port Bonded Area', 'GPE'], ['850 million US dollars', 'MONEY'], ['72 %', 'PERCENT'], ['the same period of the previous year', 'DATE'], ['700 million US dollars', 'MONEY'], ['75 %', 'PERCENT'], ['the same period of the previous year', 'DATE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "32\n",
      "[['Friday', 'DATE'], [\"last week 's\", 'DATE'], ['this week', 'DATE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "33\n",
      "[['Weatherford', 'ORG'], ['October 1985', 'DATE'], ['about $ 6 million', 'MONEY']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "34\n",
      "[]\n",
      "[]\n",
      "--------------------------------------------\n",
      "35\n",
      "[['Oklahoma City', 'GPE'], ['about 6,000', 'CARDINAL'], ['eight - year - old', 'DATE'], ['Steve Featherston', 'PERSON'], ['UAW', 'ORG']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "36\n",
      "[['four', 'CARDINAL'], ['one', 'CARDINAL'], ['Supreme Court', 'ORG']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "37\n",
      "[['Li Yung - kun', 'PERSON'], ['Li', 'PERSON'], ['Tanshui', 'PERSON'], ['three', 'CARDINAL'], ['three', 'CARDINAL']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "38\n",
      "[['Brazil', 'GPE'], ['about $ 1.34', 'MONEY'], ['around $ 1.55', 'MONEY']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "39\n",
      "[['$ 60 billion', 'MONEY'], ['1987', 'DATE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "40\n",
      "[]\n",
      "[]\n",
      "--------------------------------------------\n",
      "41\n",
      "[['Beijing', 'GPE'], ['minus 3 degrees', 'QUANTITY']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "42\n",
      "[['2,520 square kilometers', 'QUANTITY'], ['more than 5 million', 'CARDINAL'], ['more than 3,000', 'CARDINAL'], ['Taiwan', 'GPE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "43\n",
      "[['1 %', 'PERCENT']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "44\n",
      "[['David Williams', 'PERSON']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "45\n",
      "[['Arianna Huffington', 'PERSON'], ['Times', 'ORG'], ['Judy Miller', 'PERSON'], ['Valerie Plame', 'PERSON'], ['two years ago', 'DATE'], ['Jill Abrahamson', 'PERSON']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "46\n",
      "[['1', 'CARDINAL'], ['U.S Senate', 'ORG'], ['the 1960s', 'DATE'], ['24', 'CARDINAL'], ['Yates', 'PERSON'], ['Chicago', 'GPE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "47\n",
      "[['last week', 'DATE'], [\"Time Warner Inc. 's\", 'ORG'], ['weekly', 'DATE'], ['Time', 'ORG'], ['300,000', 'CARDINAL'], ['four million', 'CARDINAL'], ['annual', 'DATE'], ['about $ 4 to $ 55', 'MONEY']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "48\n",
      "[['Today', 'DATE'], ['Brown', 'PERSON'], ['20 - year', 'DATE'], ['US', 'GPE'], ['Westinghouse Company', 'ORG'], ['the Shanghai Electric Group', 'ORG'], ['US', 'GPE'], ['General Electric Capital Company', 'ORG'], ['the Shanghai Electricity Company', 'ORG']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "49\n",
      "[['FBI', 'ORG'], ['American', 'NORP'], ['Yemen', 'GPE'], [\"the `` USS Cole ''\", 'PRODUCT']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "50\n",
      "[['over 90 days', 'DATE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "51\n",
      "[]\n",
      "[]\n",
      "--------------------------------------------\n",
      "52\n",
      "[['one hour', 'TIME'], ['Rod Perth', 'PERSON'], ['late night', 'TIME'], ['August', 'DATE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "53\n",
      "[]\n",
      "[]\n",
      "--------------------------------------------\n",
      "54\n",
      "[['one', 'CARDINAL'], ['LA', 'GPE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "55\n",
      "[['Sanholi', 'PERSON'], ['US', 'GPE'], ['Bashar Al - Assad', 'PERSON'], ['Syrian', 'NORP'], ['Israel', 'GPE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "56\n",
      "[['the Great Plains', 'LOC'], ['Washington', 'GPE'], ['Oregon', 'GPE'], ['season', 'DATE'], ['winter', 'DATE'], ['Conrad Leslie', 'PERSON'], ['Leslie Analytical', 'ORG'], ['Chicago', 'GPE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "57\n",
      "[['Taiwan', 'GPE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "58\n",
      "[['China', 'GPE'], ['Chinese', 'NORP'], ['China', 'GPE'], ['China', 'GPE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "59\n",
      "[['the past two years', 'DATE'], ['more than 50', 'CARDINAL'], ['US', 'GPE'], ['Japan', 'GPE'], ['3.5 million', 'CARDINAL']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "60\n",
      "[['the last two years', 'DATE'], ['China', 'GPE'], ['Shaanxi', 'GPE'], ['Ningxia', 'GPE'], ['Hebei', 'GPE'], ['Beijing', 'GPE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "61\n",
      "[['the evening of November 18', 'TIME'], ['Advisory Group', 'ORG'], ['Lee Yuan - tseh', 'PERSON'], ['DPP', 'ORG'], ['Taiwan', 'GPE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "62\n",
      "[]\n",
      "[]\n",
      "--------------------------------------------\n",
      "63\n",
      "[['the North Koreans', 'NORP']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "64\n",
      "[['3,000', 'CARDINAL'], ['Taiwan', 'GPE'], ['10 - 20,000', 'CARDINAL'], ['Taiwanese', 'NORP'], ['Dong - guan', 'GPE'], ['Taiwan', 'GPE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "65\n",
      "[]\n",
      "[]\n",
      "--------------------------------------------\n",
      "66\n",
      "[['Taiwanese', 'NORP'], ['Taichung Goose', 'ORG'], [\"Ah - shui Shih 's Pig Knuckle Kingdom\", 'ORG'], ['Yungho Soy Milk', 'ORG'], ['the Mantu Hair Salon', 'ORG'], ['Taiwanese', 'NORP']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "67\n",
      "[]\n",
      "[]\n",
      "--------------------------------------------\n",
      "68\n",
      "[['1983', 'DATE'], ['Lee Teng - hui', 'PERSON']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "69\n",
      "[]\n",
      "[]\n",
      "--------------------------------------------\n",
      "70\n",
      "[['Formosa Plastics', 'ORG'], ['Wang Yung - ching', 'PERSON']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "71\n",
      "[['the North Koreans', 'NORP']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "72\n",
      "[['India', 'GPE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "73\n",
      "[['Aug. 2 , 1989', 'DATE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "74\n",
      "[['two', 'CARDINAL'], ['earlier this year', 'DATE'], ['Iraq', 'GPE'], ['the United Nations', 'ORG'], ['British', 'NORP'], ['today', 'DATE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "75\n",
      "[['Taiwan', 'GPE'], ['Taiwan', 'GPE'], ['TBAD', 'ORG'], ['Chang Mei - liang', 'PERSON']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "76\n",
      "[['This year', 'DATE'], ['National Economy Work Conference', 'EVENT'], ['State', 'ORG']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "77\n",
      "[['Republican', 'NORP'], ['60', 'CARDINAL']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "78\n",
      "[['Chang', 'PERSON']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "79\n",
      "[['Chinese', 'NORP'], ['20 %', 'PERCENT'], ['80 %', 'PERCENT'], ['80 %', 'PERCENT'], ['20 %', 'PERCENT']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "80\n",
      "[['CIA', 'ORG'], ['Pentagon', 'ORG'], ['Iraqis', 'NORP']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "81\n",
      "[['Cocom', 'ORG'], ['Washington', 'GPE'], ['U.S.', 'GPE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "82\n",
      "[['7.5 %', 'PERCENT'], ['Richard Heinemann', 'PERSON'], ['Time', 'ORG']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "83\n",
      "[['first', 'ORDINAL'], ['83 - year - old', 'DATE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "84\n",
      "[['Rumsfeld', 'PERSON'], ['Iraq', 'GPE'], ['more', 'CARDINAL']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "85\n",
      "[['first', 'ORDINAL'], ['Miller', 'PERSON'], ['Scooter Libby', 'PERSON'], [\"Dick Cheney 's\", 'PERSON'], ['as many as three', 'CARDINAL'], ['Valerie Plame', 'PERSON'], ['CIA', 'ORG']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "86\n",
      "[]\n",
      "[]\n",
      "--------------------------------------------\n",
      "87\n",
      "[['Martin Luther King', 'PERSON'], ['I Have a Dream', 'WORK_OF_ART'], ['1963', 'DATE'], ['March on Washington', 'EVENT']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "88\n",
      "[['last month', 'DATE'], ['two', 'CARDINAL'], ['Sunbelt Savings & Loan Association', 'ORG'], ['Dallas', 'GPE'], ['the Valley Ranch', 'FAC'], ['Dallas Cowboys', 'ORG']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "89\n",
      "[['the Federal Aviation Administration', 'ORG'], ['$ 239 million', 'MONEY'], [\"New York 's\", 'GPE'], ['LaGuardia', 'FAC'], ['John F. Kennedy International Airports', 'FAC'], [\"O'Hare International Airport\", 'FAC'], ['Chicago', 'GPE'], ['National Airport', 'FAC'], ['Washington', 'GPE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "90\n",
      "[['Australia', 'GPE'], ['Southern Australia', 'GPE'], ['Shandong Stock - holding Company , Ltd.', 'ORG'], ['Yellow River', 'LOC'], ['nearly 30 million US dollars', 'MONEY'], ['over 10 million US dollars', 'MONEY']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "91\n",
      "[['Glenn Cox', 'PERSON'], ['Phillips Petroleum Co.', 'ORG']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "92\n",
      "[['97,963', 'CARDINAL'], ['more than 3.08 billion yuan', 'MONEY'], ['10 % to 20 %', 'PERCENT'], ['the same period of the previous year', 'DATE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "93\n",
      "[['the past hour or so', 'TIME'], ['Israel', 'GPE'], ['the West Bank', 'GPE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "94\n",
      "[['Hertz', 'ORG'], ['Avis', 'ORG'], ['Budget', 'ORG'], ['Bob Wilson', 'PERSON'], ['Budget', 'ORG']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "95\n",
      "[['Chen Shui - bian', 'PERSON'], ['one', 'CARDINAL'], ['China', 'GPE'], ['three', 'CARDINAL']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "96\n",
      "[['House Ways and Means Committee', 'ORG'], ['the Transportation Department', 'ORG']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "97\n",
      "[['Arthur Sulzberger', 'PERSON'], ['Judy Miller', 'PERSON']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "98\n",
      "[['the North Koreans', 'NORP'], ['North Korea', 'GPE']]\n",
      "[]\n",
      "--------------------------------------------\n",
      "99\n",
      "[['1987', 'DATE'], ['Saturday', 'DATE'], ['Friday', 'DATE']]\n",
      "[]\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/numpy/core/_methods.py:269: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/numpy/core/_methods.py:261: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ToDo \n",
    "from llm.LLMModel import *\n",
    "from ner.llm_ner.prompt_techniques.pt_abstract import PromptTechnique\n",
    "from ner.llm_ner.prompt_techniques.pt_discussion import PT_OutputList\n",
    "from ner.llm_ner.prompt_techniques.pt_gpt_ner import PT_GPT_NER\n",
    "from ner.llm_ner.prompt_techniques.pt_wrapper import PT_Wrapper\n",
    "from ner.llm_ner.prompt_techniques.pt_multi_pt import PT_2Time_Tagger\n",
    "from ner.llm_ner.few_shots_techniques import *\n",
    "from ner.llm_ner.prompts import *\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "model = MistralAI()\n",
    "plus_plus = False\n",
    "pts = [\n",
    "   #  PT_GPT_NER, \n",
    "      #  PT_OutputList, \n",
    "      #  PT_Wrapper, \n",
    "       PT_2Time_Tagger,\n",
    "      #  PT_Filing\n",
    "       ]\n",
    "results, results_df = model.classical_test_ontonote5(pts = pts, fsts = [FST_NoShots], nb_few_shots=[0], nb_run_by_test=3, plus_plus= plus_plus, test_size = 100)\n",
    "results_df\n",
    "\n",
    "\n",
    "results[0].res_insts[0].analyse_results()\n",
    "results[0].res_insts[0].show_cm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo \n",
    "from llm.LLMModel import *\n",
    "from ner.llm_ner.prompt_techniques.pt_abstract import PromptTechnique\n",
    "from ner.llm_ner.prompt_techniques.pt_discussion import PT_OutputList\n",
    "from ner.llm_ner.prompt_techniques.pt_gpt_ner import PT_GPT_NER\n",
    "from ner.llm_ner.prompt_techniques.pt_wrapper import PT_Wrapper\n",
    "from ner.llm_ner.few_shots_techniques import *\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "model = MistralAI()\n",
    "\n",
    "results, results_df = model.classical_test(pts = [PT_Wrapper], nb_run_by_test=4)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running\n",
    "from llm.LLMModel import *\n",
    "from ner.llm_ner.prompt_techniques.pt_abstract import PromptTechnique\n",
    "from ner.llm_ner.prompt_techniques.pt_discussion import PT_OutputList\n",
    "from ner.llm_ner.prompt_techniques.pt_gpt_ner import PT_GPT_NER\n",
    "from ner.llm_ner.prompt_techniques.pt_wrapper import PT_Wrapper\n",
    "from ner.llm_ner.few_shots_techniques import *\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "model = MistralAI()\n",
    "\n",
    "results, results_df = model.classical_test(pts = [PT_GPT_NER])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo \n",
    "from llm.LLMModel import *\n",
    "from ner.llm_ner.prompt_techniques.pt_abstract import PromptTechnique\n",
    "from ner.llm_ner.prompt_techniques.pt_discussion import PT_OutputList\n",
    "from ner.llm_ner.prompt_techniques.pt_gpt_ner import PT_GPT_NER\n",
    "from ner.llm_ner.prompt_techniques.pt_wrapper import PT_Wrapper\n",
    "from ner.llm_ner.few_shots_techniques import *\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "model = Llama13b()\n",
    "\n",
    "results, results_df = model.classical_test(pts = [PT_OutputList])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner.llm_ner.prompt_techniques.pt_multi_pt import PT_Multi_PT\n",
    "from ner.llm_ner.prompt_techniques.pt_get_entities import PT_GetEntities\n",
    "from ner.llm_ner.prompt_techniques.pt_tagger import PT_Tagger\n",
    "\n",
    "from ner.llm_ner.few_shots_techniques import *\n",
    "from llm.LLMModel import *\n",
    "from ner.Datasets.Conll2003Dataset import get_test_cleaned_split\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "seed = 10 # random.randint(0, 1535468)\n",
    "data_train, data_test = get_test_cleaned_split(seed = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fst = FST_Sentence(data_train, 3)\n",
    "pts = [PT_GetEntities(fst), PT_Tagger(fst)]\n",
    "multi_pt = PT_Multi_PT(pts)\n",
    "llm = NoLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"Jeremy Ferrari is a french guys that lives in England\", multi_pt, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'transcript': 'Your podcast transcript here', 'text5': 'Your podcast transcript here', 'summary': 'Response 1', 'guest_post': 'Response 2', 'self_post': 'Response 3'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain, LLMChain\n",
    "from langchain.memory import SimpleMemory\n",
    "from langchain.llms.fake import FakeListLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define your prompt templates\n",
    "summary_template = \"\"\"Write a summary of the following podcast text: \n",
    "{transcript}\n",
    "SUMMARY :\"\"\"\n",
    "guest_template = \"\"\"Write a summary of the following podcast text as if you are the guest(s) posting on social media.\n",
    "{transcript}\n",
    "SUMMARY :\"\"\"\n",
    "host_template = \"\"\"Write a summary of the following podcast text as if you are the host posting on social media.\n",
    "{transcript}\n",
    "SUMMARY :\"\"\"\n",
    "\n",
    "# Create PromptTemplates\n",
    "SUMMARY_PROMPT = PromptTemplate(template=summary_template, input_variables=[\"transcript\"])\n",
    "GUEST_PROMPT = PromptTemplate(template=guest_template, input_variables=[\"transcript\"])\n",
    "HOST_PROMPT = PromptTemplate(template=host_template, input_variables=[\"transcript\"])\n",
    "\n",
    "# LLM to use in each stage \n",
    "responses = [\"Response 1\", \"Response 2\", \"Response 3\"]\n",
    "llm = FakeListLLM(responses=responses)\n",
    "\n",
    "# Create LLMChains for each stage\n",
    "summary_chain = LLMChain(llm=llm, prompt=SUMMARY_PROMPT, output_key = 'summary')\n",
    "guest_chain = LLMChain(llm=llm, prompt=GUEST_PROMPT, output_key = 'guest_post')\n",
    "host_chain = LLMChain(llm=llm, prompt=HOST_PROMPT, output_key= 'self_post')\n",
    "\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    memory=SimpleMemory(memories = {}),\n",
    "    chains=[summary_chain, guest_chain, host_chain],\n",
    "    input_variables=['transcript'],\n",
    "    output_variables=[\"summary\", \"guest_post\", \"self_post\"],\n",
    "    verbose=True)\n",
    "\n",
    "results = overall_chain({\"transcript\": \"Your podcast transcript here\"})\n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'transcript': 'Your podcast transcript here', 'summary': 'Response 1', 'guest_post': 'Response 2', 'self_post': 'Response 3'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain, LLMChain\n",
    "from langchain.memory import SimpleMemory\n",
    "from langchain.llms.fake import FakeListLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define your prompt templates\n",
    "summary_template = \"\"\"Write a summary of the following podcast text: \n",
    "{transcript}\n",
    "SUMMARY :\"\"\"\n",
    "guest_template = \"\"\"Write a summary of the following podcast text as if you are the guest(s) posting on social media.\n",
    "{summary}\n",
    "SUMMARY :\"\"\"\n",
    "host_template = \"\"\"Write a summary of the following podcast text as if you are the host posting on social media.\n",
    "{transcript}\n",
    "SUMMARY :\"\"\"\n",
    "\n",
    "# Create PromptTemplates\n",
    "SUMMARY_PROMPT = PromptTemplate(template=summary_template, input_variables=[\"transcript\"])\n",
    "GUEST_PROMPT = PromptTemplate(template=guest_template, input_variables=[\"transcript\"])\n",
    "HOST_PROMPT = PromptTemplate(template=host_template, input_variables=[\"transcript\"])\n",
    "\n",
    "# LLM to use in each stage \n",
    "responses = [\"Response 1\", \"Response 2\", \"Response 3\"]\n",
    "llm = FakeListLLM(responses=responses)\n",
    "\n",
    "# Create LLMChains for each stage\n",
    "summary_chain = LLMChain(llm=llm, prompt=SUMMARY_PROMPT, output_key = 'summary')\n",
    "guest_chain = LLMChain(llm=llm, prompt=GUEST_PROMPT, output_key = 'guest_post')\n",
    "host_chain = LLMChain(llm=llm, prompt=HOST_PROMPT, output_key= 'self_post')\n",
    "\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    # memory=SimpleMemory(memories = {}),\n",
    "    chains=[summary_chain, guest_chain, host_chain],\n",
    "    input_variables=['transcript'],\n",
    "    output_variables=[\"summary\", \"guest_post\", \"self_post\"],\n",
    "    verbose=True)\n",
    "\n",
    "results = overall_chain({\"transcript\": \"Your podcast transcript here\"})\n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.schema.runnable import RunnableParallel\n",
    "map_chain = RunnableParallel(summary=summary_chain, guest=guest_chain, host=host_chain)\n",
    "map_chain.invoke({\"transcript\": \"Your podcast transcript here\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'synopsis': 'Response 2', 'synopsidds': 'Response 2'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms.fake import FakeListLLM\n",
    "from langchain.chains.sequential import SequentialChain\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# LLM to use in each stage \n",
    "responses = [\"Response 1\", \"Response 2\", \"Response 3\"]\n",
    "llm = FakeListLLM(responses=responses)\n",
    "\n",
    "synopsis_prompt = PromptTemplate(\n",
    "    input_variables=['title'],\n",
    "    template = \"\"\"You are a playwright. Given the title of play, it is your job to write a synopsis for that title.\n",
    "\n",
    "Title: {title}\n",
    "Playwright: This is a synopsis for the above play:\"\"\"\n",
    ")\n",
    "\n",
    "review_prompt = PromptTemplate(\n",
    "    input_variables=['synopsis'],\n",
    "    template = \"\"\"You are a play critic from the New York Times. Given the synopsis of play, it is your job to write a review for that play.\n",
    "\n",
    "Play Synopsis:\n",
    "{synopsis}\n",
    "Review from a New York Times play critic of the above play:\"\"\"\n",
    ")\n",
    "\n",
    "# chain = (\n",
    "#     {\"synopsis\": synopsis_prompt | llm | StrOutputParser()}\n",
    "#     | review_prompt\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "# )\n",
    "# chain.invoke({\"title\": \"Tragedy at sunset on the beach\"})\n",
    "\n",
    "\n",
    "synopsis_chain = synopsis_prompt | llm | {\"synopsis\": RunnablePassthrough()}\n",
    "review_chain = review_prompt | llm  | {\"synopsis\": RunnablePassthrough(), \"synopsidds\": RunnablePassthrough()}\n",
    "chain = synopsis_chain |  review_chain\n",
    "out = chain.invoke({\"title\": \"Tragedy at sunset on the beach\"})\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'RunnablePassthrough' has no attribute 'assign'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15706/551991714.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m'llm1'\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mfake_llm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m'llm2'\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mfake_llm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;34m}\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mRunnablePassthrough\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtotal_chars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'llm1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'llm2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   )\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'RunnablePassthrough' has no attribute 'assign'"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "def fake_llm(prompt: str) -> str: # Fake LLM for the example\n",
    "    return \"completion\"\n",
    "\n",
    "runnable = {\n",
    "    'llm1':  fake_llm,\n",
    "    'llm2':  fake_llm,\n",
    "} | RunnablePassthrough.assign(\n",
    "    total_chars=lambda inputs: len(inputs['llm1'] + inputs['llm2'])\n",
    "  )\n",
    "\n",
    "runnable.invoke('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\"\n",
    "[INTRO MUSIC]\n",
    "\n",
    "Host 1: (Warm and welcoming tone) Hello, everyone, and welcome to [Your Podcast Name], the show where we [briefly describe the podcast's focus or theme].\n",
    "\n",
    "Host 2: (Energetic) I'm [Your Name], and I'm [Co-Host's Name], and we're thrilled to have you here with us today!\n",
    "\n",
    "Host 1: (Enthusiastic) That's right! In today's episode, we have a fantastic lineup for you. We'll be discussing [Briefly introduce the main topic or theme of the episode].\n",
    "\n",
    "Host 2: (Curious) And we have a very special guest joining us, [Guest's Name], who is an expert in [Guest's Expertise]. We can't wait to dive into the conversation!\n",
    "\n",
    "Host 1: (Engaging) But before we get started, a quick reminder to subscribe to our podcast if you haven't already, so you never miss an episode. And if you enjoy our show, please leave us a review on your favorite podcast platform.\n",
    "\n",
    "[SEGUE MUSIC]\n",
    "\n",
    "Host 2: (Transition) Alright, let's jump into today's topic. [Ask a question or set the stage for the discussion].\n",
    "\n",
    "Host 1: (Add relevant information) Yes, and I think it's important to mention [Add key points or interesting facts related to the topic].\n",
    "\n",
    "[DISCUSSION]\n",
    "\n",
    "[Podcast content: Engage in a thoughtful and engaging discussion, share stories, and exchange ideas with your guest.]\n",
    "\n",
    "[COMMERCIAL BREAK]\n",
    "\n",
    "Host 2: (Promotion) Before we continue, a quick word from our sponsors.\n",
    "\n",
    "[Insert a brief commercial or promotional message.]\n",
    "\n",
    "[OUTRO MUSIC]\n",
    "\n",
    "Host 1: (Closing) And that's a wrap for today's episode of [Your Podcast Name]! We hope you enjoyed the discussion as much as we did.\n",
    "\n",
    "Host 2: (Thank you message) Thank you, [Guest's Name], for joining us today. Your insights were invaluable.\n",
    "\n",
    "Host 1: (Call to action) As always, if you have any feedback, questions, or topics you'd like us to cover in future episodes, don't hesitate to reach out to us on our social media or website.\n",
    "\n",
    "Host 2: (Farewell) Until next time, this is [Your Name] and [Co-Host's Name] signing off. Stay curious and keep exploring!\n",
    "\n",
    "[OUTRO MUSIC]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./ner/saves/datasets/ontonote5_test_1403.pkl\n",
      "./ner/saves/datasets/ontonote5_train_9873.pkl\n",
      "wrapper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [06:32,  1.02it/s]\n",
      "400it [06:29,  1.03it/s]\n",
      "400it [06:50,  1.03s/it]\n",
      "400it [06:52,  1.03s/it]\n",
      "400it [07:03,  1.06s/it]\n",
      "400it [00:01, 200.41it/s]\n",
      "400it [00:02, 196.12it/s]\n",
      "400it [00:02, 194.97it/s]\n",
      "400it [00:02, 197.29it/s]\n",
      "400it [00:02, 197.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tagger\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "295it [09:00,  1.75s/it]"
     ]
    }
   ],
   "source": [
    "from ner.llm_ner.prompt_techniques.pt_abstract import PromptTechnique\n",
    "from ner.llm_ner.prompt_techniques.pt_discussion import PT_OutputList\n",
    "from ner.llm_ner.prompt_techniques.pt_gpt_ner import PT_GPT_NER\n",
    "from ner.llm_ner.prompt_techniques.pt_wrapper import PT_Wrapper\n",
    "from ner.llm_ner.prompt_techniques.pt_multi_pt import PT_Multi_PT, PT_2Time_Tagger\n",
    "from ner.llm_ner.prompt_techniques.pt_tagger import PT_Tagger\n",
    "from ner.llm_ner.prompt_techniques.pt_get_entities import PT_GetEntities\n",
    "from ner.llm_ner.few_shots_techniques import *\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from ner.Datasets.OntoNotes5Dataset import OntoNote5Dataset\n",
    "train, test = OntoNote5Dataset.my_load_dataset()\n",
    "\n",
    "\n",
    "runs = 2000\n",
    "for plus_plus in [False] :\n",
    "    # PT_GPT_NER(None,plus_plus=plus_plus),\n",
    "    for pt in [PT_Wrapper(None,plus_plus=plus_plus), PT_Tagger(None,plus_plus=plus_plus), PT_GetEntities(None,plus_plus=plus_plus),  PT_OutputList(None,plus_plus=plus_plus)] :\n",
    "        print(pt)\n",
    "        pt.process_dataset_for_finetuning(dataset = train, precision = \"++\" if plus_plus else \"\" , runs = runs, fst = FST_Sentence,nb_few_shots = [2,3])\n",
    "        pt.process_dataset_for_finetuning(dataset = train, precision = (\"++_\" if plus_plus else \"\") + \"noshots\", fst = FST_NoShots, runs = runs, nb_few_shots=[0])\n",
    "\n",
    "## Ou ## \n",
    "\n",
    "# plus_plus = False\n",
    "# for pt in [PT_GPT_NER(None,plus_plus=plus_plus), PT_OutputList(None,plus_plus=plus_plus), PT_Wrapper(None,plus_plus=plus_plus), PT_Tagger(None,plus_plus=plus_plus), PT_GetEntities(None,plus_plus=plus_plus)] :\n",
    "#     print(pt)\n",
    "#     pt.process_dataset_for_finetuning(dataset = train, precision = \"\" , runs = runs)\n",
    "#     pt.process_dataset_for_finetuning(dataset = train, precision = \"noshots\", fst = FST_NoShots, runs = runs, nb_few_shots=[0])\n",
    "\n",
    "# plus_plus = True\n",
    "# for pt in [PT_GPT_NER(None,plus_plus=plus_plus), PT_OutputList(None,plus_plus=plus_plus), PT_Wrapper(None,plus_plus=plus_plus), PT_Tagger(None,plus_plus=plus_plus), PT_GetEntities(None,plus_plus=plus_plus)] :\n",
    "#     pt.process_dataset_for_finetuning(dataset = train, precision = \"noshots\", fst = FST_NoShots, runs = runs, nb_few_shots=[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./ner/saves/datasets/ontonote5_test_1403.pkl\n",
      "./ner/saves/datasets/ontonote5_train_9873.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from ner.Datasets.OntoNotes5Dataset import OntoNote5Dataset\n",
    "train, test = OntoNote5Dataset.my_load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"If we have a profile of some kind of killer in this society and frankly there 's {a} rapist on the loose in Arizona right now and there is a picture that 's been drawn by police uh artists of what this guy looks like . And anybody that has an appearance somewhat like him should expect to be profiled by the police here /.\",\n",
       " \"Saddam Hussein 's last foreign minister Naji Sabri paid spy for +French intelligence later turned him over to the CIA to supply information about Iraq its chemical biological nuclear weapon program more than six months before the war began in March of oh three according {to} former intelligence officials /.\",\n",
       " 'The sources said he provided information {the?} Iraqi dictator had ambitions for a nuclear program but it was not active and that no biological weapons were being produced or stockpiled although research was under way /.',\n",
       " \"But the notion that somehow there was a placid Middle East , that if we just left it alone , if we 'd just not invaded Iraq , if we had just not overthrown dictators , if we {had?} just not challenged Syrian power in Lebanon everything would be just fine uh is simply not true /.\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dataset.filter(lambda row : '{' in row['text'])['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

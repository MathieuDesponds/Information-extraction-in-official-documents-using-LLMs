{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm.LLMModel import *\n",
    "from ner.llm_ner.prompt_techniques.pt_abstract import PromptTechnique\n",
    "from ner.llm_ner.prompt_techniques.pt_discussion import PT_OutputList\n",
    "from ner.llm_ner.prompt_techniques.pt_gpt_ner import PT_GPT_NER\n",
    "from ner.llm_ner.prompt_techniques.pt_gpt_ner import PT_GPT_NER\n",
    "from ner.llm_ner.prompt_techniques.pt_wrapper import PT_Wrapper\n",
    "from ner.llm_ner.few_shots_techniques import *\n",
    "from ner.llm_ner.ResultInstance import load_all_results, load_result\n",
    "import matplotlib.pyplot as plt\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_results = load_all_results(root_directory = \"ner/saves/results/ontonote5/mistral-7b-v0.1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_show = df_results[['f1_mean', 'f1_conf_inter', 'prompt_technique',\n",
    "       'few_shot_tecnique', 'nb_few_shots', 'precision', 'plus_plus']]\n",
    "\n",
    "df_to_show[df_to_show['precision'] == '300']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show results for finetuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_results['finetuned'] = df_results['model'].map(lambda s : 'ft' in s)\n",
    "df_results['nb_samples_tested'] = df_results['nb_test_run'] * df_results['len_data_test']\n",
    "ft_results = df_results[df_results['finetuned']]\n",
    "ft_results = ft_results[['f1_mean', 'f1_conf_inter', 'prompt_technique', 'model', 'noshots', 'few_shot_tecnique','nb_few_shots', 'nb_samples_tested']]\n",
    "ft_results['model_base'] = ft_results['model'].map(lambda s : '-'.join(s.split('-')[:-4]))\n",
    "ft_results['nb_train_samples'] = ft_results['model'].map(lambda s : s.split('-')[-2])\n",
    "ft_results = ft_results.sort_values(['prompt_technique',\t'few_shot_tecnique', 'f1_mean'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['f1_mean', 'f1_conf_inter', 'prompt_technique', 'few_shot_tecnique',\n",
    "    #    'nb_few_shots', 'nb_samples_tested', 'model_base', 'nb_train_samples',\n",
    "    #    'finetuned']\n",
    "grouped = ft_results.groupby(['prompt_technique', 'few_shot_tecnique', 'noshots', \n",
    "       'nb_few_shots', 'nb_samples_tested', 'model_base', 'nb_train_samples'])\n",
    "grouped \n",
    "\n",
    "idx_max = grouped['f1_mean'].idxmax()\n",
    "\n",
    "ft_cleaned = ft_results.loc[idx_max].sort_values('nb_train_samples', ascending=False)\n",
    "\n",
    "df = ft_cleaned#[ft_cleaned['nb_samples_tested'] == 500]\n",
    "# Create dummy columns for 'few_shot_tecnique'\n",
    "df.pivot(index=['model_base','prompt_technique','noshots',\n",
    "        'nb_train_samples','nb_samples_tested'], columns='few_shot_tecnique', values= 'f1_mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show results for non finetuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results['finetuned'] = df_results['model'].map(lambda s : 'ft' in s)\n",
    "df_results['nb_samples_tested'] = df_results['nb_test_run'] * df_results['len_data_test']\n",
    "ft_results = df_results[~df_results['finetuned']]\n",
    "ft_results = ft_results[['f1_mean', 'f1_conf_inter', 'prompt_technique', 'model', 'noshots', 'few_shot_tecnique','nb_few_shots', 'nb_samples_tested']]\n",
    "ft_results = ft_results.sort_values(['prompt_technique',\t'few_shot_tecnique', 'f1_mean'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['f1_mean', 'f1_conf_inter', 'prompt_technique', 'few_shot_tecnique',\n",
    "    #    'nb_few_shots', 'nb_samples_tested', 'model_base', 'nb_train_samples',\n",
    "    #    'finetuned']\n",
    "ft_results = ft_results[ft_results['model'] != 'none']\n",
    "grouped = ft_results.groupby([ 'model', 'prompt_technique', \n",
    "       'nb_few_shots', 'nb_samples_tested','few_shot_tecnique', ])\n",
    "grouped \n",
    "\n",
    "# idx_max = grouped['f1_mean'].idxmax()\n",
    "# idx_max\n",
    "\n",
    "# Create dummy columns for 'few_shot_tecnique'\n",
    "ft_results.loc[idx_max].pivot(index=['model','prompt_technique', 'nb_few_shots','nb_samples_tested',\n",
    "        ], columns='few_shot_tecnique', values= 'f1_mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLMModel.show_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at results of confidence with the tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner.utils import load\n",
    "\n",
    "res = load (\"ner/saves/results/conll2003_cleaned/mistral-7b-v0.1/multi_prompt-get-entities-tagger/sentence_3_None_1538_50_1_False.pkl\")\n",
    "\n",
    "cm, f1, precision, recall, y_true, y_pred, y_conf= res.res_insts[0].get_scores(with_y = True)\n",
    "conf = []\n",
    "for sent in res.res_insts[0].results:\n",
    "    conf.extend([s[2] for s in sent])\n",
    "\n",
    "len(y_conf), len(y_true), len(y_pred)\n",
    "# [i for i, tag in enumerate(y_pred) if tag == 'None']\n",
    "right = [y_true[i] == y_pred[i] for i in range(len(y_true))]\n",
    "rand = [i for i in range(len(right))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Example list\n",
    "my_list = list(zip(y_conf, right))\n",
    "\n",
    "# Use Counter to count occurrences\n",
    "entry_counts = Counter(sorted(my_list))\n",
    "\n",
    "# Print the counts\n",
    "for entry, count in entry_counts.items():\n",
    "    print(f'{entry}: {count} times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df_conf = pd.DataFrame(np.array([right, y_conf]).T, columns = ['Right', 'Confidence'])\n",
    "\n",
    "df_conf = pd.DataFrame(df_conf.groupby(['Right', 'Confidence'])['Right'].count()).rename(columns = {'Right' : \"count\"}).reset_index()\n",
    "sns.set(rc={'figure.figsize':(10,10)})\n",
    "order = ['high', 'medium-high', 'medium', 'medium-low', 'low']\n",
    "sns.catplot(data=df_conf, x=\"Confidence\", y=\"count\", hue=\"Right\", kind=\"bar\",aspect=1.5, order = order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing specific result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = \"mistral-7b-v0.1\"\n",
    "result = load_result(model, PT_OutputList.name(), FST_Sentence.name(), nb_few_shots= 10, with_precision = \"10_True\")\n",
    "\n",
    "for r in result.res_insts :\n",
    "    r.show_cm()\n",
    "    r.analyse_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner.Datasets.Conll2003Dataset import Conll2003Dataset\n",
    "from ner.Datasets.MyDataset import MyDataset\n",
    "data = MyDataset.my_load_dataset(dataset=Conll2003Dataset, split = 'test', cleaned= True)\n",
    "[(d['text'], d['spans']) for d in data if str(d['spans']) == \"\"\"[['Switzerland', 'LOC'], [\"Alfonse D'Amato\", 'PER'], ['U.S. Senate Banking Committee', 'ORG'], ['Poland', 'LOC'], ['Polish', 'MISC'], ['Swiss', 'MISC'], ['Poland', 'LOC']]\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(d['text'], d['spans']) for d in data if 'Hansenne' in d['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

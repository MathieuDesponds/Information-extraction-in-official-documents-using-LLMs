{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm.LLMModel import *\n",
    "from ner.llm_ner.prompt_techniques.pt_abstract import PromptTechnique\n",
    "from ner.llm_ner.prompt_techniques.pt_discussion import PT_OutputList\n",
    "from ner.llm_ner.prompt_techniques.pt_gpt_ner import PT_GPT_NER\n",
    "from ner.llm_ner.prompt_techniques.pt_gpt_ner import PT_GPT_NER\n",
    "from ner.llm_ner.prompt_techniques.pt_wrapper import PT_Wrapper\n",
    "from ner.llm_ner.few_shots_techniques import *\n",
    "from ner.llm_ner.ResultInstance import load_all_results, load_result\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results, raw_results= load_all_results(root_directory = \"ner/saves/results/ontonote5/\")\n",
    "df = df_results.sort_values(['f1_mean', 'prompt_technique', 'nb_few_shots'], ascending = False)\n",
    "df[df['model'].str.contains(\"few-shots\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, res3  in enumerate(raw_results) :\n",
    "    res3 :ResultInstanceWithConfidenceInterval = res3\n",
    "    nb_instance = dict['len_data_test']* dict['nb_test_run']\n",
    "    dict =res3.get_dict() \n",
    "    time_elapsed = sum([res.get_dict()['elapsed_time'] for res in res3.res_insts])/nb_instance\n",
    "    if dict['model'] == 'chat-gpt-3.5' and 'ft' not in dict['model'] :\n",
    "        print(i, time_elapsed, nb_instance,  dict['nb_few_shots'], dict['prompt_technique'], dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[1][7].res_insts[0].analyse_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = load_all_results(root_directory = \"ner/saves/results/ontonote5/\")\n",
    "df_results = df_results[df_results['prompt_technique'] == 'discussion']\n",
    "df_results = df_results[df_results['few_shot_tecnique'] == 'sentence']\n",
    "df_results.sort_values(['nb_few_shots', 'model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_show = df_results[~df_results['model'].str.contains('ft')][['f1_mean', 'f1_conf_inter', 'prompt_technique',\n",
    "       'few_shot_tecnique', 'nb_few_shots', 'precision', 'plus_plus']]\n",
    "\n",
    "df_res = df_to_show[df_to_show['precision'] == '300']\n",
    "df_res['tech_name'] = df_res.apply(lambda row :f\"With {row['nb_few_shots']} few_shots {'and ++' if row['plus_plus']  else ''}\", axis = 1)\n",
    "df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLMModel.show_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at results of confidence with the tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner.utils import load\n",
    "\n",
    "res = load (\"ner/saves/results/conll2003_cleaned/mistral-7b-v0.1/multi_prompt-get-entities-tagger/sentence_3_None_1538_50_1_False.pkl\")\n",
    "\n",
    "cm, f1, precision, recall, y_true, y_pred, y_conf= res.res_insts[0].get_scores(with_y = True)\n",
    "conf = []\n",
    "for sent in res.res_insts[0].results:\n",
    "    conf.extend([s[2] for s in sent])\n",
    "\n",
    "len(y_conf), len(y_true), len(y_pred)\n",
    "# [i for i, tag in enumerate(y_pred) if tag == 'None']\n",
    "right = [y_true[i] == y_pred[i] for i in range(len(y_true))]\n",
    "rand = [i for i in range(len(right))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Example list\n",
    "my_list = list(zip(y_conf, right))\n",
    "\n",
    "# Use Counter to count occurrences\n",
    "entry_counts = Counter(sorted(my_list))\n",
    "\n",
    "# Print the counts\n",
    "for entry, count in entry_counts.items():\n",
    "    print(f'{entry}: {count} times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df_conf = pd.DataFrame(np.array([right, y_conf]).T, columns = ['Right', 'Confidence'])\n",
    "\n",
    "df_conf = pd.DataFrame(df_conf.groupby(['Right', 'Confidence'])['Right'].count()).rename(columns = {'Right' : \"count\"}).reset_index()\n",
    "sns.set(rc={'figure.figsize':(10,10)})\n",
    "order = ['high', 'medium-high', 'medium', 'medium-low', 'low']\n",
    "sns.catplot(data=df_conf, x=\"Confidence\", y=\"count\", hue=\"Right\", kind=\"bar\",aspect=1.5, order = order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing specific result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = \"mistral-7b-v0.1\"\n",
    "result = load_result(model, PT_OutputList.name(), FST_Sentence.name(), nb_few_shots= 10, with_precision = \"10_True\")\n",
    "\n",
    "for r in result.res_insts :\n",
    "    r.show_cm()\n",
    "    r.analyse_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner.Datasets.Conll2003Dataset import Conll2003Dataset\n",
    "from ner.Datasets.MyDataset import MyDataset\n",
    "data = MyDataset.my_load_dataset(dataset=Conll2003Dataset, split = 'test', cleaned= True)\n",
    "[(d['text'], d['spans']) for d in data if str(d['spans']) == \"\"\"[['Switzerland', 'LOC'], [\"Alfonse D'Amato\", 'PER'], ['U.S. Senate Banking Committee', 'ORG'], ['Poland', 'LOC'], ['Polish', 'MISC'], ['Swiss', 'MISC'], ['Poland', 'LOC']]\"\"\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with a dataset \n",
    "\n",
    "We compare different model on the dataset of wikiann that contains tags `['PER', 'ORG', 'LOC']`\n",
    "We get teh score by comparing the output at the entity level and not at the token level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import defaultdict\n",
    "from ner.process_results import get_metrics, get_metrics_all, show_cm, show_cm_multi, get_metrics_all_extended, show_cm_multi_extended\n",
    "from ner.classical_models.ner_models import get_results\n",
    "from ner.Datasets.Conll2003Dataset import Conll2003Dataset\n",
    "from ner.Datasets.OntoNotes5Dataset import OntoNote5Dataset\n",
    "from ner.Datasets.MyDataset import MyDataset\n",
    "\n",
    "from ner.utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# dataset = MyDataset.my_load_dataset(dataset=Conll2003Dataset, split = 'test', cleaned= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting results with Ontonote dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37ea90c65a64a6c821aaf23ec827f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c8d67699334852b1ddd5deed59a28e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1403 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train, test = OntoNote5Dataset.my_load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 179/1402 [11:29<1:13:40,  3.61s/it]"
     ]
    }
   ],
   "source": [
    "results, f1s= get_results(test, with_save= True)\n",
    "f1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = results_files[10]\n",
    "file_path = folder_path + file_name\n",
    "with open(file_path, 'rb') as f:\n",
    "    results_llama[file_name]= pickle.load(f) \n",
    "print(results_llama[file_name].keys()) \n",
    "results_llama[file_name]['true_labels'] = dataset_llama['spans']\n",
    "cm = {}\n",
    "f1 = {}\n",
    "precision = {}\n",
    "recall = {}\n",
    "y_true, y_pred, all_nes= {},{}, {}\n",
    "for model in results_llama[file_name] : \n",
    "    if model != \"true_labels\" :\n",
    "        cm[model],f1[model], precision[model], recall[model], y_true[model], y_pred[model], all_nes[model]= get_metrics_all(results_llama[file_name][model], results_llama[file_name]['true_labels'])\n",
    "        show_cm_multi(cm[model],f1[model], precision[model], recall[model], model)\n",
    "print(f\"{file_name} {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'LABEL_0', 'score': 0.9992781, 'index': 1, 'word': 'My', 'start': 0, 'end': 2}, {'entity': 'LABEL_0', 'score': 0.9994466, 'index': 2, 'word': 'name', 'start': 3, 'end': 7}, {'entity': 'LABEL_0', 'score': 0.99946386, 'index': 3, 'word': 'is', 'start': 8, 'end': 10}, {'entity': 'LABEL_1', 'score': 0.9927504, 'index': 4, 'word': 'Wolfgang', 'start': 11, 'end': 19}, {'entity': 'LABEL_2', 'score': 0.9861588, 'index': 5, 'word': 'Mozart', 'start': 20, 'end': 26}, {'entity': 'LABEL_0', 'score': 0.9996026, 'index': 6, 'word': 'and', 'start': 27, 'end': 30}, {'entity': 'LABEL_0', 'score': 0.99673045, 'index': 7, 'word': 'I', 'start': 31, 'end': 32}, {'entity': 'LABEL_0', 'score': 0.99919397, 'index': 8, 'word': 'live', 'start': 33, 'end': 37}, {'entity': 'LABEL_0', 'score': 0.9991404, 'index': 9, 'word': 'in', 'start': 38, 'end': 40}, {'entity': 'LABEL_5', 'score': 0.9969441, 'index': 10, 'word': 'Berlin', 'start': 41, 'end': 47}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from ner.classical_models.ner_models import *\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/distilbert-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/distilbert-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Wolfgang Mozart and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)\n",
    "print(from_bert_to_tags(ner_results, \"distilbert\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Apple, U.K., $1 billion)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Apple', 'ORG', 'ORG'),\n",
       " ('U.K.', 'GPE', 'GPE'),\n",
       " ('$1 billion', 'MONEY', 'MONEY')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.examples import sentences \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "doc = nlp(sentences[0])\n",
    "print(doc.ents)\n",
    "entities = [(ent.text, ent.label_, ent.label_) for ent in doc.ents]\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a453670310ee47d2bd1706f38ea6acad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b77eebcf8c4067a1180647efdc1303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/852 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b30829a693742c1ad0d494dd5d8abcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b3617fbf0b4f049853fedfa0a338d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9721d4e131f485aa47775d114b7ee57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large-finetuned-conll03-english were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': 0.9997861,\n",
       "  'index': 1,\n",
       "  'word': '▁Al',\n",
       "  'start': 0,\n",
       "  'end': 2},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9998591,\n",
       "  'index': 2,\n",
       "  'word': 'ya',\n",
       "  'start': 2,\n",
       "  'end': 4},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99995816,\n",
       "  'index': 4,\n",
       "  'word': '▁Jasmin',\n",
       "  'start': 10,\n",
       "  'end': 16},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.9999584,\n",
       "  'index': 5,\n",
       "  'word': 'e',\n",
       "  'start': 16,\n",
       "  'end': 17},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99998057,\n",
       "  'index': 7,\n",
       "  'word': '▁Andrew',\n",
       "  'start': 23,\n",
       "  'end': 29}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "\n",
    "classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "results = classifier(\"Alya told Jasmine that Andrew could pay with cash..\")\n",
    "\n",
    "print(results)\n",
    "print(from_bert_to_tags(results, \"roberta\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cm = {}\n",
    "f1 = {}\n",
    "precision = {}\n",
    "recall = {}\n",
    "y_true, y_pred, all_nes= {},{}, {}\n",
    "res = results\n",
    "for model in res: \n",
    "    if model != \"true_labels\" :\n",
    "        cm[model],f1[model], precision[model], recall[model], y_true[model], y_pred[model], all_nes[model]= get_metrics_all(res[model], res['true_labels'])\n",
    "        # show_cm(cm[model], [{tag : f\"{f1s[model][tag]:.2f}\" } for tag in [\"general\"] + tag_type], model)\n",
    "        show_cm_multi(cm[model],f1[model], precision[model], recall[model], model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = {}\n",
    "f1 = {}\n",
    "precision = {}\n",
    "recall = {}\n",
    "y_true, y_pred, all_nes= {},{}, {}\n",
    "res = results\n",
    "for model in res : \n",
    "    if model in ['spacy', 'refined'] :\n",
    "        cm[model],f1[model], precision[model], recall[model], y_true[model], y_pred[model], all_nes[model]= get_metrics_all_extended(res[model], res['true_labels'])\n",
    "        # show_cm(cm[model], [{tag : f\"{f1s[model][tag]:.2f}\" } for tag in [\"general\"] + tag_type], model)\n",
    "        show_cm_multi_extended(cm[model],f1[model], precision[model], recall[model], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "key =\"entity\"\n",
    "resu = pd.DataFrame({\"gold\" : y_true[key], \"pred\" : y_pred[key], 'named_entity' : all_nes[key]})\n",
    "resu[resu['gold'] != resu['pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for key in results :\n",
    "    flattened = [item for sublist in results[key] for item in sublist]\n",
    "    dfs[key] = pd.DataFrame([{'entity' : ne[0], 'tag' : ne[1], 'ref_tag' : ne[2] if len(ne)>2 else \"\"} for ne in flattened])\n",
    "    dfs[key] = dfs[key].groupby('entity').agg(set)\n",
    "dfs['true_labels'].rename(columns={'tag': 'gold_tags'}, inplace=True)\n",
    "del dfs['true_labels']['ref_tag']\n",
    "\n",
    "for key in results :\n",
    "    if key != \"true_labels\" :\n",
    "        dfs[key] = dfs['true_labels'].merge(dfs[key], on='entity', how='outer')\n",
    "        dfs[key].fillna(-1, inplace=True)\n",
    "\n",
    "\n",
    "for key in results_llama :\n",
    "    flattened = [item for sublist in results_llama[key] for item in sublist]\n",
    "    dfs[key] = pd.DataFrame([{'entity' : ne, 'tag' : tag} for ne, tag in flattened])\n",
    "    dfs[key] = dfs[key].groupby('entity').agg(set)\n",
    "dfs['true_labels'].rename(columns={'tag': 'gold_tags'}, inplace=True)\n",
    "for key in results_llama :\n",
    "    if key != \"true_labels\" :\n",
    "        dfs[key] = dfs['true_labels'].merge(dfs[key], on='entity', how='outer')\n",
    "        dfs[key].fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'few_shots_random'\n",
    "tag_distr_missed_by_model(key), tag_distr_added_by_model(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'few_shots_sentence'\n",
    "tag_distr_missed_by_model(key), tag_distr_added_by_model(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'entity'\n",
    "tag_distr_missed_by_model(key), tag_distr_added_by_model(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'refined'\n",
    "tag_distr_missed_by_model(key), tag_distr_added_by_model(key)\n",
    "# temp = dfs[key][dfs[key]['gold_tags'] != -1]\n",
    "# temp[temp['tag'] == {'MISC'}][['gold_tags','tag_precision_y']]['tag_precision_y'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
